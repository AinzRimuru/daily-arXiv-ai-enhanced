<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.IT](#cs.IT) [Total: 6]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.AI](#cs.AI) [Total: 42]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Social Story Frames: Contextual Reasoning about Narrative Intent and Reception](https://arxiv.org/abs/2512.15925)
*Joel Mire,Maria Antoniak,Steven R. Wilson,Zexin Ma,Achyutarama R. Ganti,Andrew Piper,Maarten Sap*

Main category: cs.CL

TL;DR: 本文提出 SocialStoryFrames 框架及相关模型，旨在通过推断作者意图、情感反应和价值判断等，实现对大规模社交媒体故事中读者反应的细粒度计算分析。


<details>
  <summary>Details</summary>
Motivation: 阅读故事会引发丰富的解释、情感和评价反应（如动机推断或角色判断），但目前的计算模型在捕获这种微妙的读者反应方面存在局限，限制了细致的叙事分析。

Method: 提出了 SocialStoryFrames 形式化框架，基于叙事理论、语言语用学和心理学构建分类体系；开发了 SSF-Generator 和 SSF-Classifier 两个模型，并构建了包含 6,140 个社交媒体故事的 SSF-Corpus 数据集。

Result: 通过 382 名受试者的调查和专家标注验证了模型的有效性；试点分析揭示了故事讲述意图的频率、相互依赖性，以及不同社区之间叙事实践的差异。

Conclusion: SocialStoryFrames 弥补了现有计算模型的局限性，通过细粒度、感知的建模方式，为在大规模在线社区中研究叙事及其多样性提供了强有力的工具。

Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.

</details>


### [2] [TabReX : Tabular Referenceless eXplainable Evaluation](https://arxiv.org/abs/2512.15907)
*Tejas Anvekar,Juhna Park,Aparna Garimella,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文介绍了 TabReX，一种无需参考、基于图推理的表格生成评估框架，通过将表格转换为知识图谱来提供高精度、可解释且符合人类判断的质量评分。


<details>
  <summary>Details</summary>
Motivation: 现有的表格生成评估指标要么将表格视为纯文本而忽略结构，要么过度依赖固定参考，限制了泛化能力且缺乏可解释性。

Method: 提出 TabReX 框架，将源文本和生成表格转换为规范知识图谱，通过 LLM 引导的匹配过程进行对齐，并基于评分标准计算可量化的结构和事实忠实度得分。

Result: 实验表明 TabReX 在六个领域的专家排名相关性最高，在 12 种扰动类型下表现稳健，并能提供单元格级的错误追踪。

Conclusion: TabReX 为结构化生成评估建立了新范式，证明了基于属性的图形推理比传统的文本平坦化或固定参考指标更具稳健性和可解释性。

Abstract: Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.

</details>


### [3] [Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms](https://arxiv.org/abs/2512.16034)
*Kieran Henderson,Kian Omoomi,Vasudha Varadarajan,Allison Lahnala,Charles Welch*

Main category: cs.CL

TL;DR: 本文分析了个人自我披露信息对预测社会规范标注的影响，发现人口统计特征最具预测力，且多样化的标注背景能显著提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的研究尝试利用个人信息（如画像或自我披露）来预测主观任务中的标注偏差，但由于受限于数据量，尚未有深入研究探讨哪类个人信息对预测标注结果最有效。

Method: 研究人员对自我披露（self-disclosure）句子进行分类，建立了用于预测社会规范判断的标注者模型。通过多种消融实验和分析，对比了不同类型信息（人口统计、态度、关系、经验等）以及信息分类方式（理论基础 vs 自动聚类）对预测准确性的影响。

Result: 1. 人口统计信息比态度、社会关系和个人经历更具影响力；2. 基于理论分类的方法优于自动聚类；3. 与以往研究不同，仅需少量相关评论即可达到预测效果；4. 标注者自我披露样本的多样性与模型性能呈正相关。

Conclusion: 在主观任务的人类标注预测中，标注者的背景信息（尤其是人口统计学特征）至关重要。研究强调，多样化的标注者样本和基于理论的信息分类（而非自动聚类）对提升模型性能更有效。

Abstract: Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.

</details>


### [4] [Are We on the Right Way to Assessing LLM-as-a-Judge?](https://arxiv.org/abs/2512.16041)
*Yuanning Feng,Sinan Wang,Zhengxiang Cheng,Yao Wan,Dongping Chen*

Main category: cs.CL

TL;DR: Sage是一个无需人类标注的新型LLM-as-a-Judge评估基准，通过逻辑一致性维度揭示了当前顶级模型作为评测员时存在的鲁棒性缺陷，并提出了提升评测可靠性的可行方案。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-as-a-Judge评估基准过度依赖人类标注的真实标签（ground truth），这引入了人类偏好偏差，限制了评估的可靠性及在大规模场景下的扩展性。

Method: 提出名为Sage的评估框架。该框架基于理性选择理论的公理，通过“局部自洽性”（成对偏好稳定性）和“全局逻辑一致性”（全集偏好的传递性）两个维度，在无需人类标注的情况下量化LLM评测员的质量。

Result: Sage指标与LLMBar、RewardBench等有监督基准高度相关。实验揭示即使是顶级模型（如GPT-4o/Gemini-1.5等级别）在困难案例中也有近25%的比例无法保持一致性，并由此发现了“情境偏好”现象。

Conclusion: 当前先进LLM在担任评测员时仍存在显著的可靠性问题。通过细化评分标准（rubrics）、采用专家小组机制和引入深度推理，可有效提升评测一致性。同时，人类标注存在的不一致性暗示其可能并非评估评测器的唯一金标准。

Abstract: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.

</details>


### [5] [Convolutional Lie Operator for Sentence Classification](https://arxiv.org/abs/2512.16125)
*Daniela N. Rim,Heeyoul Choi*

Main category: cs.CL

TL;DR: 本研究提出 SCLie 和 DPCLie 模型，通过引入 Lie 卷积来捕捉复杂的语义变换，在句子分类任务上超越了传统 CNN。


<details>
  <summary>Details</summary>
Motivation: 传统 CNN 擅长捕获局部和位置不变特征，但在建模语言中复杂的非欧几里得对称性和变换方面能力有限。

Method: 将 Lie 卷积（Lie Convolutions）集成到基于卷积的句子分类器中，并提出两种新模型：SCLie 和 DPCLie。

Result: 实验表明，SCLie 和 DPCLie 的表现均优于传统卷积句子分类器，通过捕获语言中通常被忽视的对称变换提升了准确率。

Conclusion: Lie 卷积在文本分类任务中具有显著潜力，为探索非传统语言变换模型提供了新范式。

Abstract: Traditional Convolutional Neural Networks have been successful in capturing local, position-invariant features in text, but their capacity to model complex transformation within language can be further explored. In this work, we explore a novel approach by integrating Lie Convolutions into Convolutional-based sentence classifiers, inspired by the ability of Lie group operations to capture complex, non-Euclidean symmetries. Our proposed models SCLie and DPCLie empirically outperform traditional Convolutional-based sentence classifiers, suggesting that Lie-based models relatively improve the accuracy by capturing transformations not commonly associated with language. Our findings motivate more exploration of new paradigms in language modeling.

</details>


### [6] [MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation](https://arxiv.org/abs/2512.16145)
*Pengyu Wang,Shuchang Ye,Usman Naseem,Jinman Kim*

Main category: cs.CL

TL;DR: 本文提出MRG-R1，通过语义驱动的强化学习（SRL）和报告级奖励机制，使医学大模型从简单的文本模仿转向临床准确性评估，显著提升了放射报告生成的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的医学报告生成方法过度关注词汇和句式模仿（Token-level），导致生成的报告虽语言流畅但缺乏临床准确性。

Method: 提出SRL方法，利用GRPO算法优化基于边缘的余弦相似度（MCCS）奖励，通过提取医学发现的关键标签进行对齐，并引入轻量化推理格式约束。

Result: 在IU X-Ray和MIMIC-CXR数据集上达到SOTA，临床效用指标（CE-F1）分别提升至51.88和40.39。

Conclusion: 临床导向的报告级奖励优化比传统的词级监督更有效，是提升医学多模态大模型（Med-LVLM）临床准确性的重要方向。

Abstract: Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured "thinking report" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.

</details>


### [7] [Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning](https://arxiv.org/abs/2512.16147)
*Yash Bhaskar,Sankalp Bahad,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文介绍了一种针对印地语-英语混合社交媒体文本的 Faux-Hate 检测系统，通过多任务学习和领域预训练，实现了对虚假仇恨言论及其目标和严重程度的有效识别。


<details>
  <summary>Details</summary>
Motivation: 应对社交媒体上由虚假叙述驱动的仇恨言论（Faux-Hate）传播挑战，特别是在印地语-英语代码混杂（Code-mixed）的复杂语境下进行精准检测。

Method: 采用了结合先进自然语言处理（NLP）技术与领域特定预训练（Domain-specific pretraining）的方法，并利用多任务学习（Multi-task learning）框架来同时处理分类任务。

Result: 该系统在双向 Faux-Hate 检测以及目标与严重程度预测任务中均取得了具有竞争力的结果。

Conclusion: 多任务学习结合领域预训练是解决多维度有害内容识别（如代码混杂环境下的虚假仇恨言论）的一种稳健且有效的方法。

Abstract: Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.

</details>


### [8] [A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media](https://arxiv.org/abs/2512.16183)
*Mengfan Shen,Kangqi Song,Xindi Wang,Wei Jia,Tao Wang,Ziqiang Han*

Main category: cs.CL

TL;DR: 本文提出一种基于 Qwen2.5-7B 和 LoRA 微调的方法，能从不规范的微博警务通报中高效提取 15 类关键结构化信息，准确率极高。


<details>
  <summary>Details</summary>
Motivation: 警方通报（尤其是社交媒体上的）具有非规范性和多样性，给结构化信息提取带来了挑战，为了更准确地支持社会科学研究和现状评估。

Method: 基于 Qwen2.5-7B 模型，利用低秩自适应（LoRA）技术进行参数高效微调，并结合针对性的提示工程（Prompt Engineering）构建领域适配的提取流水线。

Result: 在 4,933 条高质量标注数据集上，该模型表现优异：死亡检测准确率超 98.36%，死亡人数精确匹配率达 95.31%，省级地理位置提取匹配率为 95.54%，显著优于基础模型。

Conclusion: 该流程为处理社交媒体上的非结构化警务文本提供了一个经过验证且高效的框架，为社会科学研究中的数据结构化转化提供了实用参考。

Abstract: Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.

</details>


### [9] [Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation](https://arxiv.org/abs/2512.16189)
*Musarrat Zeba,Abdullah Al Mamun,Kishoar Jahan Tithee,Debopom Sutradhar,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Reem E. Mohamed,Md Rafiqul Islam,Yakub Sebastian,Mukhtar Hussain,Sami Azam*

Main category: cs.CL

TL;DR: 本文提出了一种结合 LoRa 微调总结模型与独立事实核查模块的方法，旨在通过数值和逻辑校验，减少医疗摘要中的幻觉，提升生成内容的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域应用时常会出现“幻觉”，导致输出信息不可靠，这对医疗决策和患者安全构成威胁。

Method: 使用 MIMIC-III 数据集并配合 LoRa 技术微调 LLM 总结模型；同时引入独立的核查模块，通过数值测试和基于自然语言处理的离散逻辑进行事实校验。

Result: 事实核查模块的 F1 分数为 0.8556（精准率 0.8904）；总结模型在 ROUGE-1 上达到 0.5797，BERTScore 为 0.9120。

Conclusion: 结合 LoRa 微调的总结模型与独立的逻辑事实核查模块，可以显著提高医疗摘要的准确性，减少幻觉现象。

Abstract: In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.

</details>


### [10] [An Information-Theoretic Framework for Robust Large Language Model Editing](https://arxiv.org/abs/2512.16227)
*Qizhou Chen,Chengyu Wang,Taolin Zhang,Xiaofeng He*

Main category: cs.CL

TL;DR: 本文提出 IBKE，一种基于信息瓶颈理论的 LLM 知识编辑新方法，实现了高泛化、低干扰的精准模型更新。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 编辑技术在泛化能力上表现较差，且容易产生副作用，亟需一种既能精准更新知识又不干扰模型原有行为的高效策略。

Method: 提出 IBKE 框架，基于信息瓶颈理论（Information Bottleneck Theory），通过压缩和隔离关键信息，利用紧凑的潜层表示指导梯度更新。

Result: 在多种 LLM 架构和基准任务上均达到 SOTA 精度，并在编辑的泛化性（Generality）和特异性（Specificity）方面表现优异。

Conclusion: IBKE 为开放域知识编辑建立了一个理论严谨且实用的范式，提升了 LLM 在现实应用中的实用性和可信度。

Abstract: Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.

</details>


### [11] [Sigma-Moe-Tiny Technical Report](https://arxiv.org/abs/2512.16248)
*Qingguo Hu,Zhenghao Lin,Ziyue Yang,Yucheng Ding,Xiao Liu,Yuting Jiang,Ruizhe Wang,Tianyu Chen,Zhongxin Guo,Yifan Xiong,Rui Gao,Lei Qu,Jinsong Su,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: 本文推出 Sigma-MoE-Tiny，通过渐进式稀疏调度实现了超高稀疏度（20B 总参数，仅 0.5B 激活），在极低计算成本下达到了顶尖性能。


<details>
  <summary>Details</summary>
Motivation: 尽管 Mixture-of-Experts (MoE) 具有高效扩展性，但在极端稀疏的情况下（如极少激活参数），现有的负载均衡损失函数在底层往往失效，导致训练不稳定和专家利用率不均。

Method: 采用了细粒度的专家分割（每层多达 96 个专家），每 token 仅激活 1 个专家；同时引入了“渐进式稀疏化调度（Progressive Sparsification Schedule）”来解决底层专家的负载均衡问题。

Result: Sigma-MoE-Tiny 在总参数 20B、激活参数仅 0.5B 的情况下，展现了极高的训练稳定性（无不可恢复的 Loss 尖峰），其性能优于许多规模更大或激活参数更多的同类模型。

Conclusion: 高度稀疏的 MoE 架构在极端稀疏设置下（如 0.5B 激活参数）依然能保持卓越性能，为未来超大规模、低功耗的基础模型开发提供了重要参考。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.
  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny
  Code: https://github.com/microsoft/ltp-megatron-lm

</details>


### [12] [Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures](https://arxiv.org/abs/2512.16287)
*Yehor Tereshchenko,Mika Hämäläinen,Svitlana Myroniuk*

Main category: cs.CL

TL;DR: 本研究对比了GPT推理与非推理模型在芬兰语至四种低资源乌拉尔语翻译中的表现，发现推理模型的拒绝率更低，更适合濒危语言任务。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要集中在高资源语言，对低资源和濒危语言（如乌拉尔语系）的研究存在显著空白，特别是缺乏对推理架构与普通架构在这些语言上表现差异的探讨。

Method: 使用文学作品平行语料库，在芬兰语与四种低资源乌拉尔语（科米-齐良语、莫克沙语、厄尔兹亚语、乌德穆尔特语）之间进行翻译测试，通过对比OpenAI GPT推理模型与非推理模型的拒绝率来衡量其翻译意愿和能力。

Result: 推理模型在处理翻译任务时显著优于非推理模型，其翻译拒绝率比后者低16个百分点，显示出更强的复杂指令遵循能力。

Conclusion: 推理模型在低资源及濒危语言翻译中表现出更强的任务执行能力和更低的拒绝率，为相关语言的数字化保护提供了参考。

Abstract: The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.

</details>


### [13] [Hacking Neural Evaluation Metrics with Single Hub Text](https://arxiv.org/abs/2512.16323)
*Hiroyuki Deguchi,Katsuki Chousa,Yusuke Sakai*

Main category: cs.CL

TL;DR: 研究发现 COMET 等神经文本评估指标存在重大漏洞：通过特定方法找到的单一“枢纽文本”可以在不同测试用例中均骗取高分，甚至优于实际翻译模型的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管 COMET 等神经文本评估指标被广泛使用，但由于其黑盒特性，其评估结果的可靠性和安全性缺乏保障，可能存在鲁棒性漏洞。

Method: 提出了一种在离散空间中寻找单个“对抗性枢纽文本”（hub text）的方法，该文本无论针对何种测试用例均能获得高分。

Result: 在 WMT'24 英-日和英-德任务中，生成的单一枢纽文本分别获得了 79.1% 和 67.8% 的 COMET 高分，甚至超过了通用翻译模型 M2M100 为每个句子单独生成的翻译质量。同时，该现象在跨语言对（如日-英、德-英）中同样存在。

Conclusion: 本研究揭示了嵌入式评估指标（如 COMET）存在严重的安全漏洞，即单一的“枢纽文本”可以欺骗指标给出高分，呼吁开发更具鲁棒性和可靠性的文本评估方法。

Abstract: Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.

</details>


### [14] [Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs](https://arxiv.org/abs/2512.16378)
*Sara Papi,Javier Garcia Gilabert,Zachary Hopton,Vilém Zouhar,Carlos Escolano,Gerard I. Gállego,Jorge Iranzo-Sánchez,Ahrii Kim,Dominik Macháček,Patricia Schmidtova,Maike Züfle*

Main category: cs.CL

TL;DR: 该研究通过大规模评测发现，在语音翻译任务中，传统的“语音模型+大语言模型”级联系统依然优于目前的原生 SpeechLLM，证明了 LLM 的引入对提升翻译质量至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着 SpeechLLM 将语音作为原生模态引入，旨在通过跳过传统转录流程来提高效率。然而，这种集成方式在翻译质量上是否真的优于成熟的级联架构（Cascade architectures）尚未得到系统性的验证。

Method: 提出首个全面测试集 "Hearing to Translate"，对 5 个 SOTA SpeechLLM 与 16 个强基线系统（包括直接翻译系统和将语音基础模型与多语言 LLM 结合的级联系统）在 16 个基准测试、13 种语言对及 9 种挑战性条件下进行对比分析。

Result: 级联系统在整体可靠性上仍然表现最佳；当前的 SpeechLLM 仅在特定设置下能与级联系统媲美；而纯语音基础模型（SFM）表现最差。

Conclusion: 当前尽管 SpeechLLM 展现出潜力，但级联系统（SFM + LLM）在翻译质量和鲁棒性方面仍处于领先地位。将 LLM 集成到语音翻译流程中（无论是作为端到端模型还是级联组件）是获取高质量翻译结果的关键。

Abstract: As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.

</details>


### [15] [Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains](https://arxiv.org/abs/2512.16401)
*Darshil Chauhan,Adityasinh Solanki,Vansh Patel,Kanav Kapoor,Ritvik Jain,Aditya Bansal,Dhruv Kumar,Prateek Narang*

Main category: cs.CL

TL;DR: 本文针对临床环境下的ASR挑战，通过LoRA边缘持续学习和经验回放技术，实现了在保护隐私的同时显著提升识别准确率并缓解遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 尽管自动语音识别（ASR）在临床文档数字化方面潜力巨大，但在基层医疗（如农村）应用时面临数据隐私严苛、计算资源有限以及严重的声学领域偏移（Domain Shift）三大挑战。

Method: 提出一种高效且保护隐私的自适应框架，利用低秩自适应（LoRA）在边缘设备上对流式数据进行持续学习，并结合多领域经验回放（Experience Replay）策略。

Result: 在目标领域（Gram Vaani临床音频）上，WER（词错误率）相对改善了17.1%；相比于朴素自适应方法，由于引入了经验回放，灾难性遗忘减少了47%。

Conclusion: 该研究证明了在受限的临床环境中，构建可靠且具有自我改进能力的ASR系统是可行的，为医疗数字化提供了实用方案。

Abstract: Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.

</details>


### [16] [UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification](https://arxiv.org/abs/2512.16541)
*Primoz Kocbek,Gregor Stiglic*

Main category: cs.CL

TL;DR: 本文研究了 gpt-4.1 系列模型在科学文本简化中的应用，发现 mini 模型在提示工程下表现稳定，而 nano 模型在经过微调后在文档级简化中展现出潜力。


<details>
  <summary>Details</summary>
Motivation: 应对 CLEF 2025 SimpleText 任务 1，解决科学文献中句子级和文档级的文本简化挑战。

Method: 采用了 OpenAI 的 gpt-4.1 系列模型（包括 mini 和 nano 版本），对比了两种策略：基于提示工程的“无上下文（no-context）”方法和针对特定任务的“微调（FT）”方法。

Result: gpt-4.1-mini 在无上下文配置下在两个层面表现稳健；而微调模型结果不一，其中 gpt-4.1-nano-ft 在特定文档级简化任务中表现突出。

Conclusion: 在不同粒度的文本简化任务中，模型表现存在差异；微调并非总是优于提示工程，需要根据具体任务层次选择最优模型。

Abstract: This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.

</details>


### [17] [Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics](https://arxiv.org/abs/2512.16602)
*Iker García-Ferrero,David Montero,Roman Orus*

Main category: cs.CL

TL;DR: 本文提出了一种无需重训的推理端方法“Refusal Steering”，通过正则化引导向量精确控制 LLM 对政治敏感话题的选择性拒绝，在保持通用安全性的同时减少了过度防御。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理政治敏感话题时往往存在过度拒绝（Refusal）的问题，而传统的微调方法成本高且缺乏灵活性，需要在不重新训练模型的前提下精确控制拒绝行为。

Method: 提出 Refusal Steering 方法：使用 LLM-as-a-judge 评估拒绝置信度代替传统的模式匹配，并采用岭回归正则化（Ridge-regularized）计算引导向量，以更精准地分离拒绝与服从的维度。

Result: 在 Qwen3-Next-80B 等模型上，该方法成功移除了政治敏感话题的拒绝行为，同时在 JailbreakBench 安全基准上保持了安全性，且在通用性能测试中接近基准水平；研究还发现拒绝信号主要集中在 Transformer 的深层空间。

Conclusion: 激活引导（Activation Steering）是一种在推理阶段消除模型政治敏感偏见的有效手段，有助于实现更可控和透明的内容审核。

Abstract: We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.

</details>


### [18] [JustRL: Scaling a 1.5B LLM with a Simple RL Recipe](https://arxiv.org/abs/2512.16649)
*Bingxiang He,Zekai Qu,Zeyuan Liu,Yinghao Chen,Yuxin Zuo,Cheng Qian,Kaiyan Zhang,Weize Chen,Chaojun Xiao,Ganqu Cui,Ning Ding,Zhiyuan Liu*

Main category: cs.CL

TL;DR: JustRL 证明了通过简单的单阶段定参强化学习，也能在更低成本下超越复杂的多阶段大模型推理训练方案。


<details>
  <summary>Details</summary>
Motivation: 质疑当前大模型强化学习日益增加的复杂性（如多阶段流水线、动态超参数等）是否真的必要。

Method: 提出 JustRL 框架，采用单阶段训练（Single-stage training）和固定超参数，不使用动态调度或课程学习。

Result: 在两个 1.5B 推理模型上达到 SOTA 性能（54.9% 和 64.3% 均分），节省 2 倍计算资源，且训练过程极其平稳，无需传统复杂干预。

Conclusion: 当前的复杂训练技术可能并非必要，建立一个稳定、规模化的基线足以解决许多被认为需要复杂干预的问题。

Abstract: Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \textbf{Is this complexity necessary?} We present \textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\% and 64.3\% average accuracy across nine mathematical benchmarks) while using 2$\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.

</details>


### [19] [GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation](https://arxiv.org/abs/2512.16770)
*William English,Chase Walker,Dominic Simon,Rickard Ewetz*

Main category: cs.CL

TL;DR: 本文提出 GinSign 框架，通过将自然语言映射到系统签名的任务分解为层次化分类，显著提升了自然语言向接地时序逻辑翻译的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言转时序逻辑（NL-to-TL）框架通常假设已知准确的原子接地，或者在实际接地翻译精度上表现较差，限制了其在构建可靠自主系统中的应用。

Method: 提出了 GinSign 框架，将接地（Grounding）任务分解为层次化分类结构：首先预测谓词标签，然后选择类型匹配的常量参数。该方法使用较小的掩码语言模型（MLM）替代昂贵的大语言模型。

Result: GinSign 在多个领域达到了 95.5% 的接地逻辑等价得分，相比现有最先进技术（SOTA）提升了 1.4 倍，并解决了以往模型虽能生成语法正确但语义不一致的逻辑式的问题。

Conclusion: GinSign 证明了通过层次化分类而非自由文本生成来处理原子命题映射，是提升时序逻辑翻译实用性的有效途径，特别是在资源受限或需要高精度的工业场景中。

Abstract: Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\%$, a $1.4\times$ improvement over SOTA.

</details>


### [20] [From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs](https://arxiv.org/abs/2512.16795)
*Shubham Mishra,Samyek Jain,Gorang Mehrishi,Shiv Tiwari,Harsh Sharma,Pratik Narang,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文通过引入结构化推理步骤和 CATS 评估体系，解决了 RAG 系统在检索信息冲突时的决策难题，显著提高了回答的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的 RAG 系统在处理检索来源冲突、过期或主观信息时表现不佳，且缺乏统一的推理监督机制。

Method: 提出一种推理轨迹增强的 RAG 框架，包含文档评判、冲突分析和基准综合三个阶段，并开发了 CATS 评估流水线及包含 539 个查询的推理数据集。

Result: 实验显示模型性能大幅提升，尤其是 Qwen 模型在微调后，端到端回答准确率从 0.069 提升至 0.883，行为一致性从 0.074 提升至 0.722。

Conclusion: 在 RAG 中引入结构化推理链和冲突感知评估，可以显著提升模型在处理复杂外部知识冲突时的可靠性与决策透明度。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.

</details>


### [21] [Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology](https://arxiv.org/abs/2512.16802)
*Primož Kocbek,Azra Frkatović-Hodžić,Dora Lalić,Vivian Hui,Gordan Lauc,Gregor Štiglic*

Main category: cs.CL

TL;DR: 研究评估了糖生物学领域MM-RAG的两种路径，发现中型模型依赖图表转文本，而顶尖模型配合OCR-free视觉检索可实现同等性能，其中ColFlor检索器表现高效。


<details>
  <summary>Details</summary>
Motivation: 在视觉密集型的生物医学领域（如糖生物学），多模态检索增强生成（MM-RAG）应选择“将图表转为文本”还是“直接进行OCR-free视觉检索”尚不明确。

Method: 构建包含120道多选题的糖生物学基准数据集；对比四种增强方案（无、文本RAG、多模态转换、OCR-free视觉检索）；测试了Gemma-3、GPT-4o及GPT-5系列等多种模型及ColPali等检索器。

Result: Gemma-3-27B-IT在文本/多模态转换方案表现远优于视觉检索（0.74 vs 0.51）；而GPT-4o和GPT-5系列中，视觉检索方案能达到与文本转换相当的性能（~0.82-0.83），且ColFlor在保持高性能的同时更具轻量化优势。

Conclusion: 多模态RAG系统的效能具有模型容量依赖性。对于中型开源模型，文本转换（Multi-modal conversion）更稳定；而对于顶级闭源模型，OCR-free视觉检索（如ColPali）已展现出竞争力且更具效率。

Abstract: Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [22] [Information theory and discriminative sampling for model discovery](https://arxiv.org/abs/2512.16000)
*Yuxuan Bao,J. Nathan Kutz*

Main category: cs.IT

TL;DR: 本文通过将费舍尔信息矩阵（FIM）与 SINDy 框架结合，提出了一套量化动力系统信息价值的方法，旨在通过精简采样提高非线性系统建模的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动的模型发现中，如何量化不同数据片段对学习系统动力学的贡献，并利用这些指标优化采样策略以减少数据需求并提高模型性能。

Method: 将费舍尔信息矩阵（FIM）引入稀疏非线性动力学识别（SINDy）框架，结合统计装袋（bagging）、频谱分析和香农嫡，通过可视化手段分析不同系统轨迹的信息模式。

Result: 证明了信息度量能有效识别高价值采样区域，提高采样效率；通过 FIM 频谱分析阐明了 Bagging 的优势，并在单轨迹、参数可调及多初始条件三种场景下显著提升了数据利用率。

Conclusion: 基于信息的采样策略（特别是利用 FIM）是提高数据驱动模型发现效率和稳健性的有效手段，为处理复杂动力系统提供了理论支撑。

Abstract: Fisher information and Shannon entropy are fundamental tools for understanding and analyzing dynamical systems from complementary perspectives. They can characterize unknown parameters by quantifying the information contained in variables, or measure how different initial trajectories or temporal segments of a trajectory contribute to learning or inferring system dynamics. In this work, we leverage the Fisher Information Matrix (FIM) within the data-driven framework of {\em sparse identification of nonlinear dynamics} (SINDy). We visualize information patterns in chaotic and non-chaotic systems for both single trajectories and multiple initial conditions, demonstrating how information-based analysis can improve sampling efficiency and enhance model performance by prioritizing more informative data. The benefits of statistical bagging are further elucidated through spectral analysis of the FIM. We also illustrate how Fisher information and entropy metrics can promote data efficiency in three scenarios: when only a single trajectory is available, when a tunable control parameter exists, and when multiple trajectories can be freely initialized. As data-driven model discovery continues to gain prominence, principled sampling strategies guided by quantifiable information metrics offer a powerful approach for improving learning efficiency and reducing data requirements.

</details>


### [23] [Optimal Key Rates for Decentralized Secure Aggregation with Arbitrary Collusion and Heterogeneous Security Constraints](https://arxiv.org/abs/2512.16112)
*Zhou Li,Xiang Zhang,Giuseppe Caire*

Main category: cs.IT

TL;DR: 本文研究了具有异构安全约束的去中心化安全聚合，通过解决线性规划问题，确定了在各种安全限制下所需的最少密钥数量和通信成本。


<details>
  <summary>Details</summary>
Motivation: 传统去中心化安全聚合（DSA）为防御固定数量的合谋者往往需要极大的密钥开销，本研究旨在通过异构安全约束模型降低这种密钥负担。

Method: 定义了任意安全集（Security Set）与合谋集（Collusion Set），通过信息论建模，并利用线性规划（Linear Program）来刻画最优传输速率和密钥速率。

Result: 成功表征了在任意合谋和异构安全约束下的最优通信速率和最优源密钥速率（Source Key Rate）。

Conclusion: 该研究不仅解决了异构安全约束下的通信效率问题，还通过线性规划方法为安全聚合中的密钥资源分配提供了理论最优解。

Abstract: Decentralized secure aggregation (DSA) considers a fully-connected network of $K$ users, where each pair of users can communicate bidirectionally over an error-free channel. Each user holds a private input, and the goal is for each user to compute the sum of all inputs without revealing any additional information, even in the presence of collusion among up to $T$ users. Traditional DSA typically requires large key sizes to protect all information except for the input sum and the information of colluding users. To mitigate the source keys overhead, we study decentralized secure aggregation with arbitrary collusion and heterogeneous security constraints. In this setting, the inputs of a predefined collection of user subsets, called the \emph{security set} $\bm{\mathcal{S}}$, must be protected from another predefined collection, the \emph{collusion set} $\bm{\mathcal{T}}$. For an arbitrary security set $\mathcal{S}\in \bm{\mathcal{S}}$ and an arbitrary collusion set $\mathcal{T}\in \bm{\mathcal{T}}$, we characterize the optimal communication and source key rates. A key contribution of this work is the characterization of the optimal source key rate, i.e., the minimum number of key bits per input bit that must be shared among users for decentralized secure aggregation with arbitrary collusion and heterogeneous security constraints to be feasible. In general, this characterization reduces to solving a linear program.

</details>


### [24] [New Quantum Stabilizer Codes from generalized Monomial-Cartesian Codes constructed using two different generalized Reed-Solomon codes](https://arxiv.org/abs/2512.16482)
*Oisin Campion,Fernando Hernando,Gary McGuire*

Main category: cs.IT

TL;DR: 本文提出了一种新型的广义单项式笛卡尔码（GMCC），通过研究其自正交性质，成功构造了新的量子纠错码。


<details>
  <summary>Details</summary>
Motivation: 旨在扩展广义里德-所罗门码的理论框架，并探索其在量子码构造领域的应用。

Method: 定义了广义单项式笛卡尔码（GMCC），并展示了如何通过结合两个不同的广义里德-所罗门（Reed-Solomon）码来构造一个新的GMCC。

Result: 确定了使GMCC满足埃尔米特自正交性的充分条件，并基于此提出了一系列新的量子码构造方案。

Conclusion: 证明了GMCC在特定条件下具备埃尔米特自正交性，是构造高性能量子纠错码的有效工具。

Abstract: In this work, we define Generalized Monomial Cartesian Codes (GMCC), which constitute a natural extension of generalized Reed-Solomon codes. We describe how two different generalized Reed-Solomon codes can be combined to construct one GMCC. We further establish sufficient conditions ensuring that the GMCC are Hermitian self-orthogonal, thus leading to new constructions of quantum codes.

</details>


### [25] [Novel Inconsistency Results for Partial Information Decomposition](https://arxiv.org/abs/2512.16662)
*Philip Hendrik Matthias,Abdullah Makkeh,Michael Wibral,Aaron J. Gutknecht*

Main category: cs.IT

TL;DR: 本研究证明了信息论中的三个基本属性在部分信息分解（PID）框架下是互不兼容的，揭示了构建PID理论时必须在这些核心原则间进行取舍。


<details>
  <summary>Details</summary>
Motivation: PID领域缺乏统一标准，存在多种竞争方案。作者旨在通过证明某些直觉上吸引人的公理无法同时满足（即不相容性结果），来澄清PID的理论边界。

Method: 利用最近发展的整体论（Mereological）方法来研究PID，并通过数学证明建立不同公理组合之间的不相容性。

Result: 证明了非负性（Non-negativity）、链式法则（Chain rule）和可逆变换不变性（Invariance under invertible transformations）这三个核心属性在PID背景下互不兼容；同时加强了关于非负性、恒等属性与Williams-Beer公理不共存的经典结论。

Conclusion: 任何PID框架都必须在信息论的基本属性（如非负性、链式法则和不变性）之间做出权衡，不存在完美的通用解。

Abstract: Partial Information Decomposition (PID) seeks to disentangle how information about a target variable is distributed across multiple sources, separating redundant, unique, and synergistic contributions. Despite extensive theoretical development and applications across diverse fields, the search for a unique, universally accepted solution remains elusive, with numerous competing proposals offering different decompositions. A promising but underutilized strategy for making progress is to establish inconsistency results, proofs that certain combinations of intuitively appealing axioms cannot be simultaneously satisfied. Such results clarify the landscape of possibilities and force us to recognize where fundamental choices must be made. In this work, we leverage the recently developed mereological approach to PID to establish novel inconsistency results with far-reaching implications. Our main theorem demonstrates that three cornerstone properties of classical information theory, namely non-negativity, the chain rule, and invariance under invertible transformations, become mutually incompatible when extended to the PID setting. This result reveals that any PID framework must sacrifice at least one property that seems fundamental to information theory itself. Additionally, we strengthen the classical result of Rauh et al., which showed that non-negativity, the identity property, and the Williams and Beer axioms cannot coexist.

</details>


### [26] [Secure Event-triggered MolecularvCommunication - Information Theoretic Perspective and Optimal Performance](https://arxiv.org/abs/2512.16761)
*Wafa Labidi,Vida Gholamian,Yaning Zhao,Christian Deppe,Holger Boche*

Main category: cs.IT

TL;DR: 本研究将识别（ID）框架应用于分子通信中的泊松信道，提出了比传统通信更高效、更安全的编码方案，并推导了其在安全环境下的通信容量极限。


<details>
  <summary>Details</summary>
Motivation: 传统香农通信指标不适用于事件驱动的分子通信，且分子通信（如生物纳米物联网）对能耗、硬件效率及通信安全性（防窃听）有极高要求。

Method: 采用了 Ahlswede 和 Dueck 的识别框架，建立离散时间泊松信道（DTPC）模型，分别针对随机识别（RI）和安全随机识别（SRI）进行数学建模与容量公式推导。

Result: 推导出了 DTPC 下随机识别（RI）和安全随机识别（SRI）的容量公式，证明了识别码的大小随块长度呈双指数增长。

Conclusion: 识别（Identification）框架相比传统的香农传输模式，在处理离散时间泊松信道时表现出显著的效率优势，更符合分子通信的事件驱动特性。

Abstract: Molecular Communication (MC) is an emerging field of research focused on understanding how cells in the human body communicate and exploring potential medical applications. In theoretical analysis, the goal is to investigate cellular communication mechanisms and develop nanomachine-assisted therapies to combat diseases. Since cells transmit information by releasing molecules at varying intensities, this process is commonly modeled using Poisson channels. In our study, we consider a discrete-time Poisson channel (DTPC). MC is often event-driven, making traditional Shannon communication an unsuitable performance metric. Instead, we adopt the identification framework introduced by Ahlswede and Dueck. In this approach, the receiver is only concerned with detecting whether a specific message of interest has been transmitted. Unlike Shannon transmission codes, the size of identification (ID) codes for a discrete memoryless channel (DMC) increases doubly exponentially with blocklength when using randomized encoding. This remarkable property makes the ID paradigm significantly more efficient than classical Shannon transmission in terms of energy consumption and hardware requirements. Another critical aspect of MC, influenced by the concept of the Internet of Bio-NanoThings, is security. In-body communication must be protected against potential eavesdroppers. To address this, we first analyze the DTPC for randomized identification (RI) and then extend our study to secure randomized identification (SRI). We derive capacity formulas for both RI and SRI, providing a comprehensive understanding of their performance and security implications.

</details>


### [27] [An Extension of Enumerative Sphere Shaping for Arbitrary Channel Input Distributions](https://arxiv.org/abs/2512.16808)
*Frederik Ritter,Andrej Rode,Laurent Schmalen*

Main category: cs.IT

TL;DR: 本文提出了一种广义枚举球面整形（ESS）算法，通过引入可变权重实现了任意离散输入分布的生成，成功提升了非高斯通道下的数据传输速率。


<details>
  <summary>Details</summary>
Motivation: 传统的 ESS 算法产生的符号分布是固定的，仅适用于类高斯分布的通道。为了提升在非高斯容量最优输入分布通道（如光纤链路）上的性能，需要一种能生成任意分布的整形算法。

Method: 通过将 ESS 内部使用的固定权重替换为依赖于目标通道输入分布的权重，将枚举球面整形（ESS）推广到支持任意离散分布。

Result: 在 256 个符号的非放大相干光链路模拟中，广义 ESS 在帧错误率（FER）低于 10⁻⁴ 的情况下，比恒定成分分布匹配（CCDM）算法的传输速率提高了 0.0425 bit/symbol。

Conclusion: 广义 ESS (Generalized ESS) 能够产生任意离散通道输入分布，使其在大带宽或非高斯特性明显的通信场景中（如非放大相干光链路）具有极高的实用价值。

Abstract: A non-uniform channel input distribution is key for achieving the capacity of arbitrary channels. However, message bits are generally assumed to follow a uniform distribution which must first be transformed to a non-uniform distribution by using a distribution matching algorithm. One such algorithm is enumerative sphere shaping (ESS). Compared to algorithms such as constant composition distribution matching (CCDM), ESS can utilize more channel input symbol sequences, allowing it to achieve a comparably low rate loss. However, the distribution of channel input symbols produced by ESS is fixed, restricting the utility of ESS to channels with Gaussian-like capacity-achieving input distributions. In this paper, we generalize ESS to produce arbitrary discrete channel input distributions, making it usable on most channels. Crucially, our generalization replaces fixed weights used internally by ESS with weights depending on the desired channel input distribution. We present numerical simulations using generalized ESS with probabilistic amplitude shaping (PAS) to transmit sequences of 256 symbols over a simplified model of an unamplified coherent optical link, a channel with a distinctly non-Gaussian capacity-achieving input distribution. In these simulations, we found that generalized ESS improves the maximum transmission rate by 0.0425 bit/symbol at a frame error rate below 10^{-4} compared to CCDM.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [28] [BayesSum: Bayesian Quadrature in Discrete Spaces](https://arxiv.org/abs/2512.16105)
*Sophia Seulkee Kang,François-Xavier Briol,Toni Karvonen,Zonghao Chen*

Main category: stat.ML

TL;DR: 本文提出 BayesSum 算法，利用高斯过程先验将贝叶斯正交引入离散空间，旨在用更少的样本更高效地估计难解期望。


<details>
  <summary>Details</summary>
Motivation: 现有的蒙特卡罗（MC）和俄罗斯轮盘赌估计器虽然具有一致性，但在处理离散域上的难解期望时，通常需要大量样本才能获得准确结果，计算效率低下。

Method: 提出 BayesSum 估计器，将贝叶斯正交（Bayesian Quadrature）扩展到离散域，并利用高斯过程（Gaussian Process）对被积函数建模以合并先验信息。

Result: 理论证明其收敛速度在多种设定下显著快于蒙特卡罗方法；实验表明在合成环境及 Potts 模型等参数估计任务中，该方法所需的样本量更少。

Conclusion: BayesSum 为离散域期望估计提供了一种高样本效率的新方案，通过结合先验知识克服了传统采样方法的局限性。

Abstract: This paper addresses the challenging computational problem of estimating intractable expectations over discrete domains. Existing approaches, including Monte Carlo and Russian Roulette estimators, are consistent but often require a large number of samples to achieve accurate results. We propose a novel estimator, \emph{BayesSum}, which is an extension of Bayesian quadrature to discrete domains. It is more sample efficient than alternatives due to its ability to make use of prior information about the integrand through a Gaussian process. We show this through theory, deriving a convergence rate significantly faster than Monte Carlo in a broad range of settings. We also demonstrate empirically that our proposed method does indeed require fewer samples on several synthetic settings as well as for parameter estimation for Conway-Maxwell-Poisson and Potts models.

</details>


### [29] [Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning](https://arxiv.org/abs/2512.16489)
*Seyda Betul Aydin,Holger Brandt*

Main category: stat.ML

TL;DR: 本文提出利用迁移学习（TL-TARNet）来优化个体治疗效应（ITE）的估计，证明了在目标样本量较小时，通过借鉴大规模源数据集的知识可以显著降低预测偏差和误差。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，机器学习模型通常需要大规模数据才能准确估计个体治疗效应（ITE），这限制了其在行为科学等只有小规模数据集领域的应用；此外，如何将现有知识有效推广到不同背景（外部有效性）也是一大挑战。

Method: 采用了TL-TARNet方法，即基于Treatment Agnostic Representation Networks (TARNet) 进行迁移学习。通过在大型源数据集上预训练，再将知识迁移到较小的目标数据集，并针对随机和非随机干预情境进行了仿真实验和实证分析。

Result: 仿真结果显示，当源数据量大且无偏、目标样本较小时，TL-TARNet显著降低了误差并减少了偏差。实证应用（母职劳动时间对子女学习的影响）表明，迁移学习能将目标估计值拉向源估计值，有效校正了小样本偏差。

Conclusion: 迁移学习是增强因果模型外部有效性的有效手段，尤其能解决小样本环境下的个体治疗效应估计难题，提高结果的可预测性和准确性。

Abstract: Generalizing causal knowledge across diverse environments is challenging, especially when estimates from large-scale datasets must be applied to smaller or systematically different contexts, where external validity is critical. Model-based estimators of individual treatment effects (ITE) from machine learning require large sample sizes, limiting their applicability in domains such as behavioral sciences with smaller datasets. We demonstrate how estimation of ITEs with Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017) can be improved by leveraging knowledge from source datasets and adapting it to new settings via transfer learning (TL-TARNet; Aloui et al., 2023). In simulations that vary source and sample sizes and consider both randomized and non-randomized intervention target settings, the transfer-learning extension TL-TARNet improves upon standard TARNet, reducing ITE error and attenuating bias when a large unbiased source is available and target samples are small. In an empirical application using the India Human Development Survey (IHDS-II), we estimate the effect of mothers' firewood collection time on children's weekly study time; transfer learning pulls the target mean ITEs toward the source ITE estimate, reducing bias in the estimates obtained without transfer. These results suggest that transfer learning for causal models can improve the estimation of ITE in small samples.

</details>


### [30] [On The Hidden Biases of Flow Matching Samplers](https://arxiv.org/abs/2512.16768)
*Soon Hoe Lim*

Main category: stat.ML

TL;DR: 本文研究了经验流匹配的隐式偏置，发现其在结构上由于缺乏梯度场属性而导致能量次优，且生成动力学特征更多取决于初始源分布而非目标数据。


<details>
  <summary>Details</summary>
Motivation: 尽管总体流匹配倾向于产生类似最优传输的梯度场速度，但实际应用中基于有限样本的经验流匹配表现出的结构性偏置和能量次优性尚未得到充分研究。

Method: 通过经验流匹配（Empirical Flow Matching）的数学视角，分析最小化器的结构属性，并对比不同源分布（高斯与长尾分布）下的动能演化。

Result: 1. 即使条件流是梯度场，经验FM最小化器也几乎从不是梯度场；2. 生成样本的瞬时和累积动能受源分布支配，高斯源呈指数级集中，而长尾源导致多项式尾部分布。

Conclusion: Flow Matching 的经验偏差主要表现为破坏梯度场结构及由源分布主导的能量分布，而非仅由数据分布决定。

Abstract: We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Hybrid Quantum-Classical Ensemble Learning for S\&P 500 Directional Prediction](https://arxiv.org/abs/2512.15738)
*Abraham Itzhak Weinberg*

Main category: cs.LG

TL;DR: 本文通过结合量子情绪分析、Decision Transformer及多样化集成学习策略，将S&P 500方向预测准确率提升至60.14%，并证明了架构多样性在量化投资中的核心价值。


<details>
  <summary>Details</summary>
Motivation: 金融市场预测受高噪声、非平稳性和市场效率影响，准确率难以突破55%-57%，且现有研究往往忽视了架构多样性对模型性能的提升作用。

Method: 提出一种混合集成框架：1) 集成多样化架构（如Decision Transformer, LSTM, XGBoost等）；2) 引入4位变分量子电路进行情绪分析；3) 实施智能筛选机制，排除准确率低于52%的弱预测器。

Result: 在S&P 500预测中达到60.14%的准确率，较单一模型提升3.10%；架构多样性优于数据集多样性（60.14% vs 52.80%）；Sharpe比率达1.2，优于基准的0.8。

Conclusion: 该混合集成框架为高噪声金融市场预测提供了有效的解决方案，统计显著性分析和回测结果均证明了其在实际交易中的潜力。

Abstract: Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\% directional accuracy on S\&P 500 prediction, a 3.10\% improvement over individual models.
  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\% vs.\ 52.80\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\% to +1.5\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\%$), improving ensemble performance (Top-7 models: 60.14\% vs.\ all 35 models: 51.2\%).
  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.

</details>


### [32] [How Do Graph Signals Affect Recommendation: Unveiling the Mystery of Low and High-Frequency Graph Signals](https://arxiv.org/abs/2512.15744)
*Feng Liu,Hao Cang,Huanhuan Yuan,Jiaqing Fan,Yongjing Hao,Fuzhen Zhuang,Guanfeng Liu,Pengpeng Zhao*

Main category: cs.LG

TL;DR: 本文通过理论证明了在推荐任务中，高频信号与低频信号由于都能平滑相似度而具有同等重要性，并提出了频率缩放器和空间翻转方法来增强模型表现。


<details>
  <summary>Details</summary>
Motivation: 旨在厘清低频和高频图信号在推荐系统中的具体作用，挑战“推荐系统仅依赖低通滤波”的传统认知。

Method: 1. 提出频率信号缩放器（Frequency Signal Scaler），通过调整滤波函数来微调用户-项目对的平滑度。2. 提出空间翻转（Space Flip）方法，解决嵌入学习无法完全捕获信号特征的问题。

Result: 理论证明了高低频信号在增强用户-项目相似度平滑性上的等效作用；实验表明所提方法在四个公开数据集上均具有显著提升。

Conclusion: 低频和高频图信号在推荐系统中具有等效性，仅使用其中之一即可实现有效的推荐；同时，引入空间翻转（Space Flip）能显著提升嵌入表示的表达能力。

Abstract: Spectral graph neural networks (GNNs) are highly effective in modeling graph signals, with their success in recommendation often attributed to low-pass filtering. However, recent studies highlight the importance of high-frequency signals. The role of low-frequency and high-frequency graph signals in recommendation remains unclear. This paper aims to bridge this gap by investigating the influence of graph signals on recommendation performance. We theoretically prove that the effects of low-frequency and high-frequency graph signals are equivalent in recommendation tasks, as both contribute by smoothing the similarities between user-item pairs. To leverage this insight, we propose a frequency signal scaler, a plug-and-play module that adjusts the graph signal filter function to fine-tune the smoothness between user-item pairs, making it compatible with any GNN model. Additionally, we identify and prove that graph embedding-based methods cannot fully capture the characteristics of graph signals. To address this limitation, a space flip method is introduced to restore the expressive power of graph embeddings. Remarkably, we demonstrate that either low-frequency or high-frequency graph signals alone are sufficient for effective recommendations. Extensive experiments on four public datasets validate the effectiveness of our proposed methods. Code is avaliable at https://github.com/mojosey/SimGCF.

</details>


### [33] [Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic](https://arxiv.org/abs/2512.15765)
*Mélissa Tamine,Otmane Sakhi,Benjamin Heymann*

Main category: cs.LG

TL;DR: 本文提出了一种利用 DPO 算法特性来高效计算大语言模型数据价值（Shapley Value）的方法，解决了大规模模型数据贡献度评估难、成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 训练 LLM 需要大量高质量私有数据，但数据所有者面临如何决策投资策略以及如何公平分配协作训练收益的难题。现有的数据估值方法（如 Shapley Value）计算开销过大，不适用于大规模模型。

Method: 利用直接偏好优化（DPO）算法的特定数学结构，简化了传统 Shapley Value 计算中需要多次重新训练模型的极高复杂度，实现了可扩展的数据估值计算。

Result: 证明了在 DPO 框架下，计算数据贡献度的 Shapley Value 在计算上是切实可行的，克服了传统方法在 LLM 规模下的性能瓶颈。

Conclusion: DPO 的数学结构为 LLM 时代的复杂数据估值问题提供了一个高效且可扩展的解决方案，为资源池化和投资决策提供了理论支持。

Abstract: Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.

</details>


### [34] [LLaDA2.0: Scaling Up Diffusion Language Models to 100B](https://arxiv.org/abs/2512.15745)
*Tiwei Bie,Maosong Cao,Kun Chen,Lun Du,Mingliang Gong,Zhuochen Gong,Yanmei Gu,Jiaqi Hu,Zenan Huang,Zhenzhong Lan,Chengxi Li,Chongxuan Li,Jianguo Li,Zehuan Li,Huabin Liu,Ling Liu,Guoshan Lu,Xiaocheng Lu,Yuxin Ma,Jianfeng Tan,Lanning Wei,Ji-Rong Wen,Yipeng Xing,Xiaolu Zhang,Junbo Zhao,Da Zheng,Jun Zhou,Junlin Zhou,Zhanchao Zhou,Liwang Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: LLaDA2.0 通过创新的三阶段训练方案，将预训练自回归模型成功转化为高达 100B 参数的离散扩散语言模型，实现了高效且高性能的并行解码能力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决从头训练大规模离散扩散模型（dLLM）成本过高的问题，通过继承预训练自回归（AR）模型的知识，实现高效的扩散模型转化。

Method: 提出一种三阶段块级 WSD 训练方案（从渐进增加块大小开始，经过全序列扩散，最后回归紧凑块扩散），并辅以 SFT 和 DPO 后的指令微调。

Result: 成功推出 16B 和 100B 两个 MoE 版本模型；模型在保持并行解码优势的同时，在性能和效率上达到了前沿水平，并已开源。

Conclusion: LLaDA2.0 证明了通过系统性转换而非从头训练，可以成功构建前沿规模的离散扩散语言模型，为大模型的部署提供了新的范式。

Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.

</details>


### [35] [NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning](https://arxiv.org/abs/2512.16408)
*Ruifeng Xu,Liang He*

Main category: cs.LG

TL;DR: 本文提出一种嵌套双智能体强化学习方法（NDRL），通过结合宏观决策与细粒度胁迫因子感知，实现了棉花灌溉与氮肥施用的精准优化，显著提升了产量和资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有的作物产量优化研究面临水氮组合优化复杂度高、效果差，以及轻微胁迫信号量化困难和反馈延迟导致资源利用率低的问题。

Method: 提出一种嵌套双智能体强化学习（NDRL）方法：父智能体根据累积产量收益确定宏观水氮动作；子智能体结合量化的水/氮压力因子（WSF/NSF），利用混合概率分布动态优化每日策略。利用DSSAT模型与NDRL交互进行模拟实验。

Result: 与基准方法相比，2023和2024年的模拟产量均增长了4.7%；灌溉水效率分别提升5.6%和5.1%；氮肥偏生产力分别提升6.3%和1.0%。

Conclusion: NDRL方法为解决农业资源管理的复杂性和精确性问题提供了新思路，对于促进干旱地区棉花的可持续生产和资源高效利用具有重要意义。

Abstract: Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.

</details>


### [36] [A Unified Generative-Predictive Framework for Deterministic Inverse Design](https://arxiv.org/abs/2512.15746)
*Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: 本文提出 Janus 框架，通过在统一的隐空间内结合预测与生成，实现了高效、高精度的异构材料微结构逆向设计。


<details>
  <summary>Details</summary>
Motivation: 异构材料微结构的逆向设计面临高维设计空间、多模态输入以及非线性物理属性导致的病态问题和高昂计算成本，且现有生成模型往往缺乏快速稳定的确定性物理偏置。

Method: 提出名为 Janus 的统一生成-预测框架。该框架结合了编码器-解码器结构与独立的 KHRONOS 预测头，通过联合优化目标使隐流形（Latent Manifold）同时具备生成性能与物理表征能力。

Result: 在 MNIST 数据集上验证了高保真重构；在热导率材料设计任务中，前向预测 $R^2$ 达 0.98，逆向生成的材料属性误差低于 1%，且实现了极速的物理信息逆向生成。

Conclusion: Janus 框架通过物理预测与生成任务的深度耦合，不仅提高了逆向设计的精度和效率，还揭示了隐空间解耦（Disentanglement）在复杂材料建模中的核心作用。

Abstract: Inverse design of heterogeneous material microstructures is a fundamentally ill-posed and famously computationally expensive problem. This is exacerbated by the high-dimensional design spaces associated with finely resolved images, multimodal input property streams, and a highly nonlinear forward physics. Whilst modern generative models excel at accurately modeling such complex forward behavior, most of them are not intrinsically structured to support fast, stable \emph{deterministic} inversion with a physics-informed bias. This work introduces Janus, a unified generative-predictive framework to address this problem. Janus couples a deep encoder-decoder architecture with a predictive KHRONOS head, a separable neural architecture. Topologically speaking, Janus learns a latent manifold simultaneously isometric for generative inversion and pruned for physical prediction; the joint objective inducing \emph{disentanglement} of the latent space. Janus is first validated on the MNIST dataset, demonstrating high-fidelity reconstruction, accurate classification and diverse generative inversion of all ten target classes. It is then applied to the inverse design of heterogeneous microstructures labeled with thermal conductivity. It achieves a forward prediction accuracy $R^2=0.98$ (2\% relative error) and sub-5\% pixelwise reconstruction error. Inverse solutions satisfy target properties to within $1\%$ relative error. Inverting a sweep through properties reveal smooth traversal of the latent manifold, and UMAP visualization confirms the emergence of a low-dimensional, disentangled manifold. By unifying prediction and generation within a single latent space, Janus enables real-time, physics-informed inverse microstructure generation at a lower computational cost typically associated with classical optimization-based approaches.

</details>


### [37] [Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game](https://arxiv.org/abs/2512.16626)
*Barna Pásztor,Thomas Kleine Buening,Andreas Krause*

Main category: cs.LG

TL;DR: 本文提出 SLHF 框架，通过领导者-跟随者博弈进行偏好优化，实现了比 RLHF 和 NLHF 更强的鲁棒性、一致性以及无需微调的推理时模型改进。


<details>
  <summary>Details</summary>
Motivation: 传统的 RLHF 依赖标量奖励，而 NLHF 寻求同时博弈的纳什均衡，两者在处理非传递性偏好和推理时细化方面存在局限。需要一种能捕捉更丰富偏好结构的方法。

Method: 提出 Stackelberg 从人类反馈中学习 (SLHF)，将对齐建模为领导者（Leader）与跟随者（Follower）之间的博弈。Leader 先行动，Follower 根据 Leader 的行动进行条件响应（细化）。

Result: 实验显示 SLHF 在多个偏好数据集上表现强劲，支持 0.5B 到 8B 参数规模，且其推理时细化能力具有跨模型家族的通用性，无需额外微调。

Conclusion: SLHF 为大语言模型对齐提供了一种鲁棒且可扩展的新范式，尤其在处理复杂偏好和推理时优化方面表现优异。

Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.

</details>


### [38] [D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models](https://arxiv.org/abs/2512.15747)
*Javon Hickmon*

Main category: cs.LG

TL;DR: 提出一种名为 D3G 的推理阶段增强方法，通过生成多样化人口统计数据来提升分类精度并减少 CLIP 等多模态模型的预测偏见。


<details>
  <summary>Details</summary>
Motivation: 针对多模态模型在细粒度分类中表现不佳、训练数据存在人口统计偏差以及这类偏差导致零样本学习中产生不公平预测的问题。

Method: 提出 D3G（多样化人口统计数据生成）方法，结合 CLIP 视觉语言模型与 Stable Diffusion XL 生成模型，在推理阶段引入多样化的人口统计样本。

Result: 该方法在无需额外训练的情况下，成功提升了零样本图像分类的准确性，并有效减少了预训练模型中的人口统计偏见。

Conclusion: 在推理过程中提供多样化的人口统计数据，能有效提高模型性能并量化各个人口统计因素对准确性的影响。

Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.

</details>


### [39] [Provably Extracting the Features from a General Superposition](https://arxiv.org/abs/2512.15987)
*Allen Liu*

Main category: cs.LG

TL;DR: 本文提出了一种利用傅里叶空间迭代搜索的方法，从黑盒查询中恢复超完备线性叠加特征（$n > d$）及其响应函数，解决了复杂模型中特征重构的算法难题。


<details>
  <summary>Details</summary>
Motivation: 复杂机器学习模型通常通过线性表示来编码特征，但这些特征常以叠加（superposition）形式存在且难以恢复。特别是在“超完备”状态（特征数 $n$ 大于维度 $d$）以及响应函数 $σ$ 未知的情况下，如何从黑盒访问中恢复原始特征是一个极具挑战性的基础问题。

Method: 引入了一种在傅里叶空间（Fourier space）进行搜索的新算法。该算法通过迭代精细化搜索空间，定位隐藏的特征方向向量 $v_i$，并在此基础上通过噪声预言机访问（noisy oracle access）重建函数 $f$。

Result: 提出了一种高效的查询算法，能够识别所有非退化响应的特征方向。该算法在极具通用性的设置下有效：支持任意叠加（只需特征方向不几乎相同）以及通用的响应函数，突破了以往研究对特定分布或函数的限制。

Conclusion: 本研究证明了即使在复杂的超完备和非线性响应设置下，从黑盒访问中学习和分离线性表示特征也是计算可行的，为理解和解释复杂模型的内部机制提供了理论支持。

Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,σ_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $σ_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $σ_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.

</details>


### [40] [Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?](https://arxiv.org/abs/2512.15748)
*Tian Liu,Anwesha Basu,James Caverlee,Shu Kong*

Main category: cs.LG

TL;DR: 研究发现 LMMs 在特定物种识别上不如简单的 FSL 专家模型，但通过 POC 机制让 LMM 重新筛选专家模型的预测结果，可显著提升识别精度。


<details>
  <summary>Details</summary>
Motivation: 探讨大型多模态模型（LMMs）在高度专业的视觉物种识别（VSR）任务中是否优于基于少量样本学习（FSL）的专家模型，并寻求提升 VSR 准确率的新途径。

Method: 提出名为后验修正（Post-hoc Correction, POC）的方法：利用 LMM 对 FSL 专家模型给出的前几个预测候选进行重新排序。提示词中融入了 Softmax 置信度分数和少量标注样本。

Result: 在五个 VSR 基准测试中，POC 在无需额外训练的情况下，将 FSL 的准确率提升了 6.4%，且即便 LMMs 独立表现不佳，也能有效修正专家模型的错误。

Conclusion: LMMs 虽不能直接取代 FSL 专家模型，但通过 POC 框架与其结合，能显著提升 VSR 任务的性能，并具备良好的泛化性。

Abstract: Visual Species Recognition (VSR) is pivotal to biodiversity assessment and conservation, evolution research, and ecology and ecosystem management. Training a machine-learned model for VSR typically requires vast amounts of annotated images. Yet, species-level annotation demands domain expertise, making it realistic for domain experts to annotate only a few examples. These limited labeled data motivate training an ''expert'' model via few-shot learning (FSL). Meanwhile, advanced Large Multimodal Models (LMMs) have demonstrated prominent performance on general recognition tasks. It is straightforward to ask whether LMMs excel in the highly specialized VSR task and whether they outshine FSL expert models. Somewhat surprisingly, we find that LMMs struggle in this task, despite using various established prompting techniques. LMMs even significantly underperform FSL expert models, which are as simple as finetuning a pretrained visual encoder on the few-shot images. However, our in-depth analysis reveals that LMMs can effectively post-hoc correct the expert models' incorrect predictions. Briefly, given a test image, when prompted with the top predictions from an FSL expert model, LMMs can recover the ground-truth label. Building on this insight, we derive a simple method called Post-hoc Correction (POC), which prompts an LMM to re-rank the expert model's top predictions using enriched prompts that include softmax confidence scores and few-shot visual examples. Across five challenging VSR benchmarks, POC outperforms prior art of FSL by +6.4% in accuracy without extra training, validation, or manual intervention. Importantly, POC generalizes to different pretrained backbones and LMMs, serving as a plug-and-play module to significantly enhance existing FSL methods.

</details>


### [41] [CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting](https://arxiv.org/abs/2512.16046)
*Shu Wan,Reepal Shah,John Sabo,Huan Liu,K. Selçuk Candan*

Main category: cs.LG

TL;DR: 本文提出 CauStream 框架，通过学习动态的径流和汇流因果图，提升了径流预报的预测精度、长效泛化能力和物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习流控预报模型往往忽略物理过程，缺乏解释性；而现有的因果学习方法多依赖固定图结构，无法随数据动态调整。

Method: 提出 CauStream 框架，共同学习气象驱动因素间的径流因果图，以及捕获站点间动态依赖的汇流图，并建立了非参数设置下的结构可辨识性条件。

Result: 在三项美国主要流域实验中，模型性能优于现有最先进方法，且在长预报周期下优势更明显；学到的因果图与水文学专业知识高度吻合。

Conclusion: CauStream 为因果时空建模提供了原则性基础，具有扩展到其他科学和环境应用领域的潜力。

Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.

</details>


### [42] [Multivariate Uncertainty Quantification with Tomographic Quantile Forests](https://arxiv.org/abs/2512.16383)
*Takuya Kanazawa*

Main category: cs.LG

TL;DR: 本文提出了一种名为 TQF 的多变量树结构回归模型，通过学习方向投影并利用切片 Wasserstein 距离重构分布，实现了灵活、高效的多变量预测不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 在多变量回归任务中，为了实现安全可靠的 AI 部署，量化预测不确定性至关重要，但对多变量目标进行完全非参数化的条件分布估计仍然是一个巨大挑战。

Method: 提出断层扫描分位数森林（TQF）。该方法在单一模型中学习输入变量与不同方向投影之间的分位数关系，推理时通过最小化切片 Wasserstein 距离（Sliced Wasserstein distance），利用高效的交替凸优化方案重构多变量条件分布。

Result: TQF 无需对分位数区域的凸性做任何先验限制，且只需训练一个模型即可覆盖所有方向，在合成数据集和真实世界数据集上均表现出色。

Conclusion: TQF 为多输出回归提供了一个高效、非参数且具有不确定性感知能力的解决方案，克服了传统方法在凸性限制和重复训练方面的局限性。

Abstract: Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\mathbf{n}^{\top}\mathbf{y}$ as functions of the input $\mathbf{x}$ and the unit direction $\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.

</details>


### [43] [ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning](https://arxiv.org/abs/2512.15756)
*Yoonpyo Lee*

Main category: cs.LG

TL;DR: ReactorFold 利用生成式语言模型和 DPO 优化技术，实现了更具突破性的压水堆组件设计，能够自主超越人为约束并发现非对称的高性能布局。


<details>
  <summary>Details</summary>
Motivation: 传统的反应堆核心设计方法受限于固定的人为定义配置空间，难以跨越搜索瓶颈以发现根本性的新设计拓扑。

Method: 提出 ReactorFold 框架，将组件设计视为序列建模问题。利用蒙特卡罗数据、参数高效微调（PEFT）和直接偏好优化（DPO）对模型进行训练和对齐。

Result: 模型展现出涌现的设计空间扩展能力：虽受限于固定数量的可燃毒物（Gd）棒训练，却能自主调整 Gd 数量以满足功率峰值约束，并发现了打破传统对称性准则的高性能非对称配置。

Conclusion: 生成式语言模型能够内化物理规律，并能够超越传统对称性启发式设计的限制，发现更优的反应堆拓扑结构。

Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.

</details>


### [44] [Twin Restricted Kernel Machines for Multiview Classification](https://arxiv.org/abs/2512.15757)
*A. Quadir,M. Sajid,Mushir Akhtar,M. Tanveer*

Main category: cs.LG

TL;DR: 本文提出了一种新型多视图双限制核机 (TMvRKM)，通过正则化最小二乘法和跨视图耦合机制，显著提升了多视图分类的计算效率和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统多视图支持向量机（MvSVM）在处理高维空间决策边界、处理视图不一致性以及计算效率（解决大型二次规划问题）方面面临挑战。

Method: 提出多视图双限制核机 (TMvRKM)。通过正则化最小二乘法代替传统的大型二次规划问题（QPPs）来求解超平面，并在原目标函数中引入耦合项以平衡不同视图间的误差。

Result: 在 UCI、KEEL 和 AwA 等基准数据集上的实验结果和统计分析表明，TMvRKM 在各种场景下的泛化性能均优于基准模型。

Conclusion: TMvRKM 成功结合了早期和晚期融合策略，提供了一个在计算效率、鲁棒性和分类精度之间取得平衡的优越框架。

Abstract: Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.

</details>


### [45] [Yantra AI -- An intelligence platform which interacts with manufacturing operations](https://arxiv.org/abs/2512.15758)
*Varshini Krishnamurthy*

Main category: cs.LG

TL;DR: 本文为XRIT开发了一套集成机器学习模型与GPT-4虚拟助手的智能生产系统，旨在通过实时监控与AI决策优化工业生产效率。


<details>
  <summary>Details</summary>
Motivation: 应对工业4.0背景下智能制造中的能源管理、预测性维护及AI决策支持等关键挑战。

Method: 集成了随机森林（Random Forest）进行预测性维护，孤立森林（Isolation Forest）进行异常检测，并利用GPT-4构建AI虚拟助手，通过Streamlit实现实时数据可视化。

Result: 实验表明，该系统在提高工作效率、能源管理优化及维修规划能力方面表现出色，有效减少了停机时间。

Conclusion: 该系统显著提升了XRIT的操作决策能力与生产效率，未来将侧重于实时数据融合及系统的进一步优化。

Abstract: Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.

</details>


### [46] [Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning](https://arxiv.org/abs/2512.15759)
*Jahidul Arafat*

Main category: cs.LG

TL;DR: 本文提出SCFA框架，通过将领域知识约束融入联邦学习聚合过程，理论与实验同步证明了其在加速非IID数据收敛、降低模型分叉以及兼顾模型隐私与效用方面的显著优越性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非独立同分布（Non-IID）数据下收敛慢的问题，并克服现有算法忽视更新语义有效性的缺陷。

Method: 提出语义约束联邦聚合（SCFA）框架，通过引入基于知识图谱（ISA-95及MASON本体）的3000条领域知识约束进行分布式优化，并进行了收敛率分析和差异隐私实验。

Result: 在生产预测性维护场景下，SCFA使收敛速度提升22%，模型分叉减少41.3%，数据异构性降低41%；在差分隐私环境下，效用损失从12.1%降低至3.7%（提升2.7倍）。

Conclusion: SCFA框架为包含领域知识约束的联邦学习建立了理论基础，证明了通过约束减少数据异构性不仅能加速模型收敛，还能显著提升隐私保护下的模型效用。

Abstract: Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.

</details>


### [47] [Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps](https://arxiv.org/abs/2512.15761)
*Christopher Blum,Michael Neidlin*

Main category: cs.LG

TL;DR: 本研究开发了一个基于逻辑回归的可解释机器学习框架，通过物理流体特征准确预测旋转血栓泵内的血栓风险空间分布，显著提升了预测效率和机械透明度。


<details>
  <summary>Details</summary>
Motivation: 现有的计算模型难以将复杂的流动条件转化为可靠、可解释的旋转血栓泵血栓预测，且当前对特定流动特征如何导致血栓形成的机制理解尚不完整。

Method: 提出一种基于逻辑回归（LR）的可解释机器学习框架，结合结构化特征选择流程，从计算流体动力学（CFD）数据中提取物理特征（包括非线性组合），并利用已验证的宏观血栓模型生成的空间风险分布进行训练。

Result: 该模型成功识别了与血栓增加相关的特征集，即使在不同泵型（从轴流泵到离心泵）的迁移预测中也表现出合理性。其计算成本极低，可实现快速的血栓生成筛选。

Conclusion: 该框架证明了可解释性机器学习在连接局部流动特征与血栓风险方面的价值，为CFD驱动的医疗器械设计提供了高效、透明的决策支持工具。

Abstract: Thrombosis in rotary blood pumps arises from complex flow conditions that remain difficult to translate into reliable and interpretable risk predictions using existing computational models. This limitation reflects an incomplete understanding of how specific flow features contribute to thrombus initiation and growth. This study introduces an interpretable machine learning framework for spatial thrombosis assessment based directly on computational fluid dynamics-derived flow features. A logistic regression (LR) model combined with a structured feature-selection pipeline is used to derive a compact and physically interpretable feature set, including nonlinear feature combinations. The framework is trained using spatial risk patterns from a validated, macro-scale thrombosis model for two representative scenarios. The model reproduces the labeled risk distributions and identifies distinct sets of flow features associated with increased thrombosis risk. When applied to a centrifugal pump, despite training on a single axial pump operating point, the model predicts plausible thrombosis-prone regions. These results show that interpretable machine learning can link local flow features to thrombosis risk while remaining computationally efficient and mechanistically transparent. The low computational cost enables rapid thrombogenicity screening without repeated or costly simulations. The proposed framework complements physics-based thrombosis modeling and provides a methodological basis for integrating interpretable machine learning into CFD-driven thrombosis analysis and device design workflows.

</details>


### [48] [Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction](https://arxiv.org/abs/2512.15762)
*Kanxue Li,Yibing Zhan,Hua Jin,Chongchong Qi,Xu Lin,Baosheng Yu*

Main category: cs.LG

TL;DR: 本文提出 CSA-TTA 框架，通过跨样本增强和粗到细的检索策略，解决了术中低血压预测中测试样本稀缺导致的模型失真问题，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 术中低血压（IOH）具有高度的患者个体差异，而测试时自适应（TTA）常因 IOH 事件稀缺导致由于训练样本不足而预测不可靠。

Method: 提出 CSA-TTA 框架：1) 构建包含历史样本的跨样本库；2) 采用粗到细的检索策略（K-Shape 聚类+语义相似度检索）构建测试时训练数据；3) 结合自监督掩码重建与回顾性序列预测任务进行训练。

Result: 在 VitalDB 和院内数据集上的实验表明，相比最先进的基础模型（如 TimesFM、UniTS），CSA-TTA 在微调设定下 Recall/F1 提高约 1%，而在零样本设定下 Recall 提升高达 7.46%，F1 提升 5.07%。

Conclusion: CSA-TTA 为术中低血压预测提供了一种稳健且泛化能力强的解决方案，尤其是在零样本学习场景下表现突出，具有极高的临床应用潜力。

Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.

</details>


### [49] [TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration](https://arxiv.org/abs/2512.15773)
*Ye Li,Jiahe Feng,Yuan Meng,Kangye Ji,Chen Tang,Xinwan Wen,Shutao Xia,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: 本文提出 TS-DP，通过推测解码和强化学习调度器，在不损失精度的情况下显著提升了扩散策略在具身控制任务中的推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散策略（DP）因多次迭代去噪导致推理延迟高、计算开销大，且现有加速方法（如量化）难以应对具身任务中随时间变化的动态环境需求。

Method: 提出 TS-DP 框架：1) 蒸馏出一个轻量级的 Transformer 草图模型（Drafter）来模拟基础模型；2) 引入基于强化学习（RL）的调度器，根据任务难度动态调整推测参数。

Result: 在多种具身环境中，TS-DP 实现了高达 4.17 倍的推理加速，草图接受率超过 94%，推理频率达到 25 Hz，且未出现性能下降。

Conclusion: TS-DP 是首个将推测解码引入扩散策略的框架，证明了通过 RL 调度和模型蒸馏可以实现无损且实时（25 Hz）的具身智能控制。

Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.

</details>


### [50] [Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence](https://arxiv.org/abs/2512.15780)
*Samruddhi Baviskar*

Main category: cs.LG

TL;DR: 本文研究了对抗攻击对金融表格数据模型的影响，发现这些模型极易受扰动影响，而对抗训练虽有帮助但无法完全解决问题。


<details>
  <summary>Details</summary>
Motivation: 评估用于金融决策（如信用评分和欺诈检测）的表格机器学习模型在面对对抗性攻击时的稳健性及其对金融风险指标的影响。

Method: 在信用评分和欺诈检测数据集上，应用基于梯度的攻击（Gradient-based attacks），并结合对抗训练（Adversarial training）进行防御测试。

Result: 模型在微小扰动下表现出显著的性能下降；实验衡量了攻击对歧视性、校准度及财务风险指标的负面影响，并证实对抗训练可实现部分性能恢复。

Conclusion: 对抗训练能部分缓解脆弱性，但金融机构在部署此类模型时必须考虑针对特定业务风险的端到端稳健性评估。

Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.

</details>


### [51] [Boosting t-SNE Efficiency for Sequencing Data: Insights from Kernel Selection](https://arxiv.org/abs/2512.15900)
*Avais Jan,Prakash Chourasia,Sarwan Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 本研究评估了 t-SNE 的多种核函数，发现余弦相似度核在处理生物序列数据时，可视化效果、计算速度及后续挖掘性能均优于传统的高斯核。


<details>
  <summary>Details</summary>
Motivation: t-SNE 传统的高斯核缺乏数据依赖性且计算开销大，在处理类别型生物序列数据时扩展性和效果受限，因此需要探索更优的核函数。

Method: 研究对比了 9 种核函数在 3 种嵌入方法（One-Hot, Spike2Vec, minimizers）下的表现，并在 6 个多样化生物数据集上通过可视化、邻域保持度、分类和聚类实验进行全面评估。

Result: 余弦相似度核在运行效率和低维空间距离保持方面普遍优于高斯核和孤立核（Isolation Kernel），并在多个数据集的下游机器学习任务中表现出最强的稳健性。

Conclusion: 在生物序列分析中，基于余弦相似度的 t-SNE 应当作为优先于传统高斯核的选择，因为它在保持数据结构、计算效率和下游任务表现上具有显著优势。

Abstract: Dimensionality reduction techniques are essential for visualizing and analyzing high-dimensional biological sequencing data. t-distributed Stochastic Neighbor Embedding (t-SNE) is widely used for this purpose, traditionally employing the Gaussian kernel to compute pairwise similarities. However, the Gaussian kernel's lack of data-dependence and computational overhead limit its scalability and effectiveness for categorical biological sequences. Recent work proposed the isolation kernel as an alternative, yet it may not optimally capture sequence similarities. In this study, we comprehensively evaluate nine different kernel functions for t-SNE applied to molecular sequences, using three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Through both subjective visualization and objective metrics (including neighborhood preservation scores), we demonstrate that the cosine similarity kernel in general outperforms other kernels, including Gaussian and isolation kernels, achieving superior runtime efficiency and better preservation of pairwise distances in low-dimensional space. We further validate our findings through extensive classification and clustering experiments across six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer), employing multiple machine learning algorithms and evaluation metrics. Our results show that kernel selection significantly impacts not only visualization quality but also downstream analytical tasks, with the cosine similarity kernel providing the most robust performance across different data types and embedding strategies, making it particularly suitable for large-scale biological sequence analysis.

</details>


### [52] [Introduction to Symbolic Regression in the Physical Sciences](https://arxiv.org/abs/2512.15920)
*Deaglan J. Bartlett,Harry Desmond,Pedro G. Ferreira,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 本文是英国皇家学会关于“物理科学中的符号回归”特刊的总结综述，探讨了 SR 发现物理规律的潜力、现有挑战及未来与 AI 结合的方向。


<details>
  <summary>Details</summary>
Motivation: 旨在介绍“物理科学中的符号回归”特刊，探讨如何利用 SR 从物理数据中自动发现可解释的数学关系，解决复杂模拟及经验建模的难题。

Method: 通过综述特刊论文，回顾了 SR 的概念基础，对比了传统回归，并讨论了搜索空间设计、算子选择、复杂度控制及 AI 集成等核心技术。

Result: 展示了 SR 在自动方程发现、涌现现象建模、紧凑仿真模拟器构建等方面的广泛应用，并识别了在可扩展性、稳健性及对称性约束集成方面的挑战。

Conclusion: SR 正在从单纯的数据拟合工具演变为融合理论约束的科学发现助手，其在物理科学领域的应用正处于加速发展阶段。

Abstract: Symbolic regression (SR) has emerged as a powerful method for uncovering interpretable mathematical relationships from data, offering a novel route to both scientific discovery and efficient empirical modelling. This article introduces the Special Issue on Symbolic Regression for the Physical Sciences, motivated by the Royal Society discussion meeting held in April 2025. The contributions collected here span applications from automated equation discovery and emergent-phenomena modelling to the construction of compact emulators for computationally expensive simulations.
  The introductory review outlines the conceptual foundations of SR, contrasts it with conventional regression approaches, and surveys its main use cases in the physical sciences, including the derivation of effective theories, empirical functional forms and surrogate models. We summarise methodological considerations such as search-space design, operator selection, complexity control, feature selection, and integration with modern AI approaches. We also highlight ongoing challenges, including scalability, robustness to noise, overfitting and computational complexity. Finally we emphasise emerging directions, particularly the incorporation of symmetry constraints, asymptotic behaviour and other theoretical information. Taken together, the papers in this Special Issue illustrate the accelerating progress of SR and its growing relevance across the physical sciences.

</details>


### [53] [A Unification of Discrete, Gaussian, and Simplicial Diffusion](https://arxiv.org/abs/2512.15923)
*Nuria Alina Chandra,Yucen Lily Li,Alan N. Amin,Alex Ali,Joshua Rollins,Sebastian W. Ober,Aniruddh Raghu,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文通过群体遗传学中的 Wright-Fisher 模型统一了离散、高斯和单纯形扩散模型，解决了单纯形扩散的稳定性问题，并实现了单一模型跨领域的高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有的离散序列（如DNA、蛋白质、语言）扩散模型分为离散空间、高斯欧几里得空间和单纯形空间三类。它们算法和理论各异且存在权衡（如单纯形扩散虽理论优越但数值不稳定），缺乏统一框架。

Method: 提出将三种扩散方法视为同一底层过程——Wright-Fisher（群体遗传学模型）的不同参数化表现。其中单纯形和高斯扩散被视为大群体极限下的特例。同时，开发出可切换领域的统一样本训练方法。

Result: 实验表明，基于 Wright-Fisher 的单纯形扩散在 DNA 条件生成任务上比以往模型更稳定且性能更好。此外，训练出的单一模型在测试时可灵活在三种领域间切换，性能与专门针对单一领域训练的模型相当。

Conclusion: Wright-Fisher 理论成功统一了离散序列生成的扩散模型框架，解决了单纯形扩散的数值稳定性问题，并证明了多领域联合训练的可行性。

Abstract: To model discrete sequences such as DNA, proteins, and language using diffusion, practitioners must choose between three major methods: diffusion in discrete space, Gaussian diffusion in Euclidean space, or diffusion on the simplex. Despite their shared goal, these models have disparate algorithms, theoretical structures, and tradeoffs: discrete diffusion has the most natural domain, Gaussian diffusion has more mature algorithms, and diffusion on the simplex in principle combines the strengths of the other two but in practice suffers from a numerically unstable stochastic processes. Ideally we could see each of these models as instances of the same underlying framework, and enable practitioners to switch between models for downstream applications. However previous theories have only considered connections in special cases. Here we build a theory unifying all three methods of discrete diffusion as different parameterizations of the same underlying process: the Wright-Fisher population genetics model. In particular, we find simplicial and Gaussian diffusion as two large-population limits. Our theory formally connects the likelihoods and hyperparameters of these models and leverages decades of mathematical genetics literature to unlock stable simplicial diffusion. Finally, we relieve the practitioner of balancing model trade-offs by demonstrating it is possible to train a single model that can perform diffusion in any of these three domains at test time. Our experiments show that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial diffusion models on conditional DNA generation. We also show that we can train models on multiple domains at once that are competitive with models trained on any individual domain.

</details>


### [54] [DSO: Direct Steering Optimization for Bias Mitigation](https://arxiv.org/abs/2512.15926)
*Lucas Monteiro Paes,Nivedha Sivakumar,Yinong Oliver Wang,Masha Fedzechkina Donaldson,Luca Zappella,Nicholas Apostoloff*

Main category: cs.LG

TL;DR: 本文提出 DSO 方法，通过强化学习优化激活导向（Activation Steering），实现了在推理阶段可控地减少生成式模型的偏差，同时兼顾模型性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）等生成模型在决策时常受人口统计属性影响产生偏见，且用户在偏差减少与性能维持之间有不同的权衡需求。现有激活导向方法在处理需要各群体概率均等的偏差纠正时效果不佳。

Method: 提出 Direct Steering Optimization (DSO)，利用强化学习 (RL) 寻找激活值的线性变换，旨在减轻偏差的同时保持模型性能。

Result: DSO 在 VLM 和 LLM 上均实现了公平性与模型能力之间的最优权衡（SOTA trade-off），并允许在该权衡之间进行推理时控制。

Conclusion: DSO 证明了直接优化激活导向策略比依赖启发式方法更有效，为生成式模型提供了更灵活、更精确的偏差干预手段。

Abstract: Generative models are often deployed to make decisions on behalf of users, such as vision-language models (VLMs) identifying which person in a room is a doctor to help visually impaired individuals. Yet, VLM decisions are influenced by the perceived demographic attributes of people in the input, which can lead to biased outcomes like failing to identify women as doctors. Moreover, when reducing bias leads to performance loss, users may have varying needs for balancing bias mitigation with overall model capabilities, highlighting the demand for methods that enable controllable bias reduction during inference. Activation steering is a popular approach for inference-time controllability that has shown potential in inducing safer behavior in large language models (LLMs). However, we observe that current steering methods struggle to correct biases, where equiprobable outcomes across demographic groups are required. To address this, we propose Direct Steering Optimization (DSO) which uses reinforcement learning to find linear transformations for steering activations, tailored to mitigate bias while maintaining control over model performance. We demonstrate that DSO achieves state-of-the-art trade-off between fairness and capabilities on both VLMs and LLMs, while offering practitioners inference-time control over the trade-off. Overall, our work highlights the benefit of designing steering strategies that are directly optimized to control model behavior, providing more effective bias intervention than methods that rely on pre-defined heuristics for controllability.

</details>


### [55] [BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research](https://arxiv.org/abs/2512.15931)
*Tiancheng Gao,Scott C. Lowe,Brendan Furneaux,Angel X Chang,Graham W. Taylor*

Main category: cs.LG

TL;DR: 本文提出 BarcodeMamba+，结合 Mamba 架构与预训练技术，有效解决了真菌 DNA 条形码分类中数据稀疏和长尾分布的难题，实现了业内领先的分类准确度。


<details>
  <summary>Details</summary>
Motivation: 传统的监督学习在真菌 DNA 条形码分类中面临标签稀疏、长尾分布以及无法有效利用数据层次结构的问题，难以泛化至未知物种。

Method: 采用了预训练和微调（Pre-train & Fine-tune）范式，基于高效的状态空间模型（State-Space Model，Mamba）构建基础模型。在微调阶段集成了层次化标签平滑、加权损失函数以及多头输出层，以处理真菌分类的复杂性。

Result: 实验证明，BarcodeMamba+ 在具有显著分类分布偏移的真菌数据集上，各分类层级的性能均优于现有方法，且各增强组件均带来了显著的性能提升。

Conclusion: BarcodeMamba+ 为基因组学生物多样性研究提供了一个强大、可扩展的新工具，并为处理稀疏且具有层次结构的 DNA 条形码数据建立了有效的训练范范式。

Abstract: Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba.

</details>


### [56] [In-Context Semi-Supervised Learning](https://arxiv.org/abs/2512.15934)
*Jiashuo Fan,Paul Rosu,Aaron T. Wang,Michael Li,Lawrence Carin,Xiang Cheng*

Main category: cs.LG

TL;DR: 本文研究了 Transformer 的上下文半监督学习能力，揭示了其如何通过利用未标记数据来增强表征学习，从而在标签稀疏的情况下实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文学习（ICL）理论大多集中在有明确标签的监督场景，但在实际应用中，Transformer 在标签稀疏或缺失的情况下依然表现出色，其利用未标记数据内部结构的机制尚不明确。

Method: 引入并研究了“上下文半监督学习”（IC-SSL）框架，探讨在少量标签示例与大量未标记数据共存时，Transformer 如何利用未标记上下文进行推断。

Result: 研究发现 Transformer 能够利用未标记上下文学习到适应当前任务的鲁棒表征，显著提升了在极低标签量（low-label regimes）下的预测准确性和性能。

Conclusion: 证明了 Transformer 并非仅靠标签匹配，而是能通过未标记样本构建鲁棒的上下文表征（context-dependent representation），从而实现超越传统监督式 ICL 的泛化能力。

Abstract: There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.

</details>


### [57] [Governance by Evidence: Regulated Predictors in Decision-Tree Models](https://arxiv.org/abs/2512.15955)
*Alexios Veskoukis,Dimitris Kalles*

Main category: cs.LG

TL;DR: 本文通过分析已发表的决策树论文，揭示了该模型在实际应用中频繁涉及欧盟和美国隐私法管辖的敏感数据，强调了机器学习领域加强隐私保护和合规性检查的必要性。


<details>
  <summary>Details</summary>
Motivation: 决策树在结构化数据中广泛应用，但其使用的预测因子常涉及受隐私法监管的敏感信息。研究旨在评估学术研究中决策树使用受规管数据的现状及其法律风险。

Method: 通过收集已发表的决策树研究论文构建语料库，将提取的预测因子分类到受监管的数据类别（如健康、金融等），并将其与欧盟及美国的隐私法律条文进行比对关联。

Result: 大量报告的预测因子属于受监管范畴，其中医疗数据占比最高。不同行业的受监管程度存在显著差异，研究还总结了这些数据使用在不同监管框架下的时间分布模式。

Conclusion: 此项研究为需要在决策树算法中加强隐私保护措施提供了实证支持，建议从业者对照法规加强治理流程，其结论亦可推广至其他涉及敏感数据的机器学习实践。

Abstract: Decision-tree methods are widely used on structured tabular data and are valued for interpretability across many sectors. However, published studies often list the predictors they use (for example age, diagnosis codes, location). Privacy laws increasingly regulate such data types. We use published decision-tree papers as a proxy for real-world use of legally governed data. We compile a corpus of decision-tree studies and assign each reported predictor to a regulated data category (for example health data, biometric identifiers, children's data, financial attributes, location traces, and government IDs). We then link each category to specific excerpts in European Union and United States privacy laws. We find that many reported predictors fall into regulated categories, with the largest shares in healthcare and clear differences across industries. We analyze prevalence, industry composition, and temporal patterns, and summarize regulation-aligned timing using each framework's reference year. Our evidence supports privacy-preserving methods and governance checks, and can inform ML practice beyond decision trees.

</details>


### [58] [Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling](https://arxiv.org/abs/2512.15956)
*John Hateley,Sriram Narasimhan,Omid Abari*

Main category: cs.LG

TL;DR: 本文提出一种基于高斯过程和RFID技术的森林资产追踪方法，无需预设参考点即可达到GPS级的定位精度，具有低成本和高扩展性。


<details>
  <summary>Details</summary>
Motivation: 针对森林野火响应应用，解决商品化RFID系统在森林环境中因信号衰减、多径效应及环境多变导致的定位精度差问题，且无需像传统方法那样预先布置已知位置的参考标签。

Method: 利用高斯过程（Gaussian Process）基于射频信号特征对不同环境进行建模，并结合一种新型加权对数似然法，将未知环境与模型字典中最匹配的环境进行关联。

Result: 实现了与GPS相当的定位精度，支持同时追踪多个移动读写器附近的资产，且成本远低于GPS系统。

Conclusion: 在不需要预设已知位置标签的情况下，仅依靠射频响应即可实现高精度的环境自适应追踪。

Abstract: This paper presents a novel, cost-effective, and scalable approach to track numerous assets distributed in forested environments using commodity Radio Frequency Identification (RFID) targeting wildfire response applications. Commodity RFID systems suffer from poor tag localization when dispersed in forested environments due to signal attenuation, multi-path effects and environmental variability. Current methods to address this issue via fingerprinting rely on dispersing tags at known locations {\em a priori}. In this paper, we address the case when it is not possible to tag known locations and show that it is possible to localize tags to accuracies comparable to global positioning systems (GPS) without such a constraint. For this, we propose Gaussian Process to model various environments solely based on RF signal response signatures and without the aid of additional sensors such as global positioning GPS or cameras, and match an unknown RF to the closest match in a model dictionary. We utilize a new weighted log-likelihood method to associate an unknown environment with the closest environment in a dictionary of previously modeled environments, which is a crucial step in being able to use our approach. Our results show that it is possible to achieve localization accuracies of the order of GPS, but with passive commodity RFID, which will allow the tracking of dozens of wildfire assets within the vicinity of mobile readers at-a-time simultaneously, does not require known positions to be tagged {\em a priori}, and can achieve localization at a fraction of the cost compared to GPS.

</details>


### [59] [Higher-Order LaSDI: Reduced Order Modeling with Multiple Time Derivatives](https://arxiv.org/abs/2512.15997)
*Robert Stephany,William Michael Anderson,Youngsoo Choi*

Main category: cs.LG

TL;DR: 本文通过引入高阶差分方案和 Rollout 损失函数，解决了降阶模型在求解复杂 PDE 时长期预测精度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管降阶模型（ROMs）计算量小，但在处理参数化 PDE 时，其预测能力往往会随着时间演化而退化。

Method: 1) 引入灵活、高阶且低成本的有限差分策略；2) 提出 Rollout 损失函数，通过训练使模型在长序列预测中保持稳定性。

Result: 在二维 Burgers 方程上的实验证明，该方法能使 ROMs 在任意长的时间范围内产生准确的预测效果。

Conclusion: 结合高阶差分格式与跨步预测训练（Rollout loss）是提升偏微分方程降阶模型长期预测能力的有效途径。

Abstract: Solving complex partial differential equations is vital in the physical sciences, but often requires computationally expensive numerical methods. Reduced-order models (ROMs) address this by exploiting dimensionality reduction to create fast approximations. While modern ROMs can solve parameterized families of PDEs, their predictive power degrades over long time horizons. We address this by (1) introducing a flexible, high-order, yet inexpensive finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to make accurate predictions over arbitrary time horizons. We demonstrate our approach on the 2D Burgers equation.

</details>


### [60] [Topic Modelling Black Box Optimization](https://arxiv.org/abs/2512.16445)
*Roman Akramov,Artem Khamatullin,Svetlana Glazyrina,Maksim Kryzhanovskiy,Roman Ischenko*

Main category: cs.LG

TL;DR: 本文研究了 LDA 主题数选择的黑盒优化问题，实验证明学习型的摊销优化器（尤其是 SABBO）比传统进化算法在时间及样本效率上具有绝对优势。


<details>
  <summary>Details</summary>
Motivation: LDA 模型中主题数量 $T$ 的选择至关重要但极具挑战，作者旨在寻找更高效的 $T$ 值搜索方法，以平衡统计拟合度与可解释性。

Method: 将 $T$ 的选择建模为离散黑盒优化问题，通过验证集困惑度进行评估。对比了遗传算法 (GA)、进化策略 (ES) 两种人工设计方法，以及 PABBO 和 SABBO 两种摊销（amortized）优化方法。

Result: SABBO 表现最佳，仅需一次评估即可接近最优主题数；PABBO 紧随其后需数次评估；而 GA 和 ES 则需要几乎全部评估预算才能达到相近的困惑度水平。

Conclusion: 在 LDA 主题数选择任务中，基于学习的摊销优化器（PABBO 和 SABBO）在搜索效率上显著优于传统的进化算法。

Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.

</details>


### [61] [Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results](https://arxiv.org/abs/2512.16013)
*Ruolei Zeng,Arun Sharma,Shuai An,Mingzhou Yang,Shengya Zhang,Licheng Liu,David Mulla,Shashi Shekhar*

Main category: cs.LG

TL;DR: 本文提出了一种结合预训练、微调及知识引导机器学习（KGML）的新框架 FTBSC-KGML，旨在通过捕捉空间异质性来提升不同地区农田碳循环量化的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的农业生态系统碳循环量化方法往往忽略了空间异质性，且在利用迁移学习处理跨尺度、异构数据方面存在局限，导致在多变地区的适用性较差。

Method: 提出了 FTBSC-KGML 框架，将 KGML-ag 与预训练-微调过程相结合。该方案利用遥感 GPP、气候和土壤协变量进行全局预训练，随后在特定站点或州进行微调，以学习具有“位置感知”特征的表征。

Result: 实验证明，FTBSC-KGML 在验证误差上更低，解释力一致性更强。相比纯全局模型，它能更好地捕捉不同州之间的空间变异性，并在数据有限的情况下提高局部预测精度。

Conclusion: FTBSC-KGML 框架通过结合预训练-微调机制与空间感知参数，有效解决了农田碳循环量化中的数据稀疏与空间异质性问题，是对 SDSA-KGML 架构的有力扩展。

Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.

</details>


### [62] [Explainable AI in Big Data Fraud Detection](https://arxiv.org/abs/2512.16037)
*Ayush Jain,Rahul Kulkarni,Siyi Lin*

Main category: cs.LG

TL;DR: 本文探讨了在大数据欺诈检测中整合可解释AI（XAI）的挑战与方法，并提出了一个结合可扩展架构与上下文感知解释的框架。


<details>
  <summary>Details</summary>
Motivation: 尽管大数据机器学习系统在金融风险评估中至关重要，但其缺乏透明度和合规性，急需将可解释AI（XAI）集成到大规模分析流水线中。

Method: 通过文献综述分析大数据工具与欺诈检测模型，评估LIME、SHAP等XAI方法的局限性，并构建了一个集成大数据基础设施与人工反馈的架构。

Result: 识别了现有XAI在可扩展性、实时处理以及针对图模型和时间模型的解释性方面的关键研究空白。

Conclusion: 提出并展望了在大规模、实时处理环境下，结合隐私保护与标准化评估的XAI未来研究方向。

Abstract: Big Data has become central to modern applications in finance, insurance, and cybersecurity, enabling machine learning systems to perform large-scale risk assessments and fraud detection. However, the increasing dependence on automated analytics introduces important concerns about transparency, regulatory compliance, and trust. This paper examines how explainable artificial intelligence (XAI) can be integrated into Big Data analytics pipelines for fraud detection and risk management. We review key Big Data characteristics and survey major analytical tools, including distributed storage systems, streaming platforms, and advanced fraud detection models such as anomaly detectors, graph-based approaches, and ensemble classifiers. We also present a structured review of widely used XAI methods, including LIME, SHAP, counterfactual explanations, and attention mechanisms, and analyze their strengths and limitations when deployed at scale. Based on these findings, we identify key research gaps related to scalability, real-time processing, and explainability for graph and temporal models. To address these challenges, we outline a conceptual framework that integrates scalable Big Data infrastructure with context-aware explanation mechanisms and human feedback. The paper concludes with open research directions in scalable XAI, privacy-aware explanations, and standardized evaluation methods for explainable fraud detection systems.

</details>


### [63] [Impacts of Racial Bias in Historical Training Data for News AI](https://arxiv.org/abs/2512.16901)
*Rahul Bhargava,Malene Hornstrup Jespersen,Emily Boardman Ndulue,Vivica Dsouza*

Main category: cs.LG

TL;DR: 本文通过案例研究，揭示了基于历史新闻语料训练的AI模型如何携带种族偏见，警告新闻业在使用此类模型进行故事发现或摘要时，可能意外触发并传播历史性的歧视逻辑。


<details>
  <summary>Details</summary>
Motivation: 人工智能模型常编码历史偏见与刻板印象。研究者在计算新闻学应用中发现了一个令人担忧的“blacks”主题标签，需评估其编码逻辑及对实际应用的影响。

Method: 通过定量和定性的方法，并结合可解释人工智能（XAI）技术，对基于《纽约时报》语料库训练的多标签分类器中的“blacks”主题标签进行深入调查。

Result: 该标签部分充当了针对少数族裔的“种族主义探测器”，但在处理现代案例（如COVID-19期间的反亚裔仇恨新闻、BLM运动报道）时表现不佳，表现出明显的时代局限性和偏见偏离。

Conclusion: 在新闻编辑室采用人工智能工具时，必须解决如何在大规模利用自动化效率的同时，避免复制和放大历史偏见的基本矛盾。

Abstract: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning "blacks" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the "blacks" label operates partially as a general "racism detector" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.

</details>


### [64] [Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward](https://arxiv.org/abs/2512.16912)
*Peter Chen,Xiaopeng Li,Ziniu Li,Wotao Yin,Xi Chen,Tianyi Lin*

Main category: cs.LG

TL;DR: 本文揭示了RLVR中错误奖励如何通过降低策略熵提升LLM数学推理能力，并提出了奖励失配模型来解释这一看似矛盾的现象。


<details>
  <summary>Details</summary>
Motivation: 在带验证奖励的强化学习（RLVR）中，存在两个矛盾现象：惩罚剥削的错误奖励和抑制探索的熵最小化都能提升LLM推理性能，其内在机制尚不明确。

Method: 通过实证研究分析策略熵与性能的关系，并提出“奖励失配模型”（reward-misalignment model）来解释错误奖励如何通过剪切偏见和模型污染影响性能。

Result: 错误奖励通过剪切偏见强制降低策略熵，从而提高模型信心；在非污染环境下，这种奖励失配机制依然能提升推理表现。

Conclusion: 错误奖励带来的这种“剪切偏见”降低了策略熵，使模型输出更确定，这有助于在特定条件下提升推理性能，但单纯的熵最小化是不够的。

Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.

</details>


### [65] [In-Context Multi-Operator Learning with DeepOSets](https://arxiv.org/abs/2512.16074)
*Shao-Ting Chiu,Aditya Nambiar,Ali Syed,Jonathan W. Siegel,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: 本文提出并证明了 DeepOSets 架构具备上下文学习能力，无需权重更新即可求解训练中未见过的 PDE，并提供了科学机器学习领域的首个通用一致逼近理论证明。


<details>
  <summary>Details</summary>
Motivation: 探讨非 Transformer 架构（特别是针对集合和算子的 DeepOSets）是否能够实现上下文学习（ICL），并将其应用于求解未见过的偏微分方程（PDE）。

Method: 通过改进 DeepOSets 架构（结合 DeepSets 和 DeepONets），构建一个非自回归、非注意力的多算子学习器。

Result: 实验证明，DeepOSets 能够从提示词中的示例对中恢复新 PDE 的解算子，并在 Poisson 及反应扩散方程的前向/反向预测任务中表现出色；此外，理论证明其是连续算子类的通用一致逼近器。

Conclusion: DeepOSets 为科学计算中的上下文学习提供了一个强大且具有理论保证的非 Transformer 替代方案，特别适用于跨 PDE 任务的泛化。

Abstract: In-context Learning (ICL) is the remarkable capability displayed by some machine learning models to learn from examples in a prompt, without any further weight updates. ICL had originally been thought to emerge from the self-attention mechanism in autoregressive transformer architectures. DeepOSets is a non-autoregressive, non-attention based neural architecture that combines set learning via the DeepSets architecture with operator learning via Deep Operator Networks (DeepONets). In a previous study, DeepOSets was shown to display ICL capabilities in supervised learning problems. In this paper, we show that the DeepOSets architecture, with the appropriate modifications, is a multi-operator in-context learner that can recover the solution operator of a new PDE, not seen during training, from example pairs of parameter and solution placed in a user prompt, without any weight updates. Furthermore, we show that DeepOSets is a universal uniform approximator over a class of continuous operators, which we believe is the first result of its kind in the literature of scientific machine learning. This means that a single DeepOSets architecture exists that approximates in-context any continuous operator in the class to any fixed desired degree accuracy, given an appropriate number of examples in the prompt. Experiments with Poisson and reaction-diffusion forward and inverse boundary-value problems demonstrate the ability of the proposed model to use in-context examples to predict accurately the solutions corresponding to parameter queries for PDEs not seen during training.

</details>


### [66] [Privacy Blur: Quantifying Privacy and Utility for Image Data Release](https://arxiv.org/abs/2512.16086)
*Saeed Mahloujifar,Narine Kokhlikyan,Chuan Guo,Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: 研究发现传统的高斯模糊极易被破解，无法保护隐私；推荐使用像素化及其加噪版本作为替代方案，能更好地平衡隐私保护与模型训练的有效性。


<details>
  <summary>Details</summary>
Motivation: 负责任的数据发布需要隐藏人脸和车牌等私人信息，同时保留模型训练的效用。然而，目前广泛使用的工业标准——高斯模糊是否真的安全且有效尚未得到充分验证。

Method: 通过逆向攻击（Reversal attacks）和判别攻击（Discrimination attacks）评估隐私性；通过在模糊数据上训练模型并测试表征质量来评估效用。对比了高斯模糊、像素化、DP-Pix（像素化加噪）和裁剪（Cropping）四种方法。

Result: 1. 高斯模糊由于低精度实现的局限性，最容易被逆向还原，安全性最低。2. 像素化和DP-Pix在适当的粒度下能有效抵御攻击，同时保持较高的模型训练效用。3. 发布了名为 "Privacy Blur" 的开源工具包。

Conclusion: 高斯模糊在隐私保护方面存在严重缺陷。相比之下，经过参数调优的像素化（Pixelization）及其变体在兼顾隐私安全和下游视觉任务效用方面表现更优。

Abstract: Image data collected in the wild often contains private information such as faces and license plates, and responsible data release must ensure that this information stays hidden. At the same time, released data should retain its usefulness for model-training. The standard method for private information obfuscation in images is Gaussian blurring. In this work, we show that practical implementations of Gaussian blurring are reversible enough to break privacy. We then take a closer look at the privacy-utility tradeoffs offered by three other obfuscation algorithms -- pixelization, pixelization and noise addition (DP-Pix), and cropping. Privacy is evaluated by reversal and discrimination attacks, while utility by the quality of the learnt representations when the model is trained on data with obfuscated faces. We show that the most popular industry-standard method, Gaussian blur is the least private of the four -- being susceptible to reversal attacks in its practical low-precision implementations. In contrast, pixelization and pixelization plus noise addition, when used at the right level of granularity, offer both privacy and utility for a number of computer vision tasks. We make our proposed methods together with suggested parameters available in a software package called Privacy Blur.

</details>


### [67] [AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation](https://arxiv.org/abs/2512.16103)
*Sandeep Neela*

Main category: cs.LG

TL;DR: 本文介绍了 AIMM 框架，通过融合社交媒体数据和市场波动特征来识别股市操纵风险，成功提前预警了 GME 事件。


<details>
  <summary>Details</summary>
Motivation: 现代市场操纵正从孤立交易转向由社交媒体发起的协同运动，零售投资者和监管机构亟需能将在线叙事、群体协同模式与市场行为联系起来的工具。

Method: 开发了一个名为 AIMM 的 AI 驱动框架，融合了 Reddit 活动数据、机器人/协同指标以及 OHLCV 市场特征。采用 Parquet 原生态流水线和 Streamlit 仪表盘进行可视化分析，并基于真实案件构建了 AIMM-GT 标记数据集，通过前向行走（forward-walk）评估模型性能。

Result: 该框架生成每日操纵风险评分，并在实测中提前 22 天对 2021 年 1 月 GME 空头挤压事件发出了警报。项目发布了 AIMM-GT 数据集（包含 33 个标记的股票交易日）以及完整的代码与仪表盘设计。

Conclusion: AIMM 展示了初步的判别能力。虽然目前标记数据集较小，但其设计验证了通过融合社交指标与市场数据来识别复杂操纵行为的可行性，为社交媒体驱动的市场监管研究提供了基础工具。

Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.

</details>


### [68] [BUILD with Precision: Bottom-Up Inference of Linear DAGs](https://arxiv.org/abs/2512.16111)
*Hamed Ajorlou,Samuel Rey,Gonzalo Mateos,Geert Leus,Antonio G. Marques*

Main category: cs.LG

TL;DR: 本文提出BUILD算法，通过逆向推断叶节点并利用精度矩阵的结构性质，实现了线性高斯模型下DAG的鲁棒、高效重建。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中学习有向无环图（DAG）结构是因果发现中的核心问题。传统方法在处理线性高斯模型时常面临复杂性高或鲁棒性不足的问题，亟需一种既能利用精度矩阵特性又具备良好抗噪能力的推断算法。

Method: 提出BUILD算法（Bottom-Up Inference of Linear DAGs）。该算法利用观测值的集成精度矩阵结构，通过确定性逐步推断识别叶节点及其父节点，并在此过程中动态重新估计精度矩阵以减少误差积累。

Result: 在具有挑战性的合成基准测试中，BUILD算法在性能上优于当前最先进的DAG学习算法，同时在运行时间与鲁棒性之间提供了显式的权衡机制。

Conclusion: 在相等噪声方差的线性高斯模型下，BUILD算法是一种从观测数据中恢复DAG的有效、确定性且具有鲁棒性的方法，相比现有SOTA算法具有更好的复杂性控制。

Abstract: Learning the structure of directed acyclic graphs (DAGs) from observational data is a central problem in causal discovery, statistical signal processing, and machine learning. Under a linear Gaussian structural equation model (SEM) with equal noise variances, the problem is identifiable and we show that the ensemble precision matrix of the observations exhibits a distinctive structure that facilitates DAG recovery. Exploiting this property, we propose BUILD (Bottom-Up Inference of Linear DAGs), a deterministic stepwise algorithm that identifies leaf nodes and their parents, then prunes the leaves by removing incident edges to proceed to the next step, exactly reconstructing the DAG from the true precision matrix. In practice, precision matrices must be estimated from finite data, and ill-conditioning may lead to error accumulation across BUILD steps. As a mitigation strategy, we periodically re-estimate the precision matrix (with less variables as leaves are pruned), trading off runtime for enhanced robustness. Reproducible results on challenging synthetic benchmarks demonstrate that BUILD compares favorably to state-of-the-art DAG learning algorithms, while offering an explicit handle on complexity.

</details>


### [69] [Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure](https://arxiv.org/abs/2512.16126)
*Lulu Xue,Shengshan Hu,Linqiang Qian,Peijin Guo,Yechao Zhang,Minghui Li,Yanjun Zhang,Dayong Ye,Leo Yu Zhang*

Main category: cs.LG

TL;DR: 本文揭示了机器遗忘对“保留数据”造成的隐私威胁。研究表明，攻击者对比遗忘前后的两个模型版本，可以更有效地窃取保留数据的隐私。作者提出了DVIA攻击方法，在无需训练模型的情况下通过双视图对比成功实现了成员推理。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘研究主要关注“被删除数据”的隐私，而忽略了“保留数据”在遗忘操作（模型更新）过程中可能面临的隐私泄露风险，尤其是在攻击者可以同时访问遗忘前后两个版本模型的情况下。

Method: 提出了一种双视图推理攻击（DVIA）。该方法在黑盒条件下，利用原始模型和遗忘后模型的查询结果，通过轻量级的似然比推理模块（Likelihood Ratio Inference）来提取保留数据的成员身份信息，且无需训练额外的攻击模型。

Result: 通过对不同数据集和模型架构的评估，证明了DVIA能够有效推断保留数据的成员身份。实验证实了在双视图设置下，隐私信息的增益会显著放大保留数据的隐私泄露风险。

Conclusion: 机器遗忘并非在所有维度上都是隐私安全的；双视图攻击场景下，模型更新过程本身产生的隐私泄露是未来设计遗忘算法时必须考虑的关键安全边界。

Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.

</details>


### [70] [INTELLECT-3: Technical Report](https://arxiv.org/abs/2512.16144)
*Prime Intellect Team,Mika Senghaas,Fares Obeid,Sami Jaghouar,William Brown,Jack Min Ong,Daniel Auras,Matej Sirovatka,Jannik Straube,Andrew Baker,Sebastian Müller,Justus Mattern,Manveer Basra,Aiman Ismail,Dominik Scherm,Cooper Miller,Ameen Patel,Simon Kirsten,Mario Sieg,Christian Reetz,Kemal Erdem,Vincent Weisser,Johannes Hagemann*

Main category: cs.LG

TL;DR: INTELLECT-3 是一个 106B（激活 12B）的 MoE 模型，通过开源的大规模异步 RL 框架 prime-rl 训练，在推理任务上性能卓越。项目全栈开源，包含模型、训练代码及环境。


<details>
  <summary>Details</summary>
Motivation: 旨在证明通过大规模强化学习（RL）和高效的端到端基础设施，可以在相对较小的活跃参数量下（12B）实现顶尖的推理和技术能力。

Method: 在 GLM-4.5-Air-Base 基础上，利用新开发的 prime-rl 异步 RL 框架和 verifiers 库环境，在 512 片 H200 显卡上进行了大规模 SFT 和 RL 训练。

Result: INTELLECT-3 在数学、代码、科学和推理等 benchmark 上达到了同尺寸模型中的最先进水平（SOTA），表现优于许多参数量更大的前沿模型。

Conclusion: INTELLECT-3 证明了通过高效的分布式 RL 基础设施和大规模计算，可以在中等激活参数规模下实现超越重型模型的性能，并为开源社区提供了从模型到环境的完整研究路径。

Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.

</details>


### [71] [Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions](https://arxiv.org/abs/2512.16200)
*Haishan Ye*

Main category: cs.LG

TL;DR: 本文首次为基于排名的零阶优化算法建立了明确的、非渐近的查询复杂度分析，填补了该领域在强凸和非凸设置下的理论空白。


<details>
  <summary>Details</summary>
Motivation: 尽管基于排名的零阶优化算法（如CMA-ES、自然进化策略）在实践中非常成功且对噪声鲁棒，但其理论研究局限于渐近性分析，缺乏明确的非渐近收敛速度。

Method: 通过分析一种简单的、基于 top-k 方向选择的排名式零阶（ZO）算法，避开传统的漂移分析和信息几何技术，推导其收敛特性。

Result: 在 $d$ 维空间下，对于 $L$-光滑 $μ$-强凸函数，算法复杂度为 $\widetilde{\mathcal O}(\frac{dL}{μ} \log \frac{dL}{μδ} \log \frac{1}{\varepsilon})$；对于非凸平滑目标，复杂度为 $\mathcal O(\frac{dL}{\varepsilon} \log \frac{1}{\varepsilon})$，均具有高概率保证。

Conclusion: 该分析为基于排名的启发式选择为何能在零阶优化中表现高效提供了新的理论见解，并弥补了该领域长期存在的理论空白。

Abstract: Rank-based zeroth-order (ZO) optimization -- which relies only on the ordering of function evaluations -- offers strong robustness to noise and monotone transformations, and underlies many successful algorithms such as CMA-ES, natural evolution strategies, and rank-based genetic algorithms. Despite its widespread use, the theoretical understanding of rank-based ZO methods remains limited: existing analyses provide only asymptotic insights and do not yield explicit convergence rates for algorithms selecting the top-$k$ directions.
  This work closes this gap by analyzing a simple rank-based ZO algorithm and establishing the first \emph{explicit}, and \emph{non-asymptotic} query complexities. For a $d$-dimension problem, if the function is $L$-smooth and $μ$-strongly convex, the algorithm achieves $\widetilde{\mathcal O}\!\left(\frac{dL}μ\log\!\frac{dL}{μδ}\log\!\frac{1}{\varepsilon}\right)$ to find an $\varepsilon$-suboptimal solution, and for smooth nonconvex objectives it reaches $\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$. Notation $\cO(\cdot)$ hides constant terms and $\widetilde{\mathcal O}(\cdot)$ hides extra $\log\log\frac{1}{\varepsilon}$ term. These query complexities hold with a probability at least $1-δ$ with $0<δ<1$. The analysis in this paper is novel and avoids classical drift and information-geometric techniques. Our analysis offers new insight into why rank-based heuristics lead to efficient ZO optimization.

</details>


### [72] [Neural emulation of gravity-driven geohazard runout](https://arxiv.org/abs/2512.16221)
*Lorenzo Nava,Ye Chen,Maximillian Van Wyk de Vries*

Main category: cs.LG

TL;DR: 本文开发了一种高精度机器学习模型，通过模拟真实地形下的地理灾害径流，实现了比传统算法快万倍的预测速度，助力大规模预警。


<details>
  <summary>Details</summary>
Motivation: 现有的地理灾害（如山体滑坡、雪崩）径流预测模型在物理真实性与计算效率之间存在矛盾，难以满足大规模动态预警的需求。

Method: 研究团队基于超过10,000个真实地形数据（DEM）生成了10万余次数值模拟，以此训练机器学习模型，旨在模仿复杂的物理径流过程。

Result: 该模型能准确预测流动范围和堆积厚度，计算速度比传统数值求解器快100至10,000倍，且能准确复现分流和沉积等复杂物理现象。

Conclusion: 神经模拟（Neural Emulation）是实现大规模、高精度地理灾害预警的有效途径，为防灾减灾提供了新的技术手段。

Abstract: Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.

</details>


### [73] [Sharpness-aware Federated Graph Learning](https://arxiv.org/abs/2512.16247)
*Ruiyu Li,Peige Zhao,Guangxia Li,Pengcheng Wu,Xingyu Gao,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 本文提出 SEAL 算法，通过锐度感知优化和表征去相关正则化，解决了联邦图学习中的数据异构和维度塌缩问题，提升了模型的泛化性与分类性能。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习（FGL）中存在严重的数据异构性问题，现有方法易陷入损失函数的“尖锐谷底”导致泛化差，且模型表征存在“维度塌缩”现象，限制了分类能力。

Method: 提出 SEAL 算法：1) 引入锐度感知（Sharpness-aware）优化目标，同时最小化损失值及其曲率以寻找平坦解区域；2) 引入基于特征相关矩阵的正则化项，降低样本特征间的相关性。

Result: 在多个图分类基准数据集上的实验表明，SEAL 显著优于现有的联邦图学习基线方法，并能让更多的参与方受益。

Conclusion: SEAL 算法能有效提升联邦图学习中本地模型的分类准确率和泛化能力，在多个基准测试中优于现有最先进技术。

Abstract: One of many impediments to applying graph neural networks (GNNs) to large-scale real-world graph data is the challenge of centralized training, which requires aggregating data from different organizations, raising privacy concerns. Federated graph learning (FGL) addresses this by enabling collaborative GNN model training without sharing private data. However, a core challenge in FGL systems is the variation in local training data distributions among clients, known as the data heterogeneity problem. Most existing solutions suffer from two problems: (1) The typical optimizer based on empirical risk minimization tends to cause local models to fall into sharp valleys and weakens their generalization to out-of-distribution graph data. (2) The prevalent dimensional collapse in the learned representations of local graph data has an adverse impact on the classification capacity of the GNN model. To this end, we formulate a novel optimization objective that is aware of the sharpness (i.e., the curvature of the loss surface) of local GNN models. By minimizing the loss function and its sharpness simultaneously, we seek out model parameters in a flat region with uniformly low loss values, thus improving the generalization over heterogeneous data. By introducing a regularizer based on the correlation matrix of local representations, we relax the correlations of representations generated by individual local graph samples, so as to alleviate the dimensional collapse of the learned model. The proposed \textbf{S}harpness-aware f\textbf{E}derated gr\textbf{A}ph \textbf{L}earning (SEAL) algorithm can enhance the classification accuracy and generalization ability of local GNN models in federated graph learning. Experimental studies on several graph classification benchmarks show that SEAL consistently outperforms SOTA FGL baselines and provides gains for more participants.

</details>


### [74] [Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data](https://arxiv.org/abs/2512.16277)
*Jialiang Wang,Xueyan Bao,Hao Wu*

Main category: cs.LG

TL;DR: 本文将锐度感知最小化（SAM）引入二阶隐因子模型，通过Hessian向量积优化非凸目标函数，显著提升了高维缺失数据下的模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 二阶隐因子模型（SLF）在处理高维稀疏数据（HDI）时效果显著，但由于其双线性和非凸特性，优化难度大且泛化性能受限。

Method: 提出SSLF模型，核心包括：1. 利用Hessian向量积获取二阶信息；2. 通过设计的Hessian向量积向曲率中注入锐度项。

Result: 在多个工业数据集上的实验表明，SSLF模型在性能上一致优于目前最先进的基准方法。

Conclusion: SSLF模型通过在优化路径中引入锐度感知机制，有效解决了二阶隐因子模型的泛化瓶颈，具有较高的工业应用价值。

Abstract: Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.

</details>


### [75] [Feature-Selective Representation Misdirection for Machine Unlearning](https://arxiv.org/abs/2512.16297)
*Taozhao Chen,Linghan Huang,Kim-Kwang Raymond Choo,Huaming Chen*

Main category: cs.LG

TL;DR: 针对 LLM 知识卸载中数据纠缠严重的难题，本研究提出 SRMU 框架，通过选择性表征误导在精准消除有害知识的同时，最大限度地保留了模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 机器卸载（Machine Unlearning）技术大多假设卸载集与保留集完全分离，但在实际操作中两者通常高度纠缠。传统基于扰动的方法在面对这种高纠缠分布时，往往会导致模型通用能力大幅下降或无法确保安全性。

Method: 提出 SRMU (Selective Representation Misdirection for Unlearning) 框架，通过结构化的“误导向量”结合“激活重要性图”，实施特征感知且受控方向的激活编辑，而非盲目修改权重。

Result: 在 WMDP 基准测试中，SRMU 在低度和高度纠缠配置下均达到了 SOTA 性能。尤其在卸载与保留数据存在 20-30% 重叠的极端情况下，仍能保持有效性，而现有基线方法已失效。

Conclusion: SRMU 为 LLM 在安全治理、隐私合规及受控知识删除方面提供了稳健的基础，特别是在数据分布高度纠缠的复杂现实场景中。

Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.

</details>


### [76] [Quantitative Verification of Fairness in Tree Ensembles](https://arxiv.org/abs/2512.16386)
*Zhenjiang Zhao,Takahisa Toda,Takashi Kitamura*

Main category: cs.LG

TL;DR: 本文提出了一种针对树集成模型公平性的定量验证方法，通过利用模型离散特性，实现了比现有DNN迁移方法更高效、更精确的正反例比例区间估算。


<details>
  <summary>Details</summary>
Motivation: 现有的公平性验证多针对DNN且仅返回单一反例，而定量验证（估算反例比例及区域）对诊断偏见至关重要，但现有方法在树模型上仅能提供下限且效率较低。

Method: 利用树集成模型的离散结构，提出一种高效的定量技术，旨在提供可随时生成的（any-time）上限和下限估计。

Result: 在五个数据集上的实验表明，该方法在公平性测试中显著优于现有最先进技术，且具备更高的效率和估算精度。

Conclusion: 结合树集成模型的结构特性进行定量验证比通用框架更有效，能够为公平性诊断提供更精确的依据。

Abstract: This work focuses on quantitative verification of fairness in tree ensembles. Unlike traditional verification approaches that merely return a single counterexample when the fairness is violated, quantitative verification estimates the ratio of all counterexamples and characterizes the regions where they occur, which is important information for diagnosing and mitigating bias. To date, quantitative verification has been explored almost exclusively for deep neural networks (DNNs). Representative methods, such as DeepGemini and FairQuant, all build on the core idea of Counterexample-Guided Abstraction Refinement, a generic framework that could be adapted to other model classes. We extended the framework into a model-agnostic form, but discovered two limitations: (i) it can provide only lower bounds, and (ii) its performance scales poorly. Exploiting the discrete structure of tree ensembles, our work proposes an efficient quantification technique that delivers any-time upper and lower bounds. Experiments on five widely used datasets demonstrate its effectiveness and efficiency. When applied to fairness testing, our quantification method significantly outperforms state-of-the-art testing techniques.

</details>


### [77] [Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference](https://arxiv.org/abs/2512.16391)
*Dhruv Deshmukh,Saurabh Goyal,Nipun Kwatra,Ramachandran Ramjee*

Main category: cs.LG

TL;DR: Kascade 通过在锚点层计算并跨层复用精确的 Top-k 注意力索引，在不损失精度的前提下显著提升了长文本 LLM 的推理速度。


<details>
  <summary>Details</summary>
Motivation: 在长文本推理（如 RAG 和推理模型）中，注意力机制是延迟的主要来源。由于注意力分布具有内在稀疏性且高权重 Key 在相邻层间具有稳定性，存在减少冗余计算的空间。

Method: 提出 Kascade 方法：在少数锚点层（Anchor Layers）计算精确的 Top-k 索引，并在中间层（Reuse Layers）复用这些索引。采用多头感知的动态规划算法选择最优锚点层，并支持 Prefill 和 Decode 阶段的块级（Tile-level）高效实现。

Result: 在 H100 GPU 上，相比 FlashAttention-3，Kascade 在 Decode 阶段实现高达 4.1 倍加速，Prefill 阶段实现 2.2 倍加速。同时在 LongBench 和 AIME-24 等长文本基准测试中保持了与密集注意力接近的精度。

Conclusion: Kascade 是一种兼顾推理速度与模型精度的稀疏注意力方案，特别适用于长文本推理，无需重新训练即可在现有 LLM 上实现显著加速。

Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.

</details>


### [78] [Emergent Bias and Fairness in Multi-Agent Decision Systems](https://arxiv.org/abs/2512.16433)
*Maeve Madigan,Parameswaran Kamalaruban,Glenn Moynihan,Tom Kempton,David Sutton,Stuart Burrell*

Main category: cs.LG

TL;DR: 本文研究了金融领域多智能体系统的公平性评估。研究指出，多智能体决策会产生无法归因于个体的集体偏见，因此必须将其作为整体进行公平性审计。


<details>
  <summary>Details</summary>
Motivation: 虽然多智能体系统通过协作决策提升了性能，但由于缺乏有效的评估方法，其潜在的偏见风险难以衡量。在金融等高风险领域，偏见可能导致监管违规和财务损失。

Method: 通过大规模仿真，研究了具有不同通信和协作机制的多种多智能体配置，并利用公平性指标（Fairness Metrics）对金融表格数据集（如信用评分、收入估算）进行评估。

Result: 研究发现，多智能体决策中会出现“涌现性偏差”（Emergent Bias），这种偏差无法追溯到单个智能体组件，表明系统展现出了真正的集体行为特征。

Conclusion: 多智能体系统在金融领域的公平性风险是一种不可忽视的模型风险。必须将此类系统视为一个整体进行公平性评估，而非简单的组件分析，以确保应用的安全性和合规性。

Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.

</details>


### [79] [Persistent Multiscale Density-based Clustering](https://arxiv.org/abs/2512.16558)
*Daniël Bot,Leland McInnes,Jan Aerts*

Main category: cs.LG

TL;DR: 本文提出 PLSCAN 算法，利用持久同调理论自动识别最稳定的聚类结构，克服了 HDBSCAN* 依赖手动设置最小簇大小参数的局限性，实现了更优且更稳健的聚类效果。


<details>
  <summary>Details</summary>
Motivation: 密度聚类（如 DBSCAN 和 HDBSCAN*）在探索性数据分析中表现优异，但其性能极度依赖超参数的选择（如密度阈值或最小簇大小），而在缺乏先验知识的情况下很难手动选择最优参数。

Method: 提出 PLSCAN 算法，结合了尺度空间（Scale-space）聚类原理和新型度量空间上的持久同调（Persistent Homology），用于识别 HDBSCAN* 中所有能产生稳定（叶子）簇的最小簇大小。

Result: 在多个真实数据集上，PLSCAN 的平均调整兰德指数（ARI）高于 HDBSCAN*，且对相互可达邻居数量的敏感度更低。在低维数据上，其运行时间可与 k-Means 媲美，高维下则与 HDBSCAN* 相当。

Conclusion: PLSCAN 是一种稳健且高效的聚类工具，它通过自动化超参数选择过程，显著提升了密度聚类在探索性数据分析中的实用性。

Abstract: Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.

</details>


### [80] [Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling](https://arxiv.org/abs/2512.16581)
*Sullivan Castro,Artem Betlei,Thomas Di Martino,Nadir El Manouzi*

Main category: cs.LG

TL;DR: 本文提出 Abacus，一种结合用户事件频率分布预测与序列建模的自监督混合框架，解决了展示广告中用户行为建模的稀疏性与统计信息缺失问题。


<details>
  <summary>Details</summary>
Motivation: 展示广告中的用户购买行为建模面临数据稀疏、随机性强和类不平衡问题。现有方法要么依赖人工设计的计数特征（缺失时间演化信息），要么仅使用序列模型（缺失宏观统计信息）。

Method: 提出了 Abacus 方法及混合目标函数：1. 通过自监督预训练预测用户事件的经验频率分布（计数统计）；2. 将 Abacus 与序列学习目标统一，增强模型对用户意图演化的捕捉能力。

Result: 在两项真实数据集上，Abacus 预训练能有效加速下游任务收敛，混合建模方案相比基线模型在 AUC 评价指标上提升了多达 6.1%。

Conclusion: 结合聚合统计量与序列灵敏度的混合建模方法，显著优于传统的计数特征或纯序列模型。

Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.

</details>


### [81] [Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario](https://arxiv.org/abs/2512.16648)
*Liu Yang,Qiang Li,Luxiong Wen,Jian Yang*

Main category: cs.LG

TL;DR: 本文研究了无源数据跨接收器射频指纹识别（SCRFFI）问题，提出了 MS-SHOT 方法，通过动量软伪标签和结构约束，在不接触源数据的情况下实现了对目标接收器的高效自适应。


<details>
  <summary>Details</summary>
Motivation: 深度学习在射频指纹识别（RFFI）中面临跨硬件接收器性能下降的问题，且在实际部署中往往无法获取源域原始数据。此外，目标域可能存在标签偏移或非均匀分布的问题。

Method: 提出了 MS-SHOT 方法。该方法利用动量中心引导的软伪标签（Momentum Soft pseudo-labeling），并引入全局结构约束，以确保预测的置信度和多样性。

Result: 在真实数据集上的实验表明，MS-SHOT 在准确性和鲁棒性方面均优于现有方法，特别是在处理标签偏移任务时表现突出。

Conclusion: MS-SHOT 为跨接收器 RFFI 提供了一种实用且具有扩展性的解决方案，有效解决了源数据缺失和目标域类别分布偏移的难题。

Abstract: With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.

</details>


### [82] [Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification](https://arxiv.org/abs/2512.16687)
*Natnael Tilahun Sinshaw,Mengmei He,Tadesse K. Bahiru,Sudhir Kumar Mohapatra*

Main category: cs.LG

TL;DR: 本文评估了多种传统机器学习模型与神经符号AI（NeSy）在文本分类任务中的表现，发现NeSy在小样本数据集下能达到与深度学习模型相当的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管文本分类（如博客性别分类）已有成熟研究，但仍需探索新兴的神经符号AI（NeSy）在结合深度学习表示与逻辑推理方面的表现，以及它与传统方法的优劣对比。

Method: 对比分析了多种机器学习算法（SVM, NB, LR, AdaBoost, XGBoost）及SVM变体与NeSy模型；同时实验了TF-IDF、USE、RoBERTa等文本表示技术，以及Chi-Square、MI和PCA等特征提取技术。

Result: 实验结果显示，即使在数据集有限的情况下，NeSy方法的表现也能与强大的多层感知机（MLP）结果相媲美。

Conclusion: 神经符号AI（NeSy）在大文本分类任务中具有巨大的潜力，未来研究将集中在扩大知识库、嵌入类型和超参数配置，以进一步验证其有效性。

Abstract: Text classification problems, such as gender classification from a blog, have been a well-matured research area that has been well studied using machine learning algorithms. It has several application domains in market analysis, customer recommendation, and recommendation systems. This study presents a comparative analysis of the widely used machine learning algorithms, namely Support Vector Machines (SVM), Naive Bayes (NB), Logistic Regression (LR), AdaBoost, XGBoost, and an SVM variant (SVM_R) with neuro-symbolic AI (NeSy). The paper also explores the effect of text representations such as TF-IDF, the Universal Sentence Encoder (USE), and RoBERTa. Additionally, various feature extraction techniques, including Chi-Square, Mutual Information, and Principal Component Analysis, are explored. Building on these, we introduce a comparative analysis of the machine learning and deep learning approaches in comparison to the NeSy. The experimental results show that the use of the NeSy approach matched strong MLP results despite a limited dataset. Future work on this research will expand the knowledge base, the scope of embedding types, and the hyperparameter configuration to further study the effectiveness of the NeSy approach.

</details>


### [83] [CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies](https://arxiv.org/abs/2512.16700)
*John M. Statheros,Hairong Wang,Richard Klein*

Main category: cs.LG

TL;DR: CLARiTy 是一种基于 ViT 的弱监督医学影像模型，通过类别 Token 和解剖先验引导的 SegmentCAM 模块，在仅使用图像级标签的情况下，实现了高精度的胸部疾病分类与显著领先的定位效果。


<details>
  <summary>Details</summary>
Motivation: 针对胸部 X 光（CXR）多标签分类和空间定位中，区域级密集标注稀缺以及现有方法难以平衡分类精度与定位准确性的挑战。

Method: 提出 CLARiTy 模型：采用多类别特定 Token 生成判别性注意力图；引入 SegmentCAM 模块，利用解剖学先验进行前景分割与背景抑制；使用 ConvNeXtV2 进行知识蒸馏。

Result: 在 NIH ChestX-ray14 数据集上，该模型在 14 种病理分类中表现出色，且在 8 种病理的弱监督定位上达到 SOTA，性能提升达 50.7%，特别是在结节和肿块等小目标上提升明显。

Conclusion: CLARiTy 证明了通过 ViT 的全局上下文能力结合卷积引导的背景抑制，可以在不依赖密集标注的情况下，显著提升胸部疾病的分类与定位精度，尤其是针对细小病灶。

Abstract: The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.

</details>


### [84] [Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library](https://arxiv.org/abs/2512.16715)
*Oliver Stritzel,Nick Hühnerbein,Simon Rauch,Itzel Zarate,Lukas Fleischmann,Moike Buck,Attila Lischka,Christian Frey*

Main category: cs.LG

TL;DR: 本文推出了 SPICE，一个旨在解决预测过程挖掘（PPM）领域中重复性差和对比困难问题的 PyTorch 框架，并对 11 个数据集进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的预测过程挖掘（PPM）方法缺乏可重复性、决策透明度以及基准测试的易用性，导致不同模型之间的公平比较非常困难。

Method: 提出了 SPICE（Python 框架），基于 PyTorch 重构了三种主流的深度学习 PPM 基准方法。该框架设计了通用的基础架构和严格的配置机制，以确保实验的可重复性。

Result: 研究人员在 11 个数据集上将 SPICE 与原始报告的指标以及公平基准指标进行了对比验证，证明了其在模型评估中的鲁棒性。

Conclusion: SPICE 为预测过程挖掘提供了一个标准化的评估环境，通过严格的可配置性和一致的方法论，显著提高了该领域的研究透明度和模型间的可比性。

Abstract: In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.

</details>


### [85] [Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation](https://arxiv.org/abs/2512.16718)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文通过构建多调和样条级联架构，解决了传统随机函数回归模型在高维情况下的计算瓶颈和理论失效问题，实现了高效、可扩展且端到端可微的回归预测。


<details>
  <summary>Details</summary>
Motivation: 现有的基于随机函数理论的回归方案虽然具有解析最优核，但面临 $O(N^3)$ 的高计算成本，且在输入空间维度过高时原有的理论假设会失效。

Method: 提出了一种由多调和样条（Polyharmonic Splines）包构建的级联架构，并开发了用于前向计算和端到端微分的高效矩阵程序。

Result: 该架构成功解决了模型的可扩展性问题，并能够有效处理具有未知低维内在结构的复杂高维数据。

Conclusion: 多调和样条级联架构为解决高维空间下的回归问题提供了一种在理论上严谨且计算上高效的机器学习新途径。

Abstract: In a previous paper it was shown that a machine learning regression problem can be solved within the framework of random function theory, with the optimal kernel analytically derived from symmetry and indifference principles and coinciding with a polyharmonic spline. However, a direct application of that solution is limited by O(N^3) computational cost and by a breakdown of the original theoretical assumptions when the input space has excessive dimensionality. This paper proposes a cascade architecture built from packages of polyharmonic splines that simultaneously addresses scalability and is theoretically justified for problems with unknown intrinsic low dimensionality. Efficient matrix procedures are presented for forward computation and end-to-end differentiation through the cascade.

</details>


### [86] [Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis](https://arxiv.org/abs/2512.16742)
*Wisnu Uriawan,Muhamad Veva Ramadhan,Firman Adi Nugraha,Hasbi Nur Wahid,M Dantha Arianvasya,Muhammad Zaki Alghifari*

Main category: cs.LG

TL;DR: 本研究开发了一种基于机器学习（SVM表现最佳，准确率92.3%）的检测系统，通过分析APP描述和权限特征，自动识别印尼朝圣服务中的虚假移动应用。


<details>
  <summary>Details</summary>
Motivation: 印尼朝圣服务的数字化虽带来便利，但也导致虚假应用泛滥，给朝圣者带来财产损失和隐私泄露风险，因此亟需一种自动验证应用真实性的方案。

Method: 采用结合文本分析（TF-IDF）与元数据分析（敏感权限）的混合特征提取方法，并对比了支持向量机（SVM）、随机森林（RF）和朴素贝叶斯（NB）三种机器学习算法。

Result: SVM算法表现最优，准确率达92.3%，F1分数为92.0%。研究发现描述中的法律相关关键词及高风险权限（如读取手机状态）是区分虚假应用的关键特征。

Conclusion: 该研究提出的基于SVM的分类系统可作为国家级验证原型，预见性地提升宗教旅游领域的数字信任，为打击针对朝圣者的数字欺诈提供技术保障。

Abstract: The rapid digitalization of Hajj and Umrah services in Indonesia has significantly facilitated pilgrims but has concurrently opened avenues for digital fraud through counterfeit mobile applications. These fraudulent applications not only inflict financial losses but also pose severe privacy risks by harvesting sensitive personal data. This research aims to address this critical issue by implementing and evaluating machine learning algorithms to verify application authenticity automatically. Using a comprehensive dataset comprising both official applications registered with the Ministry of Religious Affairs and unofficial applications circulating on app stores, we compare the performance of three robust classifiers: Support Vector Machine (SVM), Random Forest (RF), and Na"ive Bayes (NB). The study utilizes a hybrid feature extraction methodology that combines Textual Analysis (TF-IDF) of application descriptions with Metadata Analysis of sensitive access permissions. The experimental results indicate that the SVM algorithm achieves the highest performance with an accuracy of 92.3%, a precision of 91.5%, and an F1-score of 92.0%. Detailed feature analysis reveals that specific keywords related to legality and high-risk permissions (e.g., READ PHONE STATE) are the most significant discriminators. This system is proposed as a proactive, scalable solution to enhance digital trust in the religious tourism sector, potentially serving as a prototype for a national verification system.

</details>


### [87] [NRGPT: An Energy-based Alternative for GPT](https://arxiv.org/abs/2512.16762)
*Nima Dehmamy,Benjamin Hoover,Bishwajit Saha,Leo Kozachkov,Jean-Jacques Slotine,Dmitry Krotov*

Main category: cs.LG

TL;DR: 本文提出 NRGPT，通过轻量级修改将 GPT 与能量模型统一，提升了模型性能并增强了抗过拟合能力。


<details>
  <summary>Details</summary>
Motivation: 旨在结合语言建模中主流的 GPT 架构与能量模型（EBM）范式，将推理过程视为能量景观上的动态演化。

Method: 提出 eNeRgy-GPT (NRGPT)，通过对标准 GPT 进行极小修改，使其与能量评分模型（EBM）框架统一；推理过程被建模为能量景观上的令牌搜索（特定条件下等同于梯度下降）。

Result: 模型在莎士比亚数据集、代数 ListOPS 任务及 OpenWebText 等复杂设置下表现良好，且在训练过程中表现出更强的抗过拟合能力。

Conclusion: NRGPT 成功将 EBM 思想融入 GPT，并表现出更强的泛化能力和耐过拟合特性，为语言模型的设计提供了新视角。

Abstract: Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.

</details>


### [88] [Pattern recognition in complex systems via vector-field representations of spatio-temporal data](https://arxiv.org/abs/2512.16763)
*Ingrid Amaranta Membrillo Solis,Maria van Rossem,Tristan Madeleine,Tetiana Orlova,Nina Podoliak,Giampaolo D'Alessandro,Jacek Brodzki,Malgosia Kaczmarek*

Main category: cs.LG

TL;DR: 本文提出一种基于向量场几何理论的新框架，通过双参数度量族有效解决了复杂系统海量时空数据的建模、分析与吸引子刻画难题。


<details>
  <summary>Details</summary>
Motivation: 复杂系统（如大脑、气候、经济）具有高维非线性动力学特征，产生的海量时空数据使传统维度缩减和相空间重构方法面临巨大挑战。

Method: 引入一种基于离散测度空间向量场理论的几何框架，提出一类适用于数据分析和机器学习的双参数度量族（metrics），支持图像、图形及单纯复形上的函数分析。

Result: 通过生物和物理系统的数值模拟验证，该度量方法结合多维尺度分析（MDS），成功实现了降维、模态分解、相空间重构以及吸引子表征。

Conclusion: 该几何框架为分析复杂动力系统提供了一种稳健的途径，特别适用于传统建模方法失效但实验数据充足的场景。

Abstract: A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth's climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.

</details>


### [89] [MEPIC: Memory Efficient Position Independent Caching for LLM Serving](https://arxiv.org/abs/2512.16822)
*Qian Wang,Zahra Yousefijamarani,Morgan Lindsay Heisler,Rongzhi Gu,Bai Xiaolong,Shan Yizhou,Wei Zhang,Wang Lan,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: MEPIC 是一个高效的 KV 缓存系统，通过消除位置编码依赖和对齐分页存储，实现了不同请求间相同数据块的物理显存共享，大幅降低了长文本 LLM 应用的显存压力。


<details>
  <summary>Details</summary>
Motivation: 现代 LLM 应用（如 RAG、代码助手）存在大量重复的文档或代码块，但现有的前缀缓存受限于严格匹配，而位置无关缓存（PIC）因位置编码差异和非对齐存储，导致同一数据块的 KV 缓存在显存中无法真正共享。

Method: 提出 MEPIC 系统：1. 将分块 KV 缓存与分页存储对齐；2. 将重计算从 Token 级转为 Block 级（仅首块需特定请求处理）；3. 在 Attention 算子中融合 RoPE 以移除显式位置编码，使大部分 KV 块可在请求间物理共享。

Result: 在保持延迟和精度相当的情况下，显存（HBM）占用较现有技术降低了 2 倍，在长 Prompt 场景下甚至可降低 5 倍。

Conclusion: MEPIC 在不改变模型架构的前提下，通过对齐存储、块级重计算和 RoPE 融合，显著提升了长文本应用中的显存利用效率和系统吞吐。

Abstract: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.

</details>


### [90] [Meta-RL Induces Exploration in Language Agents](https://arxiv.org/abs/2512.16848)
*Yulun Jiang,Liangze Jiang,Damien Teney,Michael Moor,Maria Brbic*

Main category: cs.LG

TL;DR: 本文提出 LaMer 框架，将元强化学习（Meta-RL）引入 LLM 智能体训练，通过跨回合学习和上下文反思机制，显著提升了智能体在复杂环境下的主动探索与自适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习（RL）训练的 LLM 智能体在需要主动探索的任务中表现不佳，且难以及时从试错经验中高效学习。

Method: 提出 LaMer 框架，包含两个核心组件：(i) 跨回合（cross-episode）训练框架，以鼓励探索和长期奖励优化；(ii) 通过反思（reflection）进行上下文内策略更新，无需梯度更新即可根据反馈调整行为。

Result: LaMer 在 Sokoban、MineSweeper 和 Webshop 任务上分别提升了 11%、14% 和 19% 的性能，并在处理更难或未见过的任务时展现出更强的泛化能力。

Conclusion: Meta-RL 为增强语言智能体的探索能力提供了一种原则性方法，使其能够通过学习到的策略更鲁棒地适应新环境。

Abstract: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.

</details>


### [91] [Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models](https://arxiv.org/abs/2512.16866)
*Jiabin Xue*

Main category: cs.LG

TL;DR: 本文提出知识转换（KT）方法，通过教师模型引导的主动学习为在线边缘设备生成伪标签，解决了在线边缘学习中新数据标注困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的边缘机器学习多假设模型是在线下训练后静态部署的，难以应对未见的实时数据；而在线边缘学习面临如何在边缘设备上为未来、未见的数据点确定标签的挑战。

Method: 提出知识转换（KT）方法，其结合了知识蒸馏、主动学习和因果推理，利用教师模型作为主动学习中的“神谕（Oracle）”为学生模型生成伪标签。

Result: 仿真实验表明，当提供稳定的教师模型时，学生模型能够逐渐达到预期的最高性能，证明了该拟定框架的有效性。

Conclusion: KT方法特别适用于预训练模型可作为教师模型的通用场景，以及获取学生模型任务标签成本高昂的情况。

Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.

</details>


### [92] [Sequencing to Mitigate Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2512.16871)
*Hesham G. Moussa,Aroosa Hameed,Arashmid Akhavain*

Main category: cs.LG

TL;DR: 本文研究了持续学习中任务顺序对灾难性遗忘的影响，并提出一种基于NAS零样本评分算法的最优任务排序方法，证明了合理的任务序列能显著提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 旨在解决持续学习中的灾难性遗忘问题，但不同于以往侧重算法架构的研究，本文探索了任务呈现顺序对减轻遗忘的影响。

Method: 提出一种确定最优任务顺序的方法，利用受神经架构搜索（NAS）启发的零样本评分算法（Zero-shot scoring）来评估任务间的关系。

Result: 智能的任务排序能显著减少灾难性遗忘；当与传统持续学习策略结合时，能进一步提升模型的性能和鲁棒性。

Conclusion: 任务排序（Task Sequencing）是缓解持续学习中灾难性遗忘的有效且互补的手段，且可扩展至课程学习等其他领域。

Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.

</details>


### [93] [Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning](https://arxiv.org/abs/2512.16911)
*Andrew Wagenmaker,Perry Dong,Raymond Tsao,Chelsea Finn,Sergey Levine*

Main category: cs.LG

TL;DR: 本文指出了传统行为克隆作为 RL 预训练起点的缺陷，并提出了一种基于后验概率建模的新方法 PostBC，显著增强了政策在微调阶段的改进能力。


<details>
  <summary>Details</summary>
Motivation: 现有的行为克隆（BC）预训练政策在后续强化学习（RL）微调时效果不佳，原因是 BC 往往无法覆盖演示者的所有可能动作。

Method: 提出了后验行为克隆（PostBC），不再是简单地拟合动作，而是通过生成模型（如扩散模型）建模给定演示数据集下演示者行为的后验分布，以确保动作空间的全面覆盖。

Result: 在机器人控制基准测试和真实世界机器人操作任务中，PostBC 在 RL 微调性能上显著优于标准 BC 预训练方案。

Conclusion: PostBC 在保持预训练性能的同时，显著提升了强化学习微调的上限和鲁棒性。

Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [94] [A Survey on Spatio-Temporal Knowledge Graph Models](https://arxiv.org/abs/2512.16487)
*Philipp Plamper,Hanna Köpcke,Anika Groß*

Main category: cs.SI

TL;DR: 本文是一篇关于时空知识图谱（STKGs）的综述，系统分析了其建模演进过程、维度及挑战，指出当前缺乏统一框架并提出了未来研究指南。


<details>
  <summary>Details</summary>
Motivation: 时空知识图谱（STKGs）应用广泛但建模难度大，且各研究领域术语不一、建模假设异构，导致现有方法缺乏概念对齐、通用性和可重用性。

Method: 通过梳理静态、时间和空间图模型的演变，从边语义、时空标注策略、时空语义等关键维度对现有STKG模型进行系统化综述。

Result: 揭示了现有模型在语义对齐方面的缺失，识别了不同领域的建模差异，并总结出一套建模指南及未来研究面临的开放挑战。

Conclusion: 当前STKGs缺乏统一的建模框架，且现有模型多为特定场景定制，缺乏重用性。未来研究应关注通用化建模准则，以实现知识的长期保存与跨领域复用。

Abstract: Many complex real-world systems exhibit inherently intertwined temporal and spatial characteristics. Spatio-temporal knowledge graphs (STKGs) have therefore emerged as a powerful representation paradigm, as they integrate entities, relationships, time and space within a unified graph structure. They are increasingly applied across diverse domains, including environmental systems and urban, transportation, social and human mobility networks. However, modeling STKGs remains challenging: their foundations span classical graph theory as well as temporal and spatial graph models, which have evolved independently across different research communities and follow heterogeneous modeling assumptions and terminologies. As a result, existing approaches often lack conceptual alignment, generalizability and reusability. This survey provides a systematic review of spatio-temporal knowledge graph models, tracing their origins in static, temporal and spatial graph modeling. We analyze existing approaches along key modeling dimensions, including edge semantics, temporal and spatial annotation strategies, temporal and spatial semantics and relate these choices to their respective application domains. Our analysis reveals that unified modeling frameworks are largely absent and that most current models are tailored to specific use cases rather than designed for reuse or long-term knowledge preservation. Based on these findings, we derive modeling guidelines and identify open challenges to guide future research.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [95] [Online Resource Allocation via Static Bundle Pricing](https://arxiv.org/abs/2512.16570)
*Dimitris Fotakis,Charalampos Platanos,Thanos Tolias*

Main category: cs.GT

TL;DR: 本文提出了一套统一的静态捆绑定价机制，解决了具有互补性需求的在线资源分配问题，并在资源容量较大时实现了指数级改善的竞争比及其匹配的下界。


<details>
  <summary>Details</summary>
Motivation: 探讨在未来请求信息不完全的情况下，如何高效分配有限资源。传统定价机制在处理买家估价具有“互补性”（即对商品组合的需求）时存在局限：单品定价无法利用资源多样性，而现有的静态捆绑定价又缺乏通用性。

Method: 开发了一种统一的技术框架，用于设计静态且匿名的捆绑定价机制，并将其应用于三种领域：最大捆绑包大小为 d 的单向组合拍卖、通用单向组合拍卖以及基于图的路由模型。此外，利用信息论方法证明了竞争比的理论下界。

Result: 1. 针对大小为 d 的捆绑包，在大容量 B 下实现了 $O(d^{1/B})$ 的竞争比；2. 针对通用场景和路由模型，实现了 $O(m^{1/(B+1)})$ 的竞争比；3. 证明了性能随资源容量增加呈指数级提升；4. 给出了匹配的理论下界，如通用场景下的 $Ω((m/\ln m)^{1/(B+2)})$。

Conclusion: 本研究证明了在具有互补性的在线资源分配中，静态匿名捆绑定价不仅可行，且在处理大容量资源时具有显著的理论优势。此外，通过揭示与极值组合数学（定性独立划分）的深层联系，为该领域提供了新的下界分析工具。

Abstract: Online Resource Allocation addresses the problem of efficiently allocating limited resources to buyers with incomplete knowledge of future requests. In our setting, buyers arrive sequentially demanding a set of items, each with a value drawn from a known distribution. We study environments where buyers' valuations exhibit complementarities. In such settings, standard item-pricing mechanisms fail to leverage item multiplicities, while existing static bundle-pricing mechanisms rely on problem-specific arguments that do not generalize.
  We develop a unified technique for online resource allocation with complementarities for three domains: (i) single-minded combinatorial auctions with maximum bundle size $d$, (ii) general single-minded combinatorial auctions, and (iii) a graph-based routing model in which buyers request to route a unit of flow from a source node $s$ to a target node $t$ in a capacitated graph. Our approach yields static and anonymous bundle-pricing mechanisms whose performance improves exponentially with item capacities. For the $d$-single-minded setting with minimum item capacity $B$, we obtain an $O(d^{1/B})$-competitive mechanism, recovering the known $O(d)$ bound for unit capacities ($B=1$) and achieving exponentially better guarantees as capacities grow. For general single-minded combinatorial auctions and the graph-routing model, we obtain $O(m^{1/(B+1)})$-competitive mechanisms, where $m$ is the number of items.
  We complement these results with information-theoretic lower bounds. We show that no online algorithm can achieve a competitive ratio better than $Ω((m/\ln m)^{1/(B+2)})$ in the general single-minded setting and $Ω((d/\ln d)^{1/(B+1)})$ in the $d$-single-minded setting. In doing so, we reveal a deep connection to the extremal combinatorics problem of determining the maximum number of qualitatively independent partitions of a ground set.

</details>


### [96] [On the Edge of Core (Non-)Emptiness: An Automated Reasoning Approach to Approval-Based Multi-Winner Voting](https://arxiv.org/abs/2512.16895)
*Ratip Emin Berker,Emanuel Tewolde,Vincent Conitzer,Mingyu Guo,Marijn Heule,Lirong Xia*

Main category: cs.GT

TL;DR: 本文通过混合整数线性规划和对偶理论，研究了多胜者投票中核心稳定委员会的存在性问题，克服了以往计算方法的局限性，并发现了公平性属性间的新联系。


<details>
  <summary>Details</summary>
Motivation: 在多胜者投票的组公平性研究中，“核心稳定性”（Core Stability）委员会是否存在（特别是在获准投票制下）仍是一个重大的未解决开放问题。现有基于SAT的方法受限于选民人数，效率较低。

Method: 开发了一种基于混合整数线性规划（MILP）的方法，并引入了基于对偶性的重新表述（Duality-based reformulation），用于判定核心稳定委员会的存在性及其条件。

Result: 该方法能够生成与选民人数无关的、针对特定候选人数的证明；并在特定特殊情况下获得了新的委员会存在性结果。

Conclusion: 核心稳定性与可定价性等属性之间存在此前未知的关联，研究通过数学规划和对偶变换为解决多胜者投票中的公平性问题提供了新工具。

Abstract: Core stability is a natural and well-studied notion for group fairness in multi-winner voting, where the task is to select a committee from a pool of candidates. We study the setting where voters either approve or disapprove of each candidate; here, it remains a major open problem whether a core-stable committee always exists. In this work, we develop an approach based on mixed-integer linear programming for deciding whether and when core-stable committees are guaranteed to exist. In contrast to SAT-based approaches popular in computational social choice, our method can produce proofs for a specific number of candidates independent of the number of voters. In addition to these computational gains, our program lends itself to a novel duality-based reformulation of the core stability problem, from which we obtain new existence results in special cases. Further, we use our framework to reveal previously unknown relationships between core stability and other desirable properties, such as notions of priceability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [97] [Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments](https://arxiv.org/abs/2512.15736)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: Anubuddhi是一个多智能体AI系统，能通过自然语言指令自动设计、验证并模拟量子光学实验，实现了高准确度的物理架构建模。


<details>
  <summary>Details</summary>
Motivation: 旨在降低量子光学实验设计的门槛，让不具备专业编程知识的研究者也能通过自然语言设计并模拟复杂的量子物理实验。

Method: 通过多智能体架构（Anubuddhi）结合意图路由、知识增强生成（RAG）、语义检索组件库及双模式验证（QuTiP与FreeSim物理引擎）进行迭代优化。

Result: 在13个涵盖基础物理到先进通信协议的实验中，系统获得8-9/10的设计对齐评分，且证明了自由形式模拟在处理多样化量子任务上优于受限框架。

Conclusion: 系统成功实现了从自然语言到复杂量子光学实验设计的转化，虽然能生成高结构正确性的方案，但仍需专家进行最终的数值分析。

Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.

</details>


### [98] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 本文提出“比例责任原则”（PPD），通过数学模型证明道德责任在不确定性下会从“果断行动”转变为“主动核查”，为构建具备谦虚参数的可审计AI系统提供了跨学科的理论支持。


<details>
  <summary>Details</summary>
Motivation: 传统道德框架在处理不确定性下的决策时存在局限，往往将其视为简单的行动限制，缺乏对责任如何随认知状态变化的动态建模。作者旨在解决不确定性环境下“行动责任”与“验证责任”之间的转化问题。

Method: 通过数学建模引入公式 $D_{total} = K[(1-HI) + HI \times g(C_{signal})]$，将道德责任量化，并结合蒙特卡洛模拟（Monte Carlo simulations）验证系统在不同谦逊系数下的稳定性。同时，跨学科应用于临床伦理、法律、经济治理和AI四个领域进行验证。

Result: 研究发现，道德责任不会随不确定性消失，而是发生转化：当不确定性增加时，“行动责任”按比例转化为“修复责任”（即验证、查询和消除不确定性的责任）。保持基线谦逊系数（$\lambda > 0$）的系统能产生更稳定的责任分配，降低过度自信决策的风险。

Conclusion: 比例责任原则（PPD）为复杂系统中的道德决策提供了一个稳定且可数学建模的框架。它不仅防止了因过度自信导致的鲁棒性缺失，也避免了因不确定性而产生的行动瘫痪，为可审计AI系统的道德设计提供了理论依据。

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [99] [Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions](https://arxiv.org/abs/2512.15743)
*David Noever*

Main category: cs.AI

TL;DR: 本文提出一种将自然语言转化为可物理制造组装指令的框架，通过“积木包”式离散表示法，确保AI生成的3D模型符合几何约束且具备实际可构建性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成3D方法（如扩散模型或传统CAD）在处理复杂组装指令、组件交换及物理可制造性方面存在不足，缺乏一种能连接语义与物理结构的标准化语言。

Method: 开发了一个集成工具的LLM框架，使用LDraw作为富文本中间表示，并配套Python库进行程序化模型生成，通过离散零件库强制执行几何有效性、连接约束和构建顺序。

Result: 成功实现了包含超过3000个零件的复杂模型（如卫星、飞机、建筑）的自动化生成，并能产出物理可实现的步进式组装说明。

Conclusion: 该框架通过引入基于零件的语义表示（LDraw），成功弥合了自然语言意图与可制造物理实体之间的鸿沟，为制造和工程原型设计提供了新的物理API。

Abstract: We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel "bag of bricks" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a "bag of words" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.

</details>


### [100] [Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying](https://arxiv.org/abs/2512.15776)
*Shaun Baek,Sam Liu,Joseph Ukpong*

Main category: cs.AI

TL;DR: 本研究揭示了大模型在非对称信息下的协作缺陷（知识诅咒），证明了相比于被动接收指令，智能体通过“主动提问”减少不确定性能显著平衡信息差并提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在具身环境中存在“符号落地”困难，且容易产生“知识诅咒”倾向，即拥有信息的智能体往往因为缺乏心理理论（Theory of Mind）而无法有效引导信息匮乏的协作伙伴。

Method: 在AI2-THOR环境中设计了一套“非对称辅助推理”框架，分别设置拥有全局信息的“领导者”和传感器受限的“跟随者”，对比研究了主动拉取（Pull）与被动推送（Push）两种通信协议的效果。

Result: 存在明显的“成功差距”：领导者感知成功率为35%，但团队协作成功率仅为17%，意味着50%的可行计划因通信误差失败；此外，主动询问（Pull-based）比标准指令（Push-based）更鲁棒，成功案例中的澄清请求频率是普通案例的2倍。

Conclusion: 在非对称协作中，主动减少不确定性（即“Pull-based”模式）是确保具身智能体协作成功的关键先决条件，对人机协作和多机器人系统具有重要意义。

Abstract: Large Language Models (LLMs) act as powerful reasoning engines but struggle with "symbol grounding" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or "Curse of Knowledge"), where a knowledgeable "Leader" agent fails to guide a sensor-limited "Follower" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant "Success Gap": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a "Pull-based" protocol (active querying) is significantly more robust than standard "Push-based" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.

</details>


### [101] [AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding](https://arxiv.org/abs/2512.16250)
*Sanjoy Chowdhury,Karren D. Yang,Xudong Liu,Fartash Faghri,Pavan Kumar Anasosalu Vasu,Oncel Tuzel,Dinesh Manocha,Chun-Liang Li,Raviteja Vemulapalli*

Main category: cs.AI

TL;DR: 本文引入了 AMUSE 基准来评估多模态模型的对话推理能力，并提出了 RAFT 对齐框架，通过奖励优化和高效参数适应显著提升了模型在复杂音视频交互中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在多发言人对话场景中推理能力不足，难以追踪说话人身份、维持角色状态并跨时间进行事件定位，传统的非智能体评估方式无法有效衡量这些复杂交互。

Method: 1. 提出 AMUSE 基准，涵盖空间-时间说话人定位、对话摘要等 6 类任务。
2. 提出 RAFT 框架：一种数据高效的智能体对齐框架，结合了奖励优化、基于多模态自我评估的内在奖励机制以及选择性参数调整。

Result: 1. 实验发现现有模型在多发言人推理和智能体评估下表现疲软。
2. 使用 RAFT 框架后，在 AMUSE 基准上的准确率获得了高达 39.52% 的相对提升。

Conclusion: AMUSE 基准揭示了当前模型在对话式视频理解中的缺陷，而 RAFT 框架提供了一种高效的途径来增强多模态模型的智能体推理能力。

Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.

</details>


### [102] [AI Epidemiology: achieving explainable AI through expert oversight patterns](https://arxiv.org/abs/2512.15783)
*Kit Tempest-Walters*

Main category: cs.AI

TL;DR: 本文提出“AI 流行病学”框架，借鉴医学流行病学方法，通过分析专家与 AI 交互的群体统计模式而非模型内部逻辑，实现大规模 AI 系统的安全监控和风险预测。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性方法（如 SHAP）在处理大规模部署的多样化模型时，因模型复杂性过高而难以奏效，且在模型更新时缺乏治理连续性。

Method: 通过标准化捕获 AI 与专家的交互数据（如风险等级、对齐和准确性评分），将其作为“暴露变量”，利用统计关联性（而非内部机制分析）来预测输出失效。

Result: 该框架实现了无感的专家审计跟踪，提供了自动化审计路径，并能通过语义评估（如识别类似曾被否决的案例）在伤害发生前检测不可靠的输出。

Conclusion: AI 流行病学为治理黑盒模型提供了一种实用、可持续且用户友好的方案，有助于在复杂的实际部署中建立信任，并将 AI 的监管权重新交还给领域专家。

Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.

</details>


### [103] [Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM](https://arxiv.org/abs/2512.15784)
*Zibin Liu,Cheng Zhang,Xi Zhao,Yunfei Feng,Bingyu Bai,Dahu Feng,Erhu Feng,Yubin Xia,Haibo Chen*

Main category: cs.AI

TL;DR: MOBIMEM 是一个为移动端设计的 memory-centric Agent 系统，通过解耦模型权重与 Agent 进化，利用三种专用存储原语和类 OS 调度机制，在不微调模型的情况下实现了高效的个性化自进化、任务成功率提升及显著的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM Agent 在部署后难以实现自我进化（个性化、能力提升和效率优化）。传统的模型微调方法计算开销过大，且在模型精度与推理效率之间存在难以调和的权衡。

Method: 提出了以存储为中心（Memory-centric）的系统架构：\n1. 引入三种内存原语：个人偏好存储 (Profile Memory)、经验逻辑存储 (Experience Memory) 和 动作细节存储 (Action Memory)。\n2. 集成类操作系统服务：包括调度器、Agent 回放机制 (AgentRR) 以及上下文感知异常处理。

Result: 在 AndroidWorld 和前 50 常用应用上的测试显示：\n1. 个人偏好对齐度达 83.1%，检索速度比 GraphRAG 快 280 倍。\n2. 任务成功率最高提升 50.3%。\n3. 移动端端到端延迟降低了 9 倍。

Conclusion: MOBIMEM 证明了通过“以存储为中心”的架构设计，可以在不重新训练模型的情况下，显著提升 LLM Agent 的个性化水平、任务成功率和运行效率，是移动端 Agent 自进化的一种高效方案。

Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.

</details>


### [104] [State-Augmented Graphs for Circular Economy Triage](https://arxiv.org/abs/2512.15824)
*Richard Fox,Rui Li,Gustav Jonsson,Farzaneh Goli,Miying Yang,Emel Aktas,Yongjing Wang*

Main category: cs.AI

TL;DR: 本文开发了一个基于图论的决策框架，通过状态增强的拆解规划，为废旧产品（如EV电池）在循环经济中的去向（重用、回收等）提供最优分选建议。


<details>
  <summary>Details</summary>
Motivation: 有效的循环经济（CE）分选需要平衡产品剩余价值与处理/人力成本，目前缺乏能应对复杂运营约束、适应不同产品健康状况的自动化决策工具。

Method: 提出一种基于状态增强拆解序列规划（DSP）图的确定性求解框架。通过将拆解历史编码到状态中以满足马尔可夫属性，实现递归的寻优评估。

Result: 通过电动汽车（EV）电池的分选架构验证了框架的灵活性。该模型集成了基于健康评分的效用评估，并在复杂的机械、安全和经济约束下实现了最优路径选择。

Conclusion: 该统一的形式化方法为优化不同产品和运营背景下的循环经济分选决策提供了一个易于处理且具有通用性的基础。

Abstract: Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.

</details>


### [105] [PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations](https://arxiv.org/abs/2512.15894)
*Vahideh Zolfaghari*

Main category: cs.AI

TL;DR: 本文通过新构建的 PediatricAnxietyBench 基准，评估发现 Llama 等大语言模型在应对家长焦虑和对抗性压力时的儿科咨询安全性不足，特别是容易在压力下给出不恰当的诊断建议。


<details>
  <summary>Details</summary>
Motivation: 虽然家长越来越多地向 LLM 寻求儿科指导，但这些模型在应对现实世界中焦虑情绪带来的对抗性压力（如紧迫性语言）时的安全性能尚不明确，可能引发有害建议。

Method: 构建了包含 300 个高质量查询的开源基准测试 PediatricAnxietyBench，涵盖 10 个儿科主题，包含患者来源及对抗性查询（如紧迫性、经济障碍等）。评估了 Llama 70B 和 8B 模型在诊断克制、转诊建议、对冲表述及紧急识别方面的表现。

Result: 模型平均安全得分仅为 5.50/15。70B 模型在安全性上优于 8B 模型，但对抗性查询导致安全性下降了 8%（其中紧迫性影响最大）。在癫痫和疫苗接种后的诊断上存在明显漏洞，且普遍缺乏紧急情况识别能力。

Conclusion: 模型规模虽然能提升安全性，但在面对现实中父母的焦虑压力时仍普遍存在漏洞。PediatricAnxietyBench 为揭示临床上的重大故障模式提供了一个可重复的评估框架。

Abstract: Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.

</details>


### [106] [Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries](https://arxiv.org/abs/2512.15906)
*Jonathan A. Handler*

Main category: cs.AI

TL;DR: 本文介绍了 Darth Vecdor (DV)，这是一个开源工具，旨在将大语言模型的知识提取并转化为结构化的 SQL 知识库，以解决直接使用 LLM 时的效率和准确性问题，特别是在医疗领域。


<details>
  <summary>Details</summary>
Motivation: 虽然 LLM 拥有海量知识，但在高吞吐量操作中存在成本高、速度慢、安全性存疑及置信度低等问题，直接查询 SQL 数据库比直接查询 LLM 更具优势。

Method: 开发名为 Darth Vecdor (DV) 的软件，提供基于浏览器的图形界面，支持提示词工程，并内置了针对错误、离题、非结构化及不一致响应的缓解机制。

Result: 成功构建了一个可扩展的开源系统，能够将 LLM 知识提取到结构化的、经过术语映射的 SQL 知识库中，同时降低了非技术专家使用该技术的门槛。

Conclusion: DV 作为一个开源工具，通过将 LLM 知识结构化为 SQL 数据库，为医疗等领域的知识提取提供了新途径，但用户需严格把控其潜在的 Bug 和风险。

Abstract: Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database ("knowledge base" or "knowledge graph"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an "as is" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.

</details>


### [107] [Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems](https://arxiv.org/abs/2512.15922)
*Jovan Pavlović,Miklós Krész,László Hajdu*

Main category: cs.AI

TL;DR: 本文提出一种利用扩散激活算法在自动构建的知识图谱上检索信息的新型RAG框架，旨在通过增强多跳推理能力提升大模型在复杂任务中的表现，且能作为插件应用于多种RAG系统。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在处理复杂多步推理时难以可靠关联证据，而现有GraphRAG方案过度依赖高质量预构建图谱或在大模型引导图遍历时存在可靠性与成本问题。

Method: 提出一种新型RAG框架，通过自动构建知识图谱连接文档语料，并采用“扩散激活（Spreading Activation）”算法进行信息检索，而非单纯依赖大模型引导图遍历。

Result: 实验表明该方法优于或等同于迭代式RAG。与思维链（CoT）迭代检索结合时，在小规模开源模型上比原生RAG的回答准确率提升了高达39%的绝对值。

Conclusion: 结合自动图构建与扩散激活算法是处理复杂推理任务的有效途径，特别是对于计算资源有限的环境，该方法具有显著的实用价值。

Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.

</details>


### [108] [Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning](https://arxiv.org/abs/2512.15943)
*Polaris Jhandi,Owais Kazi,Shreyas Subramanian,Neel Sendas*

Main category: cs.AI

TL;DR: 本文研究了如何通过微调350M参数的小语言模型（SLM）来替代高成本的LLM，实验证明优化后的SLM在特定任务上的性能显著优于ChatGPT等大型模型，大幅提升了成本效益。


<details>
  <summary>Details</summary>
Motivation: 随着企业规模化应用生成式AI，LLM的高昂计算成本和资源消耗成为障碍，因此需要探索轻量级的小语言模型（SLMs）以降低成本并提高运营效率。

Method: 使用Hugging Face的TRL库和SFT（监督微调）训练器，对facebook/opt-350m模型进行单epoch的领域自适应微调，使其执行文档摘要、问答和结构化数据解释等任务。

Result: 微调后的SLM在ToolBench评估中获得了77.55%的通过率，远超ChatGPT-CoT (26.00%)、ToolLLaMA-DFS (30.18%)等基准模型。

Conclusion: 经过精心设计和针对性训练的SLM（如OPT-350M）在大规模生产系统中具有替代LLM的潜力，能显著降低企业采用生成式AI的经济和技术门槛。

Abstract: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.

</details>


### [109] [Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting](https://arxiv.org/abs/2512.16022)
*Defu Cao,Michael Gee,Jinbo Liu,Hengxuan Wang,Wei Yang,Rui Wang,Yan Liu*

Main category: cs.AI

TL;DR: 本文通过 R1 风格微调，将 LLM 转化为具备因果解释能力的智能集成器，通过动态协调多个时间序列基础模型，实现了 SOTA 的预测精度与解释性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型层出不穷，但无单一模型能胜任所有场景。同时，直接将 LLM 用于预测效果不佳，亟需一种能协调多模型并提供可解释性的集成方法。

Method: 提出一种将 LLM 作为智能评判者的方案，利用 R1 风格的多轮对话微调，并引入基于 SHAP 的忠实度评分（faithfulness scores）作为引导，使模型能将集成权重转化为因果逻辑解释。

Result: 在 GIFT-Eval 基准（涵盖 23 个数据集、97 种设置）上，该方法在 CRPS 和 MASE 指标上均显著优于现有的领先模型，刷新了 SOTA 记录。

Conclusion: LLM 可以作为具备可解释性的时间序列集成协调者，其通过微调获得的时间序列领域知识，能够有效解决多模型集成中的不确定性问题。

Abstract: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.

</details>


### [110] [Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets](https://arxiv.org/abs/2512.16030)
*Lukas Nel*

Main category: cs.AI

TL;DR: 本文通过新基准测试发现，当前尖端大语言模型在预测未来事件时普遍过度自信，且模型规模或推理能力的提升并未改善这一校准问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型性能强大，但其在面对训练集之外的未知未来事件时，能否准确评估自身不确定性（即认识论校准）尚不明确，且缺乏使用真实世界结果进行验证的基准测试。

Method: 引入新基准 KalshiBench，包含从合规交易所 Kalshi 获取的 300 个具有真实世界验证结果的预测市场问题（事件均发生在模型训练截止日期后）。通过评估 Claude、GPT 等 5 款前沿模型的预测置信度与实际准确率的匹配程度（使用 ECE 和 Brier Skill Score 等指标）进行分析。

Result: 所有模型普遍存在过度自信问题；即使是最优模型（Claude Opus 4.5）也有明显的校准误差；具有更强推理能力的新型模型（如 GPT-5.2-XHigh）在校准表现上反而更差；大多数模型的预测表现甚至不如简单的频率基准预测。

Conclusion: 模型参数规模的扩大和推理能力的增强并不能自动改善校准性能。认识论校准（Epistemic Calibration）是一项独立的能力，需要专门的针对性开发。

Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.

</details>


### [111] [Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education](https://arxiv.org/abs/2512.16036)
*Diane Myung-kyung Woodbridge,Allyson Seba,Freddie Seba,Aydin Schwartz*

Main category: cs.AI

TL;DR: 本文开发并评估了一个结合主题建模与大语言模型的自动化系统，用于发现、分类和解析高校大纲及网站中的生成式 AI 相关政策，以解决学生对 AI 使用规范不明确的问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式 AI 在学术中的广泛使用，教育机构虽制定了政策，但标准不一且在不断演变，导致学生对合规使用 GenAI 的界限感到困惑。

Method: 结合了无监督主题建模（Topic Modeling）技术用于识别政策主题，以及大语言模型（LLMs，如 GPT-4.0）用于对 GenAI 的允许程度和具体要求进行自动分类。

Result: 系统在主题发现方面的连贯性得分（Coherence Score）为 0.73；在八个政策主题的分类任务中，GPT-4.0 的精确率达到 0.92-0.97，召回率达到 0.85-0.97。

Conclusion: 该自动化系统通过提供结构化且可解释的政策信息，有助于学生理解并遵守 GenAI 使用规定，推动生成式 AI 在教育中的安全与公平应用。

Abstract: As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.

</details>


### [112] [WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning](https://arxiv.org/abs/2512.16108)
*Wendong Bi,Yirong Mao,Xianglong Liu,Kai Tian,Jian Zhang,Hanjie Wang,Wenhui Que*

Main category: cs.AI

TL;DR: 本文推出 WeMusic-Agent 框架及其模型 M1，通过 50B 语料预训练和智能体机制，实现内化音乐知识与外部工具调用的无缝结合，并发布了首个基于真实数据的对话式音乐推荐评准。


<details>
  <summary>Details</summary>
Motivation: 现有的对话式音乐推荐方法难以平衡专业领域知识的掌握与外部工具（如检索 API）的灵活集成。

Method: 提出 WeMusic-Agent 框架，结合“知识内化”与“智能体边界学习”；开发了 WeMusic-Agent-M1 模型（在 50B 音乐相关语料上持续预训练）；并构建了基于微信听一听真实数据的开源评估基准。

Result: 实验证明 WeMusic-Agent-M1 在相关性、个性化和多样性等多个维度上显著优于现有模型，并展现了精准的工具调用决策能力。

Conclusion: WeMusic-Agent 能够通过平衡内化知识与外部工具调用，有效解决对话式音乐推荐中的复杂需求，并在真实场景中展现了优越性能。

Abstract: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.

</details>


### [113] [ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs](https://arxiv.org/abs/2512.16149)
*Hao Chen,Zhexin Hu,Jiajun Chai,Haocheng Yang,Hang He,Xiaohan Wang,Wei Lin,Luhang Wang,Guojun Yin,Zhuofeng zhao*

Main category: cs.AI

TL;DR: ToolForge 是一个自动化的合成数据框架，通过虚拟工具和多层验证生成高质量多跳工具调用数据。仅需 8B 参数的模型在此数据训练后，性能即可超越 GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 工具调用合成数据生成方案依赖数万次真实 API 调用，导致成本极高，且普遍缺乏多跳推理和自我反思能力。

Method: 提出 ToolForge 框架：利用 (问题, 核心上下文, 答案) 三元组构建少量虚拟工具，摒弃真实 API 调用。引入多跳推理和自我反思机制增强数据，并结合规则与模型引导的“多层验证框架”确保数据质量。

Result: 基于 ToolForge 合成数据训练的 8B 参数模型在多个基准测试中表现优于 GPT-4o。

Conclusion: ToolForge 证明了通过高质量、低成本的合成数据训练，较小参数规模的开源模型能够在复杂工具调用任务上超越顶级闭源模型。

Abstract: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .

</details>


### [114] [Science Consultant Agent](https://arxiv.org/abs/2512.16171)
*Karthikeyan K,Philip Wu,Xin Tang,Alexandre Alves*

Main category: cs.AI

TL;DR: Science Consultant Agent 是一个基于 Web 的 AI 工具，通过问卷引导和文献推荐，协助用户快速筛选 AI 建模策略并生成原型。


<details>
  <summary>Details</summary>
Motivation: 旨在帮助从业者（如产品经理、开发人员及研究员）在面对 AI 解决方案时，能够更高效地选择并实施最有效的建模策略。

Method: 构建了一个包含四个核心模块（问卷调查、智能填充、研究导向推荐及原型构建器）的 Web 平台，整合结构化咨询与自动化代码生成。

Result: 实现了从需求输入到研究驱动的建议，再到可运行原型生成的全流程自动化，加速了 AI 项目的开发进程。

Conclusion: 该工具不仅能通过文献支持提高方案的可靠性，还能显著缩短从需求分析到原型开发的周期。

Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.

</details>


### [115] [PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving](https://arxiv.org/abs/2512.16214)
*Jianming Liu,Ren Zhu,Jian Xu,Kun Ding,Xu-Yao Zhang,Gaofeng Meng,Cheng-Lin Liu*

Main category: cs.AI

TL;DR: PDE-Agent 是首个结合 LLM 与外部工具链的多智能体框架，能够根据自然语言描述自动、自愈地求解偏微分方程（PDE）。


<details>
  <summary>Details</summary>
Motivation: 传统 PDE 求解方法及现有的 PINN/DeepXDE 框架仍高度依赖专家知识和手动设置，缺乏处理自然语言描述并自主求解 PDE 的能力。

Method: 提出 PDE-Agent 框架，包含两个核心创新：(1) Prog-Act 框架配合图记忆机制，通过双循环（局部修复与全局修订）实现动态规划与纠错；(2) 资源池（Resource-Pool）集成工具参数分离机制，用于管理运行时制品并解决工具间的依赖问题。

Result: 在构建的新基准测试 PDE-Bench 上的评估表明，PDE-Agent 在处理复杂、多步骤且具有跨步骤依赖的 PDE 求解任务中表现出卓越的适用性和性能。

Conclusion: PDE-Agent 为自动化科学计算开辟了新范式，通过 LLM 驱动的多智能体协作，显著降低了 PDE 求解的门槛并提高了复杂任务的处理能力。

Abstract: Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.

</details>


### [116] [Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis](https://arxiv.org/abs/2512.16237)
*Zhi Helu,Huang Jingjing,Xu Wang,Xu Yangbin,Zhang Wanyue,Jiang Baoyang,Deng Shirui,Zhu Liang,Li Fangfang,Zhao Tiejun,Lin Yankai,Yao Yuan*

Main category: cs.AI

TL;DR: 本文提出 SPRITE 框架，通过 LLM 编写验证程序并结合模拟器元数据，自动合成高精度、高多样性的空间推理数据，有效解决了空间智能模型训练中的数据瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLM）在空间理解与推理方面受限，现有数据集要么结构僵化（基于模板），要么难以扩展且精度不足（人工标注），缺乏兼具多样性与计算精确性的数据。

Method: 提出 SPRITE 框架，将 ground-truth 生成重构为代码生成任务：利用 LLM 将复杂的空间问题编译为可执行程序，并在模拟器的元数据中进行验证，从而实现自动化、高精度、多样化的指令数据合成。

Result: 构建了一个涵盖 3 种模拟器、1.1 万个场景和 30 万条图文/视频指令对的数据集。在该数据上训练的模型在多个空间基准测试中表现显著提升，优于同等规模的开源数据集。

Conclusion: SPRITE 为具身智能空间推理提供了一个可扩展且精确的数据生成方案，证明了提高数据语言多样性对于构建泛化性强的空间智能至关重要。

Abstract: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.

</details>


### [117] [AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints](https://arxiv.org/abs/2512.16245)
*Aniruddha Roy,Jyoti Patel,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: AlignMerge 是一种新型几何感知模型合并框架，通过在合并过程中显式约束对齐子空间，在提升模型任务能力的同时，有效解决了传统合并方法容易破坏模型安全对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 合并方法（如权重汤、任务向量等）虽能保持性能，但往往会破坏模型的对齐安全性。作者认为合并应以对齐后的模型为锚点，从几何角度确保安全性，而非事后修复。

Method: 提出 AlignMerge 框架。该方法在指令微调基座的局部 Fisher 图谱中评估对齐子空间，并优化一个包含几何损失（L_geo）、对齐损失（L_align）和软对齐预算（L_bud）的复合损失函数。同时，利用对齐质量指数（AQI）作为潜空间判别标准。

Result: 在 LLaMA-3、Mistral 等五大模型家族的实验显示，AlignMerge 在对齐指标（毒性、LLM 评分）上表现更优，同时在推理和帮助性上达到或超越了最佳专家模型，且对齐子空间漂移控制优于 TIES 和 SafeMerge 等基准方法。

Conclusion: 模型合并应被视为一种受几何约束的操作，而非简单的数值技巧。AlignMerge 将对齐作为合并过程中的显式不变量，为未来基础模型实现保安全性的几何感知组合提供了可行路径。

Abstract: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.

</details>


### [118] [OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models](https://arxiv.org/abs/2512.16295)
*Zhenyu Wu,Jingjing Xie,Zehao Li,Bowen Yang,Qiushi Sun,Zhaoyang Liu,Zhoumianze Liu,Yu Qiao,Xiangyu Yue,Zun Wang,Zichen Ding*

Main category: cs.AI

TL;DR: 本文提出 OS-Oracle 框架，通过合成高质量数据和创新的训练方法，训练出能精准评估 GUI 操作行为的批判模型，有效解决了计算机助手在复杂操作中的错误积累问题。


<details>
  <summary>Details</summary>
Motivation: VLM 驱动的计算机使用智能体在长流程操作中容易积累错误且动作不可逆，但目前缺乏多样化、高质量的 GUI 反馈数据及其对应的步级（step-level）评估基准。

Method: 1) 建立可大规模合成跨平台 GUI 批判数据的流水线；2) 采用 SFT 加 一致性保持组相对策略优化（CP-GRPO）的两阶段训练范式；3) 构建涵盖移动、网页和桌面端的多平台基准测试 OS-Critic Bench。

Result: 发布了包含 310k 样本的数据集；模型 OS-Oracle-7B 在开源 VLM 中达到 SOTA，在移动端超越闭源模型；作为预检测器显著提升了 UI-TARS 等智能体在 OSWorld 和 AndroidWorld 中的表现。

Conclusion: OS-Oracle 框架能有效提升现有 GUI 智能体的任务成功率，为构建更可靠、可纠错的计算机操作智能体提供了关键的基础设施。

Abstract: With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.

</details>


### [119] [Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection](https://arxiv.org/abs/2512.16300)
*Fanrui Zhang,Qiang Zhang,Sizhuo Zhou,Jianwen Sun,Chuanhao Li,Jiaxin Ai,Yukang Feng,Yujie Zhang,Wenjie Li,Zizhen Li,Yifan Chang,Jiawei Liu,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 提出了ForenAgent，这是一个基于多模态大模型的交互式图像取证框架，通过多轮推理和自主调用底层工具，实现了高层语义与底层特征的有效统一，并发布了大规模取证数据集FABench。


<details>
  <summary>Details</summary>
Motivation: 现有的图像篡改检测方法要么仅利用低层工件，要么依赖高层语义，两者在范式和推理上高度异构，难以统一或有效建模跨层交互。

Method: 提出了ForenAgent框架，该框架采用两阶段训练（冷启动与强化微调），并设计了模拟人类推理的动态循环（全局感知、局部关注、迭代探测及整体判别）。此外，构建了高质量数据集FABench。

Result: 在挑战性IFD任务中表现卓越，能够自主生成、执行并迭代优化Python工具，实现了更灵活、可解释的伪造分析。

Conclusion: ForenAgent展示了涌现出的工具使用能力和反思性推理能力，为构建通用图像取证系统提供了一条极具前景的路径。

Abstract: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.

</details>


### [120] [Adaptation of Agentic AI](https://arxiv.org/abs/2512.16301)
*Pengcheng Jiang,Jiacheng Lin,Zhiyi Shi,Zifeng Wang,Luxi He,Yichen Wu,Ming Zhong,Peiyang Song,Qizheng Zhang,Heng Wang,Xueqiang Xu,Hanwen Xu,Pengrui Han,Dylan Zhang,Jiashuo Sun,Chaoqi Yang,Kun Qian,Tian Wang,Changran Hu,Manling Li,Quanzheng Li,Hao Peng,Sheng Wang,Jingbo Shang,Chao Zhang,Jiaxuan You,Liyuan Liu,Pan Lu,Yu Zhang,Heng Ji,Yejin Choi,Dawn Song,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 本文通过建立一个包含智能体适配和工具适配的统一框架，系统性地分析了构建前沿Agentic AI系统的各类策略、权衡及未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI能力的提升，如何通过适配（Adaptation）来协同智能体、任务与工具，提高系统的性能、可靠性和泛化能力成为核心挑战。

Method: 提出一种统一的系统化框架，将研究景观细分为：1. 智能体适配（由工具执行信号或智能体输出信号驱动）；2. 工具适配（智能体无关型或智能体监督型）。

Result: 通过该框架明确了不同适配策略的设计空间与权衡（Trade-offs），并系统性地梳理了各领域的代表性方法及其优缺点。

Conclusion: 该框架为开发者提供了从“选择何种适配策略”到“如何权衡性能与成本”的实践指南，是构建高效智能体系统的理论基石。

Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.

</details>


### [121] [AI Needs Physics More Than Physics Needs AI](https://arxiv.org/abs/2512.16344)
*Peter Coveney,Roger Highfield*

Main category: cs.AI

TL;DR: 本文评述了当前AI在科学应用中的局限性，提出物理学应引领AI进化，通过结合理论严谨性与机器学习，构建下一代“大人工智能”。


<details>
  <summary>Details</summary>
Motivation: 尽管AI备受关注，但在科研和商业领域的实际衡量影响有限。当前AI模型存在参数冗余、分布偏差、缺乏不确定性量化及机械性见解等缺陷，无法捕捉基本科学规律。

Method: 通过回顾现有AI架构（如LLM、推理模型）的局限性，探讨量子人工智能和模拟计算的潜力，并提出一种结合“理论严谨性”与“机器学习灵活性”的“大人工智能”发展路径。

Result: 指出当前主流AI架构在处理复杂科学问题时的物理学局限性，并描绘了物理学赋能人工智能（尤其是量子与模拟计算方向）的路线图。

Conclusion: 物理学应当为AI注入理论严谨性，通过“大人工智能（Big AI）”驱动科研范式变革，而非仅仅将AI视作黑盒工具。

Abstract: Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.

</details>


### [122] [PCIA: A Path Construction Imitation Algorithm for Global Optimization](https://arxiv.org/abs/2512.16392)
*Mohammad-Javad Rezaei,Mozafar Bag-Mohammadi*

Main category: cs.AI

TL;DR: 本文提出了一种模仿人类路径构建行为的新型元启发式优化算法（PCIA），通过在 66 个基准测试中的出色表现验证了其强大的寻优能力。


<details>
  <summary>Details</summary>
Motivation: 受人类在日常生活中选择、组合及创造交通路径的行为启发，旨在开发一种能够平衡探索与开发的全新群优化算法。

Method: 通过模拟人类构建和使用路径的行为，结合路径选择的随机性、现有路径的智能整合以及应对路径关闭时的重新构建机制，建立数学模型进行寻优。

Result: 在 53 个数学优化问题和 13 个受限优化问题的测试中，PCIA 的表现优于或相当于当前流行及最新的元启发式算法。

Conclusion: PCIA 是一种极具竞争力的优化工具，能够有效解决复杂的数学及受限优化问题，为元启发式算法家族增添了新的有效成员。

Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.

</details>


### [123] [Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs](https://arxiv.org/abs/2512.16424)
*Nguyen Xuan-Vu,Daniel Armstrong,Milena Wehrbach,Andres M Bran,Zlatko Jončev,Philippe Schwaller*

Main category: cs.AI

TL;DR: 本文介绍了 Synthelite，这是一个基于 LLM 的合成规划框架，支持化学家通过自然语言进行人机交互，在多种复杂约束下实现了极高的合成路径搜索成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机辅助合成规划（CASP）工具缺乏与人类专家互动的机制，难以整合化学家的专业见解和特定约束意图。

Method: 提出 Synthelite 框架，利用大语言模型（LLMs）直接提出逆合成转化，并结合 LLM 的化学推理能力与自然语言提示（prompts），允许人类专家介入引导合成路径。

Result: Synthelite 在受策略约束和起始原料约束的合成任务中均达到了 95% 的成功率，能够灵活调整规划轨迹，并能充分考虑化学可行性。

Conclusion: Synthelite 展示了 LLM 能够作为合成规划的核心“编排器”，通过自然语言交互缩小了自动化工具与化学家直觉之间的差距。

Abstract: Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.

</details>


### [124] [TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles](https://arxiv.org/abs/2512.16442)
*Allard Oelen,Sören Auer*

Main category: cs.AI

TL;DR: 本文介绍了 TIB AIssistant，这是一个集成多种 AI 助手的科研平台，旨在辅助整个研究生命周期，并通过 RO-Crate 确保研究过程的可复现性和透明度。


<details>
  <summary>Details</summary>
Motivation: 人工智能（特别是大语言模型）正在深刻影响学术界，研究人员需要一个集成化工具来支持整个研究生命周期，并解决科研透明度和可重复性问题。

Method: 开发了一个包含多个专用助手和外部学术服务接口的平台，利用 RO-Crate 标准存储和导出数据，并通过生成研究论文草稿的顺序演示来验证功能。

Result: 成功实现了一套能够协同工作以生成研究论文各章节的助手集合，并能导出符合标准的数据包以增强科研诚信。

Conclusion: TIB AIssistant 为学术界提供了一个社区维护的、透明且可重复的 AI 辅助研究基础平台。

Abstract: The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.

</details>


### [125] [Towards AI-Supported Research: a Vision of the TIB AIssistant](https://arxiv.org/abs/2512.16447)
*Sören Auer,Allard Oelen,Mohamad Yaser Jaradeh,Mutahira Khalid,Farhana Keya,Sasi Kiran Gaddipati,Jennifer D'Souza,Lorenz Schlüter,Amirreza Alasti,Gollam Rabby,Azanzi Jiomekong,Oliver Karras*

Main category: cs.AI

TL;DR: 本文介绍了 TIB AIssistant，这是一个通用的科研人机协作平台，旨在通过模块化架构集成 AI 智能体，辅助研究人员完成从构思到写作的全生命周期科研任务。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式 AI 潜力巨大，但由于领域需求差异、AI 素养受限、工具协调复杂以及准确性不明，将其整合到科研流程中仍面临挑战。

Method: 提出了一个包含提示词库、工具库、共享数据存储和灵活编排框架的模块化架构，开发了支持跨学科任务的科研辅助原型系统。

Result: 构建了一个领域中立的协作平台，能够辅助研究者进行构思、文献分析、方法论制定、数据分析和学术写作。

Conclusion: TIB AIssistant 的早期原型证明了通过人机协作平台增强科研全生命周期任务的可行性。

Abstract: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.

</details>


### [126] [TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries](https://arxiv.org/abs/2512.16453)
*Jiayang Yang,Chunhui Zhao,Martin Guay,Zhixing Cao*

Main category: cs.AI

TL;DR: 本文提出TS2R框架，将电池运行数据转化为结构化报告，使大模型无需微调即可在电池管理中达到专家级分析与决策水平。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在处理多变量时序数据方面有潜力，但在电池储能系统（BESS）的运维应用中仍未得到充分探索，且低级传感器信号与高层语境决策之间存在鸿沟。

Method: 提出TS2R框架，通过分段、语义抽象和基于规则的解释，将原始时序信号编码为结构化的自然语言报告。

Result: 在异常检测、SOC预测和充放电管理任务中，TS2R在准确性、鲁棒性和可解释性上均优于视觉、嵌入和纯文本提示基线。

Conclusion: TS2R为锂电池储能系统的自适应、LLM驱动智能管理开辟了实际路径，无需重新训练即可实现专家级决策。

Abstract: Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.

</details>


### [127] [cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution](https://arxiv.org/abs/2512.16465)
*Jinwu Chen,Qidie Wu,Bin Li,Lin Ma,Xin Si,Yang Hu,Shouyi Yin,Jun Yang*

Main category: cs.AI

TL;DR: 本文提出 cuPilot，一个利用策略协调多智能体进化优化的框架，能自动生成高性能 CUDA 核函数，性能显著超越 PyTorch。


<details>
  <summary>Details</summary>
Motivation: 优化 CUDA 核函数极度耗时且需要专业知识，现有的 LLM + 进化算法方案因智能体设计不佳和进化表示不匹配，导致生成的算子性能不理想。

Method: 提出了 cuPilot 多智能体框架，利用策略协调进化算法、Roofline 模型引导的提示词（prompting）以及策略级种群初始化来优化 CUDA 算子。

Result: 在 100 个算子基准测试中，cuPilot 生成的核函数平均比 PyTorch 快 3.09 倍；在 GEMM 任务中实现了高度的硬件效率和复杂的优化策略。

Conclusion: cuPilot 证明了通过引入“策略”级别的中间语义和多智能体协作，可以使 LLM 在复杂的 GPU 算子优化任务中达到极高性能，且具备开源参考价值。

Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.

</details>


### [128] [Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery](https://arxiv.org/abs/2512.16468)
*Danial Safaei,Siddartha Khastgir,Mohsen Alirezaei,Jeroen Ploeg,Son Tong,Xingyu Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种新的保真度指标 DFF，用于衡量自动驾驶模型在仿真与现实环境中是否基于相同的决策特征做出判断，并以此提升仿真器的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶仿真测试过于关注视觉真实感（像素级），但即便图像看起来真实，受测系统（SUT）在仿真和现实中的决策机制可能并不一致。目前缺乏一种衡量这种“行为/因果机制一致性”的指标。

Method: 引入“决策特征保真度（DFF）”度量，利用可解释 AI（XAI）和反事实解释来识别并比较 SUT 在真实与合成图像中的决策依据，并提出 DFF 引导的仿真器校准方案。

Result: 在 KITTI-VirtualKITTI2 数据集上的实验表明，DFF 能发现传统输出值指标无法察觉的差异；采用 DFF 引导的校准能有效提升决策特征的一致性，且不损害原有输出精度。

Conclusion: DFF 为评估和提升自动驾驶仿真器保真度提供了新的视角，强调了决策机制一致性较于单纯视觉真实感的重要性。

Abstract: Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images "look real" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.

</details>


### [129] [Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network](https://arxiv.org/abs/2512.16491)
*Theresa Eimer,Lennart Schäpermeier,André Biedenkapp,Alexander Tornede,Lars Kotthoff,Pieter Leyman,Matthias Feurer,Katharina Eggensperger,Kaitlin Maile,Tanja Tornede,Anna Kozak,Ke Xue,Marcel Wever,Mitra Baratchi,Damir Pulatov,Heike Trautmann,Haniye Kashgarani,Marius Lindauer*

Main category: cs.AI

TL;DR: 本文是一份针对元算法（Meta-algorithmics）研究的实践指南，旨在解决实验设计复杂、成本高昂及标准不统一的问题，涵盖了从提问到结果呈现的完整生命周期。


<details>
  <summary>Details</summary>
Motivation: 元算法研究（如算法选择、配置和调度）涉及复杂的实验设计，存在大量潜在错误源，且现有的最佳实践分散在不同领域。此外，高昂的计算成本也威胁着科研发现的可扩展性和有效性。

Method: 由于这是一篇综述型报告，作者通过整合来自COSEAL（配置与选择算法小组）社区下各子领域的分散经验，覆盖了从实验设计、执行到结果分析的完整流程。

Result: 汇总出了一套覆盖整个实验周期的“最佳实践”集合，旨在减少实验误差，提升科学洞察的可重复性和效力。

Conclusion: 该报告确立了元算法研究的现状基准，并为该领域的科研人员和从业者提供了一套标准化的实践指南。

Abstract: Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.

</details>


### [130] [Scaling Laws for Energy Efficiency of Local LLMs](https://arxiv.org/abs/2512.16531)
*Ander Alvarez,Alessandro Genuardi,Nilotpal Sinha,Antonio Tiene,Samuel Mugel,Román Orús*

Main category: cs.AI

TL;DR: 本文系统研究了纯 CPU 边缘设备上的 LLM 和 VLM 推理规律，揭示了计算成本随长度和分辨率的变化特征，并证明了量子启发式压缩在降低能耗方面的显著效果。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署大模型需权衡精度与能耗，尽管大多数消费级硬件依赖 CPU，但目前尚缺乏针对纯 CPU 推理计算规律的系统研究。

Method: 在 MacBook Pro M2（移动端）和 Raspberry Pi 5（嵌入式）上，采用基于处理器/内存连续采样及 AUC 积分的统一基准测试方法，系统评估 LLM 和 VLM 的推理性能。

Result: 发现了两项经验法则：LLM 推理成本随 Token 长度线性扩展；VLM 存在“分辨率拐点”。此外，量子启发式压缩最高可减少 71.9% 的资源占用和 62% 的能耗，且不损失精度。

Conclusion: CPU 仍然是边缘 AI 部署的关键平台，通过模型压缩和输入预处理优化，可以在受限硬件上实现高效且可持续的本地多模态推理。

Abstract: Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

</details>


### [131] [Prefix Probing: Lightweight Harmful Content Detection for Large Language Models](https://arxiv.org/abs/2512.16650)
*Jirui Yang,Hengqi Guo,Zhihui Lu,Yi Zhao,Yuansen Zhang,Shijing Hu,Qiang Duan,Yinggui Wang,Tao Wei*

Main category: cs.AI

TL;DR: 本文提出 Prefix Probing，利用前缀对比和缓存机制，在极低延迟和零额外部署成本下实现高效的大模型有害内容检测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际安全应用中面临检测准确率、推理延迟和部署成本之间的三方权衡难题。

Method: 提出 Prefix Probing 方法：1. 对比“执行”与“拒绝”类前缀的条件对数概率；2. 利用前缀缓存（Prefix Caching）降低延迟；3. 设计自动化构件算法以筛选高判别性的探测前缀。

Result: 该方法仅需单次对数概率计算，延迟接近首个 Token 输出；实验证明其检测效果与主流外部安全模型相当，但计算成本极低且无需额外部署模型。

Conclusion: Prefix Probing 是一种实用且高效的黑盒有害内容检测方案，通过创新的前缀对比机制，在不增加部署负担的情况下实现了高性能安全防护。

Abstract: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of "agreement/execution" versus "refusal/safety" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.

</details>


### [132] [Comprehensive AI Literacy: The Case for Centering Human Agency](https://arxiv.org/abs/2512.16656)
*Sri Yash Tadimalla,Justin Cary,Gordon Hull,Jordan Register,Daniel Maxwell,David Pugalee,Tina Heafner*

Main category: cs.AI

TL;DR: 本文呼吁将AI教育重心从单纯的工具使用转向培养“人类主体性”，提倡建立涵盖素养、流利度和胜任力的框架，赋能师生在AI时代进行批判性抉择。


<details>
  <summary>Details</summary>
Motivation: 当前的AI教育存在严重的“素养鸿沟”：过度关注工具的功能性操作，而忽视了批判性逻辑和伦理推理。这种失衡导致人类在技术面前逐渐丧失决策的主动权（主体性）。

Method: 通过论证（Position Paper）提出了一种系统性的转型方案。文中介绍了“AI素养（Literacy）、流利度（Fluency）和胜任力（Competency）”框架，强调认识论、批判性思维和人类主体性的融合。

Result: 提出了一个以人为中心的AI教育路径，界定了学生质疑和选择的权利，以及教师维持教学控制权的重要性。该框架为评估AI决策对学术、职业和社会的影响提供了清晰的路径。

Conclusion: AI素养不应仅停留在技术操作层面，而应转向以“人类主体性”为核心的教育范式。通过建立明确的素养与能力框架，使教育者和学生能够批判性地掌控AI，而非被动适应技术。

Abstract: The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.

</details>


### [133] [Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm](https://arxiv.org/abs/2512.16694)
*Wisnu Uriawan,Achmad Ajie Priyajie,Angga Gustian,Fikri Nur Hidayat,Sendi Ahmad Rafiudin,Muhamad Fikri Zaelani*

Main category: cs.AI

TL;DR: 本研究利用Apriori算法对《布哈里圣训》印尼语译文进行关联规则挖掘，成功实现了圣训主题的自动化语义关联分析。


<details>
  <summary>Details</summary>
Motivation: 随着伊斯兰文本数字化的增长，迫切需要一种自动化方法对未标注的圣训数据进行主题分组和关联模式识别。

Method: 对布哈里圣训印尼语译文进行预处理（清洗、分词、去停用词、词干提取），随后应用Apriori算法进行关联规则挖掘，并基于支持度、置信度及提升度进行参数分析。

Result: 成功识别出具有实际意义的关联模式，如“拉卡特-礼拜”、“经文-启示”以及“圣训-故事”，准确描述了崇拜、启示和圣训叙事等主题。

Conclusion: Apriori算法能够有效揭示圣训中潜在的语义特征，为数字伊斯兰研究和技术辅助学习系统提供了自动化的主题分类方案。

Abstract: This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.

</details>


### [134] [Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning](https://arxiv.org/abs/2512.16698)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Mohammad Nehad Alam,Proma Hossain Progga,Swakkhar Shatabda*

Main category: cs.AI

TL;DR: 本研究探讨了多智能体设计对几何解题的影响，发现其对开源模型提效显著，而对顶级闭源模型增益有限。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在解决几何问题上取得进展，但尚不清楚多智能体设计相较于单智能体是否具有明确优势及其实用性边界。

Method: 系统性地比较了单智能体与多智能体流水线在四个视觉数学基准测试（Geometry3K, MathVerse, OlympiadBench, We-Math）上的表现，涵盖了开源模型（Qwen-2.5-VL）和闭源模型（Gemini-2.0-Flash）。

Result: 开源模型（如 Qwen-2.5-VL）在多智能体模式下性能显著提升；而闭源模型（Gemini-2.0-Flash）在传统基准上单智能体表现更佳，仅在较新且更具挑战性的 We-Math 数据集上能通过多智能体获得微弱收益。

Conclusion: 多智能体协作能显著提升开源模型的性能，但在顶尖闭源模型上表现并不一致。智能体化分解并非万能药，其效果取决于模型的固有能力及任务的熟悉程度。

Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver

</details>


### [135] [Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems](https://arxiv.org/abs/2512.16707)
*Abhisek Ganguly*

Main category: cs.AI

TL;DR: 由于逻辑不完备性和动力学不可预测性，算法智能体无法一般性地计算出自身的预测能力极限。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨限制算法智能的计算边界，特别是研究智能体在有限精度和逻辑一致性条件下，对其自身预测能力的认知局限。

Method: 通过形式化“形式不完备性”（Formal Incompleteness）和“动态不可预测性”（Dynamical Unpredictability）这两个独立的计算限制，构建了一个分析算法智能约束的理论框架。

Result: 证明了算法智能体普遍无法计算出其自身的“最大预测范围”（Maximal Prediction Horizon），即无法预知自己在何时会失去预测能力。

Conclusion: 智能系统的推理、预测与自我分析能力之间存在固有的权衡（trade-offs），智能无法超越其自身的计算极限来实现完全的自我认知。

Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.

</details>


### [136] [Discovering and Learning Probabilistic Models of Black-Box AI Capabilities](https://arxiv.org/abs/2512.16733)
*Daniel Bramblett,Rushang Karia,Adrian Ciotinga,Ruthvick Suresh,Pulkit Verma,YooJung Choi,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 本文介绍了一种利用PDDL符号表示法来逆向建模黑盒AI规划能力的方法。通过MCTS搜索测试任务，该方法能推导出AI在不同条件下执行动作的概率结果，从而增强了AI决策系统的安全性和透明度。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒AI（如大模型）在序贯决策领域的广泛应用，如何为其提供一个可靠且具备可解释性的模型表达，以确保其运行和部署的安全性，已成为一个亟待解决的挑战。

Method: 提出一种基于蒙特卡洛树搜索（MCTS）的方法。该方法通过系统性地创建测试任务、收集实验数据，并不断剪枝可能的符号模型假设空间，最终学习出PDDL风格的随机模型。

Result: 理论证明了所学模型的完备性、正确性和收敛性；实验结果表明，该方法在多种黑盒AI系统中表现出良好的适用范围、学习效率和建模准确度。

Conclusion: 该方法能够有效且准确地学习并描述黑盒AI的规划能力，为复杂的决策系统提供了一种兼具可靠性与可解释性的分析手段。

Abstract: Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.

</details>


### [137] [AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach](https://arxiv.org/abs/2512.16739)
*Yipeng Zhuang,Yifeng Guo,Yuewen Li,Yuheng Wu,Philip Leung-Ho Yu,Tingting Song,Zhiyong Wang,Kunzhong Zhou,Weifang Wang,Li Zhuang*

Main category: cs.AI

TL;DR: 该研究通过结合机器学习与大语言模型，利用多模态数据精准预测肺癌患者未来48-72小时内的疼痛发作，显著提升了预测的敏感性和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 针对肺癌住院患者突发性疼痛高发（达91%）的问题，旨在通过结合电子健康档案（EHR）中的结构化与非结构化数据，实现疼痛发作的提前精准预测。

Method: 提出一种结合传统机器学习（ML）与大语言模型（LLM）的混合流水线。ML负责处理机构化数据（如体征、给药趋势），LLM负责解读模糊的用药记录和非结构化临床文本。

Result: 模型在48小时和72小时疼痛预测中的准确率分别达到0.874和0.917；引入LLM后，敏感性分别提升了8.6%和10.4%。

Conclusion: 该混合模型在捕捉疼痛趋势和解读复杂临床文本方面表现出色，为肿瘤科提供了一个可扩展且具解释性的疼痛预测工具，有助于优化医疗资源分配。

Abstract: Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.

</details>


### [138] [CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?](https://arxiv.org/abs/2512.16755)
*Siqi Wang,Chao Liang,Yunfan Gao,Erxin Yu,Sen Li,Yushi Li,Jing Li,Haofen Wang*

Main category: cs.AI

TL;DR: 本文提出 CitySeeker 基准，评估 VLM 在城市中根据隐式需求进行导航的能力。实验发现现有模型表现不佳，并基于人类认知映射提出了通过回溯、增强空间感和记忆检索来优化复杂导航的见解。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLM）在显式指令导航上表现良好，但在动态城市环境中理解人类隐式需求（如“我渴了”）并进行自主空间导航的能力尚未得到充分研究。

Method: 1. 构建 CitySeeker 基准测试：包含 8 个城市、7 种场景、6,440 条轨迹，模拟基于隐式需求的导航；2. 针对性评估：测试主流 VLM 在长程推理、空间认知和经验召回方面的表现；3. 探索性策略研究：引入回溯机制（Backtracking）、空间认知增强和基于记忆的检索（BCR）进行对比实验。

Result: 即使是最顶尖的模型（如 Qwen2.5-VL-32B）在 CitySeeker 任务中的完成率也仅为 21.1%。模型的主要瓶颈在于长程推理中的误差累积、空间认知不足以及缺乏经验召回能力。

Conclusion: CitySeeker 揭示了当前 VLM 在处理复杂城市任务中的局限性，并强调了模仿人类认知映射（迭代观察、增强空间知觉和记忆检索）是实现具身空间智能、解决“最后一公里”导航挑战的关键路径。

Abstract: Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., "I am thirsty") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling "last-mile" navigation challenges.

</details>
