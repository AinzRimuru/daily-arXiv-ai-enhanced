<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 25]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 50]
- [cs.IT](#cs.IT) [Total: 3]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition](https://arxiv.org/abs/2512.13884)
*Jonas Golde,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: FiNERweb 是一个多语言命名实体识别（NER）数据集创建管道，它通过教师-学生范式，利用大型语言模型（LLM）对91种语言和25种文字进行标注，生成的22.5万个通道包含23.5万个不同的实体标签。该方法在零样本迁移设置中表现出与强基线模型相当或更优的性能，同时标注质量高，并发布了包含英语和目标语言标签的数据集。


<details>
  <summary>Details</summary>
Motivation: 以往多语言命名实体识别（NER）研究中，大型语言模型（LLM）的合成监督数据多作为实验副产品，缺乏系统性和可复用性。

Method: FiNERweb 数据集创建管道基于 FineWeb-Edu，训练回归模型识别与 NER 相关的文本段落，并使用多语言 LLM 进行标注，涵盖91种语言和25种文字。

Result: 回归模型在识别相关段落方面 achieves 84 F1以上的表现。在 FiNERweb 上训练的模型，在零样本迁移设置中，于英语、泰语和斯瓦希里语上表现与强基线模型相当或更优，尽管数据量仅为基线模型的1/19。LLM 作为评判者评估标注质量，忠实度得分3.99/5，完整性得分4.05/5。使用目标语言标签评估时，现有SOTA模型性能下降0.02至0.09 F1。

Conclusion: FiNERweb 及其相关资源已发布，旨在促进多语言命名实体识别领域更有效的教师-学生训练，为研究社区提供系统、可复用的多语言NER数据集。

Abstract: Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.

</details>


### [2] [Olmo 3](https://arxiv.org/abs/2512.13961)
*Team Olmo,:,Allyson Ettinger,Amanda Bertsch,Bailey Kuehl,David Graham,David Heineman,Dirk Groeneveld,Faeze Brahman,Finbarr Timbers,Hamish Ivison,Jacob Morrison,Jake Poznanski,Kyle Lo,Luca Soldaini,Matt Jordan,Mayee Chen,Michael Noukhovitch,Nathan Lambert,Pete Walsh,Pradeep Dasigi,Robert Berry,Saumya Malik,Saurabh Shah,Scott Geng,Shane Arora,Shashank Gupta,Taira Anderson,Teng Xiao,Tyler Murray,Tyler Romero,Victoria Graf,Akari Asai,Akshita Bhagia,Alexander Wettig,Alisa Liu,Aman Rangapur,Chloe Anastasiades,Costa Huang,Dustin Schwenk,Harsh Trivedi,Ian Magnusson,Jaron Lochner,Jiacheng Liu,Lester James V. Miranda,Maarten Sap,Malia Morgan,Michael Schmitz,Michal Guerquin,Michael Wilson,Regan Huff,Ronan Le Bras,Rui Xin,Rulin Shao,Sam Skjonsberg,Shannon Zejiang Shen,Shuyue Stella Li,Tucker Wilde,Valentina Pyatkin,Will Merrill,Yapei Chang,Yuling Gu,Zhiyuan Zeng,Ashish Sabharwal,Luke Zettlemoyer,Pang Wei Koh,Ali Farhadi,Noah A. Smith,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: Olmo 3是7B和32B参数规模的领先、完全开源的大型语言模型系列，其构建旨在实现长上下文推理、函数调用、编码、指令遵循、通用聊天和知识召回。


<details>
  <summary>Details</summary>
Motivation: 构建达到最先进水平的完全开源语言模型，并公开模型的整个构建流程。

Method: 开发了Olmo 3模型系列，包括7B和32B参数规模的模型。这个系列模型针对长上下文推理、函数调用、编码、指令遵循、通用聊天和知识召回等功能进行优化。

Result: 发布了Olmo 3模型系列，其中Olmo 3 Think 32B是迄今为止最强大的完全开源思考模型。本次发布还包含了模型的整个生命周期，包括每个阶段、检查点、数据点和构建模型所使用的依赖项。

Conclusion: Olmo 3系列模型在多种功能上表现出色，特别是旗舰模型Olmo 3 Think 32B，它作为完全开源模型在同类产品中达到领先水平，并且其完整开发流程的公开为未来的研究提供了宝贵的资源。

Abstract: We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.

</details>


### [3] [Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models](https://arxiv.org/abs/2512.13980)
*Zhimin Qiu,Di Wu,Feng Liu,Chenrui Hu,Yuxiao Wang*

Main category: cs.CL

TL;DR: 本文提出了一种结构感知解码方法，解决了传统方法在嵌套和重叠实体提取中难以同时保持语义完整性和结构一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在嵌套和重叠实体提取中难以同时保持语义完整性和结构一致性的问题。

Method: 该方法引入了候选跨度生成机制和结构化注意力建模，实现了实体边界、层次关系和交叉依赖的统一建模。模型首先使用预训练语言模型获取上下文感知语义表示，然后通过候选表示组合捕获多粒度实体跨度特征，并在解码过程中引入层次结构约束，以确保语义和结构之间的一致性。为了增强复杂场景下的稳定性，模型联合优化了分类损失和结构一致性损失。

Result: 在ACE 2005数据集上进行的实验表明，该方法在准确率、精确率、召回率和F1-Score方面均有显著提高，尤其是在嵌套和重叠实体识别方面，模型显示出更强的边界定位和结构建模能力。

Conclusion: 本研究验证了结构感知解码在复杂语义提取任务中的有效性，为开发具有层次理解能力的语言模型提供了新视角，并为高精度信息提取奠定了方法论基础。

Abstract: This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.

</details>


### [4] [What Affects the Effective Depth of Large Language Models?](https://arxiv.org/abs/2512.14064)
*Yi Hu,Cai Zhou,Muhan Zhang*

Main category: cs.CL

TL;DR: 该研究系统性探讨了大型语言模型（LLMs）的“有效深度”，发现模型规模、训练类型和任务难度对其利用可用深度没有显著影响，并指出提高层利用率เป็น未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在增加深度时性能增益递减，此前研究提出“有效深度”概念，认为更深的模型并未充分利用其层进行有效计算。因此，本研究旨在系统性地探究有效深度如何随模型规模、训练类型和任务难度而变化。

Method: 本研究分析了Qwen-2.5系列模型（1.5B-32B）的行为，并比较了基础模型和长CoT模型之间的有效深度。此外，还在不同难度任务下评估了模型的表现。

Result: 研究发现，虽然有效层数随模型规模的增加而增长，但有效深度比率保持稳定。基础模型和长CoT模型之间的有效深度没有增加，表明推理能力的提升源于更长的上下文而非更深的每token计算。不同难度的任务评估显示，模型并未为解决更难的问题动态地使用更多层。

Conclusion: 目前LLMs在不同规模、训练范式和任务难度下都未能充分利用其可用深度。这为提高LLMs的层利用率、模型剪枝和提前退出等研究方向提供了机会。

Abstract: The scaling of large language models (LLMs) emphasizes increasing depth, yet performance gains diminish with added layers. Prior work introduces the concept of "effective depth", arguing that deeper models fail to fully utilize their layers for meaningful computation. Building on this, we systematically study how effective depth varies with model scale, training type, and task difficulty. First, we analyze the model behavior of Qwen-2.5 family (1.5B-32B) and find that while the number of effective layers grows with model size, the effective depth ratio remains stable. Besides, comparisons between base and corresponding long-CoT models show no increase in effective depth, suggesting that improved reasoning stems from longer context rather than deeper per-token computation. Furthermore, evaluations across tasks of varying difficulty indicate that models do not dynamically use more layers for harder problems. Our results suggest that current LLMs underuse available depth across scales, training paradigms and tasks of varying difficulties, pointing out research opportunities on increasing the layer utilization rate of LLMs, model pruning, and early exiting. Our code is released at https://github.com/AheadOFpotato/what_affects_effective_depth.

</details>


### [5] [Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed](https://arxiv.org/abs/2512.14067)
*Yonggan Fu,Lexington Whalen,Zhifan Ye,Xin Dong,Shizhe Diao,Jingyu Liu,Chengyue Wu,Hao Zhang,Enze Xie,Song Han,Maksim Khadkevich,Jan Kautz,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种将预训练自回归（AR）语言模型转换为扩散语言模型（dLM）的方法，从而在保持AR模型任务准确性的同时提高生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型在从头开始训练时，学习效率低于自回归语言模型，本研究旨在解决这个问题，通过将预训练的AR模型转换为高效的dLMs，以提高生成速度并保持任务准确性。

Method: 本研究通过改进注意力模式和训练目标，提出了更有效的AR到dLM转换方法。具体来说，研究者比较了不同的注意力模式，发现保持预训练AR模型的权重分布对于有效的转换至关重要。为此，他们引入了一种块式注意力模式的连续预训练方案，该方案在块间保持因果关系，同时在每个块内实现双向建模。其次，为了弥补掩码令牌分布的训练-测试差距，他们提出了一种位置相关的令牌掩码策略，在训练期间为后面的令牌分配更高的掩码概率，以更好地模拟测试时的行为。

Result: 通过这些改进，本研究开发了Efficient-DLM系列模型。Efficient-DLM 8B模型在准确性上比Dream 7B高出5.4%，比Qwen3 4B高出2.7%，同时吞吐量分别提高了4.5倍和2.7倍。这些结果表明，Efficient-DLM系列在性能上优于最先进的AR模型和dLM。

Conclusion: 本研究提出了一种高效的AR到dLM转换框架，通过改进注意力模式和训练策略，成功地将预训练AR模型转换为高性能的扩散语言模型。该框架为可扩展的AR到dLM转换提供了可行的见解，并显著提升了语言模型的生成效率和准确性。

Abstract: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.

</details>


### [6] [A Unified Sparse Attention via Multi-Granularity Compression](https://arxiv.org/abs/2512.14082)
*Siran Liu,Zane Cao,Yongchao He*

Main category: cs.CL

TL;DR: 本文提出了一种名为UniSparse的新机制，通过引入组合 tokens 和动态构建稀疏注意力，解决了大型语言模型在处理长上下文时的效率问题，实现了在保持高准确性的前提下显著加速注意力计算。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在多轮对话和程序分析等应用中对长上下文的理解和推理能力要求越来越高。然而，核心的自注意力机制计算复杂度与序列长度的平方成正比，导致了计算瓶颈。现有的稀疏注意力方法虽然缓解了这个问题，但仍面临权衡：基于训练的方法成本高昂且无法直接作为其他模型的加速插件，而推理时的方法通常会牺牲效率或跨模态通用性。

Method: UniSparse 引入了“组合 tokens”的概念，这是一种聚合多粒度上下文信息的紧凑表示。在此基础上，UniSparse 通过多粒度压缩和块级选择动态构建稀疏注意力，从而实现高效且对 GPU 友好的执行。

Result: UniSparse 在多种模态和任务（从合成基准测试到实际应用）中，在准确性和效率方面均超越了最先进的稀疏注意力方法（例如 MInference、XAttention、FlexPrefill）。它能达到全注意力准确度的 99% 以上，并将注意力计算速度比 FlashAttention 加快多达 2.61 倍。

Conclusion: UniSparse 通过引入组合 tokens 和动态稀疏注意力机制，有效地解决了长上下文理解中的计算瓶颈，并在准确性和效率上取得了显著的提升，为大型语言模型在实际应用中的长上下文处理提供了强大的解决方案。

Abstract: Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\ge$ 99% of full-attention accuracy and up to 2.61$\times$ faster attention computation than FlashAttention.

</details>


### [7] [Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study](https://arxiv.org/abs/2512.14085)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Taiga Mori,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 该研究提出了一个多语言、连续的、基于Transformer的日本人、英语和汉语的后通道预测模型，并用它来调查跨语言的时序行为。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探究跨语言的后通道时序行为，并为设计更自然、更具文化意识的口语对话系统提供依据。

Method: 该模型是一个基于Transformer的、在帧级别操作的模型，通过辅助任务在约300小时的对话数据上进行了联合训练。研究人员还进行了零样本迁移学习和扰动分析，并对上下文长度进行了研究。最后，将训练好的模型集成到实时处理软件中。

Result: 多语言模型在所有三种语言上都与单语言基线模型匹配或超越，表明它学习了语言通用线索和语言特定的时序模式。零样本迁移效果有限，突显了实质性的跨语言差异。扰动分析显示了不同的线索使用：日语更依赖短期语言信息，而英语和汉语对静音持续时间和韵律变化更敏感；多语言训练鼓励共享但可适应的表示，并减少了对汉语中音高的过度依赖。上下文长度研究表明，日语对较短的上下文相对稳健，而汉语受益于较长的上下文。

Conclusion: 该研究提供了一个统一的模型和经验证据，说明后通道时序在不同语言中的差异，为设计更自然、更具文化意识的口语对话系统提供了信息。

Abstract: We present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.

</details>


### [8] [CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models](https://arxiv.org/abs/2512.14118)
*Yiran Zhang,Jincheng Hu,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: CogMem 是一种受认知启发、记忆增强的 LLM 架构，它通过结构化、持久性记忆支持持续的迭代推理。该架构包含三层：整合跨会话推理策略的长期记忆（LTM）、维护会话级笔记并检索相关长期记忆的直接访问（DA）记忆，以及在每个回合动态重建简洁、任务相关上下文的注意力焦点（FoA）机制。在 TurnBench 上的实验表明，CogMem 能够减轻推理失败，控制上下文增长，并提高扩展推理链的一致性。


<details>
  <summary>Details</summary>
Motivation: LLMs 在单轮推理方面表现出色，但在长时间、多轮交互中往往会失去准确性和连贯性。TurnBench 等最新评估突出显示了重复出现的失败模式，例如推理偏差、任务漂移、幻觉、过度自信和记忆衰退。

Method: 本文引入了一种名为 CogMem 的认知启发式记忆增强型 LLM 架构，该架构通过结构化的持久性记忆来支持持续的迭代推理。CogMem 包括三层：长期记忆（LTM）用于整合跨会话推理策略；直接访问（DA）记忆用于维护会话级笔记并检索相关的长期记忆；以及注意力焦点（FoA）机制，用于在每个回合动态重建简洁且与任务相关的上下文。

Result: 在 TurnBench 上的实验表明，这种分层设计减轻了推理失败，控制了上下文增长，并提高了扩展推理链的一致性。

Conclusion: CogMem 架构通过其分层记忆设计，有效地缓解了大型语言模型在多轮交互中出现的推理失败，提高了模型的一致性和可靠性，使其推理过程更接近于人类。

Abstract: Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.

</details>


### [9] [A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs](https://arxiv.org/abs/2512.14179)
*K. M. Jubair Sami,Dipto Sumit,Ariyan Hossain,Farig Sadeque*

Main category: cs.CL

TL;DR: 这篇论文提出了两种大语言模型（LLM）的检索增强生成（RAG）管道，用于孟加拉语标准语到方言的翻译，其中基于标准化句对的管道表现优于基于转录的管道，并使小型模型也能达到卓越性能。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺和语言变异，特别是在孟加拉语中，标准语到地域方言的翻译是一个重要的自然语言处理（NLP）挑战。

Method: 本研究提出了两种新颖的RAG管道：1. 基于转录的管道，利用音频转录中的大量方言句子上下文。 2. 标准化句对管道，使用结构化的本地_方言:标准_孟加拉语句对。研究通过BLEU、ChrF、WER和BERTScore评估了这两种管道在六种孟加拉方言和多个大型语言模型上的表现。

Result: 标准化句对管道始终优于基于转录的管道，例如，在吉大港方言中，词错误率（WER）从76%降低到55%。更重要的是，这种RAG方法使得小型模型（如Llama-3.1-8B）能够超越大型模型（如GPT-OSS-120B）。

Conclusion: 精心设计的检索策略比模型大小更关键，该研究为低资源方言翻译提供了一个有效的、无需微调的解决方案，为保护语言多样性提供了一个实用的蓝图。

Abstract: Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\_dialect:standard\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\% to 55\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.

</details>


### [10] [Two CFG Nahuatl for automatic corpora expansion](https://arxiv.org/abs/2512.14239)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Graham Ranger Martha-Lorena Avendaño-Garrido*

Main category: cs.CL

TL;DR: 本文介绍了两种用于Nawatl语料库扩展的上下文无关文法（CFG），以解决该语言数字资源稀缺的问题。通过这两种CFG，可以生成大量句法正确的Nawatl人工句子，从而显著扩展语料库，并用于非上下文嵌入的学习和评估。


<details>
  <summary>Details</summary>
Motivation: Nawatl语是一种数字资源稀缺的语言，导致其在大型语言模型（LLMs）学习中缺乏可用的语料库。

Method: 引入两种新的Nawatl上下文无关文法（CFG），并以生成模式使用它们来生成大量的句法有效的Nawatl人工句子，从而扩展语料库。

Result: 与仅使用原始语料库而不进行人工扩展的结果相比，本文方法取得了改进。此外，结果表明，经济型嵌入通常比某些大型语言模型（LLMs）表现更好。

Conclusion: 通过引入两种新的Nawatl CFG并生成人工句子来扩展语料库，可以显著改善Nawatl语言在LLMs学习中的语料资源不足问题，并且所获得的嵌入在语义相似性任务中表现良好。

Abstract: The aim of this article is to introduce two Context-Free Grammars (CFG) for Nawatl Corpora expansion. Nawatl is an Amerindian language (it is a National Language of Mexico) of the $π$-language type, i.e. a language with few digital resources. For this reason the corpora available for the learning of Large Language Models (LLMs) are virtually non-existent, posing a significant challenge. The goal is to produce a substantial number of syntactically valid artificial Nawatl sentences and thereby to expand the corpora for the purpose of learning non contextual embeddings. For this objective, we introduce two new Nawatl CFGs and use them in generative mode. Using these grammars, it is possible to expand Nawatl corpus significantly and subsequently to use it to learn embeddings and to evaluate their relevance in a sentences semantic similarity task. The results show an improvement compared to the results obtained using only the original corpus without artificial expansion, and also demonstrate that economic embeddings often perform better than some LLMs.

</details>


### [11] [From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition](https://arxiv.org/abs/2512.14244)
*Yiqing Zhou,Yu Lei,Shuzheng Si,Qingyan Sun,Wei Wang,Yifei Wu,Hao Wen,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: 该论文介绍了一种名为EDU-based Context Compressor的新型显式压缩框架，用于解决大型语言模型在处理长文本时遇到的瓶颈问题。它通过将文本转化为结构关系树，并选择与查询相关的子树进行线性化，以保留全局结构和细粒度信息。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长文本时面临上下文管理困难，现有压缩技术存在破坏局部连贯性或受位置偏差影响的问题。

Method: 该方法包括两个步骤：首先，LingoEDU将线性文本转化为基本话语单元（EDU）的结构关系树；其次，一个轻量级排序模块选择与查询相关的子树进行线性化。

Result: 该方法在结构预测准确性方面达到了最先进水平，显著优于现有的LLMs，并降低了成本。它还显著提升了长上下文任务和复杂深度搜索等下游任务的性能。

Conclusion: EDU-based Context Compressor框架通过结构化压缩，有效解决了大型语言模型处理长文本时的挑战，并在多个方面取得了显著改进。

Abstract: Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.

</details>


### [12] [Inflation Attitudes of Large Language Models](https://arxiv.org/abs/2512.14306)
*Nikoleta Anesti,Edward Hill,Andreas Joseph*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLMs）形成通货膨胀感知和预期的能力，并将其与家庭调查数据和官方统计数据进行比较，发现LLMs能有效跟踪总体调查预测和官方统计数据，但在消费者价格通胀方面缺乏一致模型。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）在基于宏观经济价格信号形成通货膨胀感知和预期方面的能力，并将其与人类行为进行比较。

Method: 本文采用准实验设计，利用GPT-3.5-turbo（GPT）的数据截止日期（2021年9月）来模拟信息集和人口统计学特征，并与英格兰银行的通货膨胀态度调查（IAS）进行比较。此外，还使用Shapley值分解来分析模型输出的驱动因素。

Result: GPT模型能够跟踪短期的总体调查预测和官方统计数据。在分类层面，GPT复制了家庭通货膨胀感知的关键经验规律，尤其是在收入、住房和的社会阶层方面。GPT对食品通货膨胀信息表现出与人类受访者相似的高度敏感性。但GPT缺乏一个关于消费者价格通货膨胀的持续模型。

Conclusion: LLMs在模拟通货膨胀感知和预期方面表现出 promising 的能力，但在某些方面仍存在局限性。本文提出的方法可用于评估LLMs在社会科学中的应用，比较不同模型，或辅助调查设计。

Abstract: This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.

</details>


### [13] [Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.14427)
*Gabriele Prato,Shagun Sodhani,Alessandro Sordoni,Sarath Chandar*

Main category: cs.CL

TL;DR: 这篇论文研究了文档打包对大型语言模型多跳推理能力的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中的文档打包是标准做法，但其对模型能力的影响尚未被充分探索。该研究旨在填补这一空白，探究不同文档打包策略如何影响LLM潜在的多跳推理能力。

Method: 通过研究不同文档打包策略对LLMs的影响，并通过消融实验来理解其底层机制。

Result: 文档打包相比于单独文档训练能提升模型性能，但需要更多的计算资源。消融研究揭示了解释打包优势的关键因素。

Conclusion: 本研究加深了对LLM训练动态的理解，并为优化模型开发提供了实用见解。

Abstract: The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.

</details>


### [14] [SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models](https://arxiv.org/abs/2512.14481)
*Shizhuo Mao,Song Chen,Yi Kang*

Main category: cs.CL

TL;DR: SASQ是一个针对LLM的量化感知训练框架，它通过优化激活量化因子，在不改变预训练权重的情况下，实现了高精度、高部署效率的静态推理，并在多个LLM模型上超越了现有SOTA量化方案和FP16模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理任务中表现出色，但其不断增长的模型规模超出了GPU内存的增长速度。此外，现有的量化方法，如动态量化和静态量化，存在计算开销高、部署困难或牺牲准确性等问题，量化感知训练（QAT）方法也存在权重训练成本过高的问题。

Method: 本文提出了SASQ框架，这是一个轻量级的QAT框架，专门针对激活量化因子进行优化。SASQ只优化量化因子，不改变预训练权重，从而实现高精度静态推理和部署效率。SASQ还自适应地截断一些异常值，以降低量化难度并保留激活的分布特征。

Result: SASQ的量化效果不仅超越了现有的SOTA量化方案，甚至优于相应的FP16模型。在LLaMA2-7B模型上，SASQ在WikiText2数据集上的困惑度比QuaRot低5.2%，比FP16模型低4.7%。

Conclusion: SASQ是一个高效且轻量级的LLM量化方案，它解决了现有量化方法的痛点，通过优化激活量化因子并在不改变预训练权重的情况下，实现了高精度、高部署效率的静态推理。SASQ的卓越性能使其成为LLM部署的有力工具。

Abstract: Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.

</details>


### [15] [C-ing Clearly: Enhanced Binary Code Explanations using C code](https://arxiv.org/abs/2512.14500)
*Teodor Poncu,Ioana Pintilie,Marius Dragoi,Dragos Tantaru,Florin Brad*

Main category: cs.CL

TL;DR: C-ing Clearly 是一种合成数据生成方法，它利用相应的 C 代码来增强 LLM 对汇编的理解，从而提高LLM在二进制代码总结和漏洞检测方面的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs 通常擅长处理高级编程语言的编码任务，但在处理低级编程语言（例如汇编）时表现不佳

Method: 我们提出了一种名为 C-ing Clearly 的合成数据生成方法，该方法利用相应的 C 代码来增强 LLM 对汇编的理解。

Result: 通过对我们方法生成的数据进行微调，我们展示了 LLM 在二进制代码总结和漏洞检测方面的性能提升。我们的方法在不同 LLM 系列和模型尺寸上都表现出 SOTA 的增益。

Conclusion: C-ing Clearly 方法能够有效提升LLM对汇编代码的理解能力，从而在二进制代码总结和漏洞检测等任务中取得更好的表现。

Abstract: Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.

</details>


### [16] [Linguists should learn to love speech-based deep learning models](https://arxiv.org/abs/2512.14506)
*Marianne de Heer Kloots,Paul Boersma,Willem Zuidema*

Main category: cs.CL

TL;DR: 这篇文章探讨了LLMs和语言学之间的关系，强调了音频在其中应扮演的关键角色。


<details>
  <summary>Details</summary>
Motivation: 作者认为Futrell和Mahowald的框架很有用，但他们关注生成式文本LLM的局限性限制了与语言学的有效互动，因为人类语言的许多有趣问题无法通过书面文本捕捉。

Method: 本文提出音频模型可以作为弥补文本LLM局限性的一种方法。

Result: 通过引入音频模型，可以更全面地研究人类语言，解决书面文本无法捕捉的问题。

Conclusion: 为了更好地连接深度学习系统和语言学理论，音频模型应该发挥关键作用。

Abstract: Futrell and Mahowald present a useful framework bridging technology-oriented deep learning systems and explanation-oriented linguistic theories. Unfortunately, the target article's focus on generative text-based LLMs fundamentally limits fruitful interactions with linguistics, as many interesting questions on human language fall outside what is captured by written text. We argue that audio-based deep learning models can and should play a crucial role.

</details>


### [17] [Dual Language Models: Balancing Training Efficiency and Overfitting Resilience](https://arxiv.org/abs/2512.14549)
*David Samuel,Lucas Georges Gabriel Charpentier*

Main category: cs.CL

TL;DR: 这篇论文提出了一种结合自回归和掩蔽扩散训练目标的方法，创建了性能优于单一目标模型的语言模型。


<details>
  <summary>Details</summary>
Motivation: 自回归模型虽然训练效率高，但容易过拟合。掩蔽扩散模型对过拟合更有弹性，但训练效率较低。本文旨在结合两者的优点，以提升语言模型的性能。

Method: 本文通过结合自回归和掩蔽扩散训练目标，在不改变模型架构的情况下进行训练。为了找到最佳的目标组合比例，研究人员训练并评估了50个语言模型，这些模型在不同程度的数据重复下进行了测试。

Result: 研究结果表明，在所有评估设置下，结合自回归和掩蔽扩散两种目标是最佳选择。此外，无论是针对自回归还是掩蔽扩散的下游性能，最佳的组合比例都相似。

Conclusion: 通过结合自回归和掩蔽扩散训练目标，可以创建出既高效又对过拟合具有更好鲁棒性的语言模型，从而在各种任务中取得更好的性能。

Abstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.

</details>


### [18] [VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models](https://arxiv.org/abs/2512.14554)
*Nguyen Tien Dong,Minh-Anh Nguyen,Thanh Dat Hoang,Nguyen Tuan Ngoc,Dao Xuan Quang Minh,Phan Phi Hai,Nguyen Thi Ngoc Anh,Dang Van Tu,Binh Vu*

Main category: cs.CL

TL;DR: 越南法律评估基准 (VLegal-Bench) 旨在系统评估大型语言模型在处理越南法律任务时的表现，涵盖多层次法律理解，并包含10,450个经过法律专家标注和交叉验证的样本，以促进更可靠、可解释和符合道德的AI法律辅助系统的发展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在法律领域的应用潜力巨大，但越南立法的复杂性、层次性和频繁修订给评估LLMs解释和利用法律知识的能力带来了挑战。

Method: VLegal-Bench通过严格的标注流程生成了10,450个样本，法律专家使用标注系统对每个实例进行标注和相互验证，确保每个样本都基于权威法律文件，并模拟现实世界的法律助理工作流程，包括一般法律问答、检索增强生成、多步推理和基于场景的问题解决，这些都针对越南法律进行了调整。

Result: VLegal-Bench提供了一个标准化、透明且认知知情的评估框架。

Conclusion: VLegal-Bench为评估LLMs在越南法律背景下的表现奠定了坚实基础，并支持开发更可靠、可解释和符合道德的AI辅助法律系统。

Abstract: The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.

</details>


### [19] [Polypersona: Persona-Grounded LLM for Synthetic Survey Responses](https://arxiv.org/abs/2512.14562)
*Tejaswani Dash,Dinesh Karri,Anudeep Vurity,Gautam Datla,Tazeem Ahmad,Saima Rafi,Rohith Tangudu*

Main category: cs.CL

TL;DR: 本文介绍了 PolyPersona，一个用于生成多领域条件式调查问卷回复的生成框架。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的环境下，有效地生成高质量、条件式的合成调查数据以支持可扩展的评估和偏差分析。

Method: PolyPersona 框架通过使用参数高效的 LoRA 适配器和 4 比特量化，对小型聊天模型进行指令调优。它采用基于对话的数据管道，明确保留了角色线索，确保生成回复的行为一致性。以此管道构建了一个包含 3,568 条合成调查回复的数据集，覆盖十个领域和 433 个不同的角色。

Result: 实验结果表明，像 TinyLlama 1.1B 和 Phi-2 这样的小型模型，其性能与更大的 7B 到 8B 基线模型相当，最高 BLEU 得分为 0.090，ROUGE-1 得分为 0.429。这表明，通过角色条件式微调，小型语言模型能够生成可靠且连贯的合成调查数据。

Conclusion: PolyPersona 框架为调查数据生成提供了一种高效且可复现的方法，支持可扩展的评估，并通过透明开放的协议促进偏差分析。

Abstract: This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.

</details>


### [20] [JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction](https://arxiv.org/abs/2512.14620)
*Atsuyuki Miyai,Shota Onohara,Jeonghun Baek,Kiyoharu Aizawa*

Main category: cs.CL

TL;DR: JMMMU-Pro是JMMMU的扩展，它将问题图像和问题文本组合成一个单一图像，以此来创建需要通过视觉感知进行视觉-文本综合理解的基准。


<details>
  <summary>Details</summary>
Motivation: 目前，现有的大型多模态模型（LMMs）在处理需要综合视觉-文本理解的日语图像基准方面存在困难，尤其是在多学科和多模态理解方面。

Method: 本文提出了Vibe基准构建方法，利用图像生成模型（如Nano Banana Pro）生成候选视觉问题，然后人工验证输出，并在必要时使用调整后的提示重新生成以确保质量。

Result: 实验结果表明，所有开源的LMMs在JMMMU-Pro上都表现不佳，这突显了JMMMU-Pro作为一个重要基准的价值。

Conclusion: JMMMU-Pro为评估LMMs的日语能力提供了更严格的评估工具，并且Vibe基准构建方法也为未来基于图像的VQA基准开发提供了高效的指导。

Abstract: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.

</details>


### [21] [TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines](https://arxiv.org/abs/2512.14645)
*David Schulmeister,Valentin Hartmann,Lars Klein,Robert West*

Main category: cs.CL

TL;DR: TiME: 针对效率关键型应用的紧凑型多语言编码器


<details>
  <summary>Details</summary>
Motivation: 目前大型通用语言模型在处理大量数据或提供实时响应时速度不够快，且能耗高，部署在电池供电设备上存在问题。因此，需要针对效率关键型应用训练小型模型。

Method: 我们展示了如何训练小型模型以满足效率关键型应用的需求，并采用蒸馏等现代训练技术，以及支持低资源语言。我们将模型命名为TiME（Tiny Monolingual Encoders）。

Result: TiME模型在性能、吞吐量、延迟和能耗之间取得了更好的平衡。

Conclusion: 从多语言教师模型中蒸馏出单语言模型是可行的，同样，从具有相对位置嵌入的教师模型中蒸馏出具有绝对位置嵌入的模型也是可行的。

Abstract: Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.

</details>


### [22] [Fast and Accurate Causal Parallel Decoding using Jacobi Forcing](https://arxiv.org/abs/2512.14681)
*Lanxiang Hu,Siqi Kou,Yichao Fu,Samyam Rajbhandari,Tajana Rosing,Yuxiong He,Zhijie Deng,Hao Zhang*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种名为Jacobi Forcing的渐进式蒸馏范式，旨在将自回归模型转换为高效的并行解码器，同时保留其预训练的因果推理特性，从而实现Transformer大模型的推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有的多token生成方法（如dLLMs）在加速Transformer大模型推理方面面临挑战，主要原因是预训练与后训练之间的不匹配，导致速度提升有限。具体来说，后训练中掩码数据的分布与预训练中真实数据的分布存在显著差异，并且dLLMs依赖双向注意力，这与预训练中学到的因果先验相冲突，并阻碍了精确KV缓存的重用。

Method: 本文引入了Jacobi Forcing，这是一种渐进式蒸馏范式。在该范式下，模型在其自身生成的并行解码轨迹上进行训练。这种方法能够平稳地将自回归模型转换为高效的并行解码器，同时保留其预训练的因果推理特性。此外，基于Jacobi Forcing模型的轨迹特性，论文还引入了带拒绝回收的多块解码技术，以进一步提高token接受数量并降低推理延迟。

Result: 通过Jacobi Forcing范式训练的模型（Jacobi Forcing Model）在编码和数学基准测试中实现了3.8倍的实际运行速度提升，且性能损失最小。此外，结合多块解码与拒绝回收技术，每个迭代的token接受数量提高了4.5倍，实际运行速度提升了近4.0倍。

Conclusion: Jacobi Forcing范式成功解决了现有并行解码方法中预训练与后训练不匹配的问题，将自回归模型转化为高效的并行解码器。该方法不仅显著提升了推理速度，且基本保持了性能，为Transformer大模型的推理加速提供了有效途径。

Abstract: Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.

</details>


### [23] [Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization](https://arxiv.org/abs/2512.14687)
*Yen-Ju Lu,Kunxiao Gao,Mingrui Liang,Helin Wang,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: 介绍了Spoken DialogSum一个包含对话音频和对应摘要以及副语言线索的数据集，并展示了其在情感摘要任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的研究缺乏将语音、摘要和副语言线索结合起来的数据集，这限制了情感感知或口语对话摘要的研究。

Method: Spoken DialogSum数据集的构建分为两个阶段：首先，LLM用Switchboard风格的填充词和反向通道重写DialogSum脚本，然后用情感、音高和语速标记每个话语。其次，富有表现力的TTS引擎根据标记脚本合成语音，并与副语言标签对齐。

Result: Spoken DialogSum包含13,460个情感多样化的对话，每个对话都配有事实性和情感性摘要。基线测试表明，Audio-LLM相对于级联ASR-LLM系统，将情感摘要的ROUGE-L提高了28%，证实了端到端语音建模的价值。

Conclusion: Spoken DialogSum数据集的引入，为情感感知和口语对话摘要研究提供了宝贵资源，并证明了端到端语音模型在此类任务中的优越性。

Abstract: Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.

</details>


### [24] [MMGR: Multi-Modal Generative Reasoning](https://arxiv.org/abs/2512.14691)
*Zefan Cai,Haoyi Qiu,Tianyi Ma,Haozhe Zhao,Gengze Zhou,Kung-Hsiang Huang,Parisa Kordjamshidi,Minjia Zhang,Xiao Wen,Jiuxiang Gu,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: MMGR是一个评估基础视频模型推理能力的框架，解决了现有指标忽视推理错误的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型评估指标，如FVD，主要关注感知质量，而忽视模型在物理、逻辑和空间约束方面的推理能力，导致无法准确评估其作为世界模拟器的可靠性。

Method: 我们引入了MMGR（多模态生成推理评估与基准），这是一个基于五种推理能力（物理、逻辑、3D空间、2D空间和时间）的评估框架。MMGR在三个领域（抽象推理、具身导航和物理常识）中评估生成推理，并应用细粒度指标，要求视频和图像生成在整体上都正确。

Result: 我们对领先的视频模型（Veo-3, Sora-2, Wan-2.2）和图像模型（Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image）进行了基准测试。结果显示，模型在不同领域之间存在显著的性能差距。模型在物理常识任务上表现出中等成功，但在抽象推理（ARC-AGI准确率低于10%）和具身设置中的长程空间规划方面表现不佳。

Conclusion: 当前模型存在对感知数据的过度依赖、全局状态一致性较弱以及目标函数奖励视觉合理性而非因果正确性等关键限制。MMGR提供了一个统一的诊断基准，为实现具有推理能力的生成式世界模型提供了途径。

Abstract: Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.

</details>


### [25] [MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset](https://arxiv.org/abs/2511.19317)
*Md. Tanzim Ferdous,Naeem Ahsan Chowdhury,Prithwiraj Bhattacharjee*

Main category: cs.CL

TL;DR: 本文介绍了一个新的孟加拉语抽象摘要数据集，解决了现有研究主要集中在新闻文章的问题，并通过包含来自博客、报纸和社交媒体的54,000多篇文章，实现了数据集的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有孟加拉语文本摘要研究主要集中在新闻文章上，缺乏对真实世界孟加拉语文本多样性的适应性；同时，数字时代孟加拉语内容爆炸式增长，需要摘要系统来减少信息过载。

Method: 收集了来自Cinegolpo等博客以及Samakal和The Business Standard等报纸的54,000多篇孟加拉语文章及其摘要。该数据集涵盖了多个领域和写作风格。在此数据集上训练并评估了多种深度学习和迁移学习模型，包括LSTM、BanglaT5-small和MTS-small，以建立强大的基线。

Result: 新数据集在多领域和多写作风格方面表现出更强的适应性和实用相关性。基于该数据集训练和评估的深度学习模型（如LSTM、BanglaT5-small和MTS-small）取得了良好的性能，证明了其作为孟加拉语自然语言处理未来研究基准的潜力。

Conclusion: 本研究成功构建了一个多样化的孟加拉语抽象摘要数据集，填补了现有研究的空白，并为孟加拉语摘要系统的发展和低资源语言的NLP资源扩展奠定了坚实基础。

Abstract: This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [26] [One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing](https://arxiv.org/abs/2512.13892)
*Albert Dorador*

Main category: stat.ML

TL;DR: 这篇论文提出了一种新的、确定性的置换方法，用于在机器学习模型中进行特征贡献估计，此方法比传统方法更快、更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统的置换方法在机器学习模型中估计特征贡献时，计算开销大且存在随机不稳定性。

Method: 本论文通过用一个单一的、确定性的、最优的置换来代替多次随机置换，实现了一种更快、更稳定且非随机的特征贡献估计方法。此外，它还引入了“系统变量重要性”框架，以应对特征相关性并进行模型压力测试。

Result: 该方法在近200种场景中（包括实际的家庭金融和信用风险应用）得到了验证，在小样本、高维度和低信噪比等挑战性条件下，改进了偏差-方差权衡和准确性。系统变量重要性框架能够揭示标准变量重要性度量所遗漏的依赖关系，并可用于审计模型对受保护属性的隐藏依赖，从而评估公平性和系统性风险。

Conclusion: 该研究提出了一种高效且稳定的特征贡献估计方法，并通过系统变量重要性框架，为模型审计和公平性/系统性风险评估提供了新途径。

Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.

</details>


### [27] [Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics](https://arxiv.org/abs/2512.13997)
*Aaron Wei,Milad Jalali,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 本文提出了一种针对MMD估计器的新框架，该框架允许在样本大小不相等的情况下进行双样本测试，从而避免了数据丢弃并提高了测试功效。


<details>
  <summary>Details</summary>
Motivation: 现有的双样本测试技术，特别是基于MMD的核选择方法，通常假设来自两个分布的样本大小相等。这在实践中可能导致丢弃有价值的数据，从而不必要地降低测试功效。

Method: 本文通过扩展广义U统计量的理论，并将其应用于MMD估计器，从而在样本大小不相等的情况下（特别是在现有部分结果所需的比例范围之外），对MMD估计器的渐近分布进行了新的表征。

Result: 这种泛化提供了一个新的判据，用于优化MMD测试在样本大小不相等情况下的功效。我们的方法保留了所有可用数据，从而提高了测试的准确性和在实际设置中的适用性。此外，我们还对MMD估计器的方差进行了更清晰的表征，揭示了在某些情况下，尽管MMD为非零，估计器仍然可能退化。

Conclusion: 本文提出的新框架有效地解决了MMD双样本测试中样本大小不等的限制，提高了测试功效和数据利用率，并对MMD估计器的方差特性有了新的认识。

Abstract: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.

</details>


### [28] [On the Hardness of Conditional Independence Testing In Practice](https://arxiv.org/abs/2512.14000)
*Zheng He,Roman Pogodin,Yazhe Li,Namrata Deka,Arthur Gretton,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 该文章分析了Kernel-based Conditional Independence (KCI) 测试，指出了其在实际应用中失败的主要原因，并强调了条件均值嵌入估计中的误差和条件核选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统的条件独立性（CI）测试在实际应用中经常失效，而Shah和Peters（2020）的工作虽然指出无法实现非平凡功效，但没有解释这些失效的原因。本文旨在深入探讨KCI测试，并找出其在实践中行为的主要影响因素。

Method: 本文主要通过分析Kernel-based Conditional Independence (KCI) 测试来研究CI测试的实际行为。作者指出，许多近期测试所采用的广义协方差度量是KCI的一种特例。文章重点考察了导致KCI测试实践中出现问题的主要因素。

Result: 研究发现，条件均值嵌入估计中的误差是导致KCI测试I类错误的关键因素。此外，选择合适的条件核对于测试的功效至关重要，但同时也会导致I类错误的增加，这一点在以往的研究中并未被认识到。

Conclusion: 为了提高条件独立性测试的性能，未来的研究和应用需要更加关注条件均值嵌入估计的准确性，并仔细权衡条件核选择对测试功效和I类错误的影响。

Abstract: Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on "hiding" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.

</details>


### [29] [Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms](https://arxiv.org/abs/2512.14221)
*Jiarong Fan,Juhyun Park. Thi Phuong Thuy Vo,Nicolas Brunel*

Main category: stat.ML

TL;DR: 本文提出了一种新的共形预测框架，用于处理缺失协变量数据，通过预插补、掩码和校正过程，保证了边际覆盖率和掩码条件有效性，同时显著减小了预测区间的宽度。


<details>
  <summary>Details</summary>
Motivation: 传统的共形预测方法在面对缺失协变量时无法保证覆盖率，而掩码条件有效性（MCV）是比边际覆盖率更理想的特性，因此需要一种新的方法来解决缺失数据引起的异质性。

Method: 本文提出了一种预插补-掩码-然后校正的框架，用于处理缺失值。该方法利用重新加权的共形预测过程来校正经过分布插补（多重插补）的校准数据集的预测集，使其与标准插补流程兼容。论文推导了两种算法，并证明它们近似满足边际有效性和MCV。

Result: 所提出的方法在保证目标覆盖率和MCV的同时，显著减小了预测区间的宽度。

Conclusion: 本文成功地提出了一种处理缺失协变量的共形预测框架，通过其独特的方法，不仅保证了预测的有效性，还提高了预测区间的效率。

Abstract: Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.

</details>


### [30] [Improving the Accuracy of Amortized Model Comparison with Self-Consistency](https://arxiv.org/abs/2512.14308)
*Šimon Kucharský,Aayush Mishra,Daniel Habermann,Stefan T. Radev,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 本文探讨了在模型选择环境中，摊销贝叶斯推断（ABI）中模型误设定对神经替代模型的影响，并研究了自洽性（SC）如何改进四种不同的摊销模型比较方法。


<details>
  <summary>Details</summary>
Motivation: ABI方法对模型误设定高度敏感，当观测数据超出训练分布时，神经替代模型的行为可能不可预测，这在模型比较中是一个挑战。

Method: 研究调查了自洽性（SC）如何改进四种不同的摊销模型比较方法。论文在两个合成案例和两个真实世界案例中进行了研究。

Result: 通过近似参数后验来估计边际似然的模型比较方法始终优于直接近似模型证据或后验模型概率的方法。SC训练在似然可用时提高了鲁棒性，即使在严重模型误设定的情况下也是如此。对于无法获得解析似然的方法，SC的益处较有限且不一致。

Conclusion: 为了实现可靠的摊销贝叶斯模型比较，建议优先选择基于参数后验的方法，并通过在经验数据集上进行SC训练来增强它们，以减轻模型误设定下的外推偏差。

Abstract: Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.

</details>


### [31] [Continual Learning at the Edge: An Agnostic IIoT Architecture](https://arxiv.org/abs/2512.14311)
*Pablo García-Santaclara,Bruno Fernández-Castro,Rebeca P. Díaz-Redondo,Carlos Calvo-Moa,Henar Mariño-Bodelón*

Main category: stat.ML

TL;DR: 该文章提出了一种在工业边缘计算场景中应用增量学习的新方法，用于制造系统中的实时质量控制。


<details>
  <summary>Details</summary>
Motivation: 传统的集中式计算系统面临延迟和带宽限制，而边缘计算可以解决这些问题。同时，传统的机器学习算法不适用于边缘计算系统中动态和持续的数据流，因此需要增量学习方案。

Method: 文章介绍了一种在工业边缘计算场景中应用增量学习的新方法，旨在实现制造系统中的实时质量控制。通过应用持续学习，可以减少灾难性遗忘的影响。

Result: 该方法提供了一种高效且有效的解决方案，减少了灾难性遗忘的影响。

Conclusion: 通过在工业边缘计算环境中采用增量学习，可以有效地解决实时质量控制问题，并减轻灾难性遗忘。

Abstract: The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.

</details>


### [32] [From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification](https://arxiv.org/abs/2512.14404)
*Hangjun Cho,Fabio V. G. Amaral,Andrei A. Klishin,Cassio M. Oishi,Steven L. Brunton*

Main category: stat.ML

TL;DR: 本文探讨了字典式稀疏回归，特别是序列阈值最小二乘法 (STLS)，并提出了一种分数引导的库选择方法，旨在为数据驱动建模提供实用指导，尤其侧重于 SINDy 类型算法。


<details>
  <summary>Details</summary>
Motivation: STLS 是一种求解 $\ell_0$ 稀疏最小二乘问题的算法，它依赖于分裂来有效求解最小二乘部分，同时通过近端方法处理稀疏项。它产生的系数向量的分量取决于投影重建误差（此处称为分数）和字典项的互相关性。

Method: 本文的第一个贡献是对分数和字典选择策略的理论分析。这可以在原始和弱 SINDy 两种机制下理解。其次，在常微分方程和偏微分方程上的数值实验突出了基于分数的筛选的有效性，提高了动力系统识别的准确性和可解释性。

Result: 这些结果表明，在某些情况下，集成分数引导方法来更准确地改进字典可能有助于 SINDy 用户增强其数据驱动发现控制方程的鲁棒性。

Conclusion: 本文提出了一种分数引导的库选择方法，旨在为数据驱动建模提供实用指导，尤其侧重于 SINDy 类型算法。数值实验证实了基于分数的筛选在提高动力系统识别的准确性和可解释性方面的有效性。这些结果表明，分数引导方法有助于 SINDy 用户增强其数据驱动发现控制方程的鲁棒性。

Abstract: In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.

</details>


### [33] [LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts](https://arxiv.org/abs/2512.14604)
*Prasanjit Dubey,Aritra Guha,Zhengyi Zhou,Qiong Wu,Xiaoming Huo,Paromita Dubey*

Main category: stat.ML

TL;DR: 该文章介绍了一个名为LLmFPCA-detect的框架，它结合了大型语言模型（LLM）的文本嵌入和函数数据分析，用于处理稀疏纵向文本数据，以实现聚类检测和异常推断。


<details>
  <summary>Details</summary>
Motivation: 稀疏纵向文本数据在多种场景下都有广泛应用，但由于目前缺乏专门的分析方法且数据本身具有噪声大、异构性强、易出现异常等特点，导致从中发现和推断关键模式非常困难。

Method: LLmFPCA-detect框架首先使用LLM提示将每段文本嵌入到特定于应用的数值空间中。然后，在数值空间中进行稀疏多元函数主成分分析（mFPCA），以恢复主要的群体特征并生成个体层面的分数。这些分数与基线静态协变量结合，可以用于数据分段、无监督异常检测和推断，并支持其他下游任务。该框架还会利用LLM对LLmFPCA-detect发现的数据段和异常进行动态关键词分析，并指出LLmFPCA-detect生成的特定于聚类的函数PC分数可以作为现有流水线中的特征，以提高预测性能。

Result: LLmFPCA-detect框架在Amazon客户评论轨迹和维基百科讨论页评论流这两个公共数据集上进行了实验验证，结果表明其在不同领域都具有实用性，并且优于目前最先进的基线方法。它能够稳定地检测聚类和推断异常，并提升预测性能。

Conclusion: LLmFPCA-detect是一个有效且灵活的框架，它通过结合LLM文本嵌入和函数数据分析，成功解决了稀疏纵向文本数据分析中的挑战，为发现关键模式、检测异常和提升下游任务的预测性能提供了新的解决方案。

Abstract: Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [34] [Multi-Agent Collaborative Framework for Intelligent IT Operations: An AOI System with Context-Aware Compression and Dynamic Task Scheduling](https://arxiv.org/abs/2512.13956)
*Zishan Bai,Enze Ge,Junfeng Hao*

Main category: cs.MA

TL;DR: AOI是一个多智能体协作框架，可以通过其动态任务调度策略和三层记忆架构来缓解信息过载并提高操作效率，从而实现下一代IT基础设施的自主运营。


<details>
  <summary>Details</summary>
Motivation: 云原生架构的普及使现代IT基础设施变得异常复杂和不稳定。这种复杂性导致 عمليات 数据量过大，传统的系统在信息处理、任务协调以及故障诊断和修复过程中的上下文连续性方面存在严重的瓶颈。

Method: 我们提出了AOI（AI-Oriented Operations），这是一种新颖的多智能体协作框架，它集成了三个专门的智能体和一个基于LLM的上下文压缩器。其核心创新包括：1. 动态任务调度策略，根据实时系统状态自适应地确定操作优先级；2. 包含工作层、情景层和语义层的三层记忆架构，优化上下文的保留和检索。

Result: 在合成和真实世界的基准测试中进行的大量实验表明，AOI有效地缓解了信息过载，实现了72.4%的上下文压缩率，同时保留了92.8%的关键信息，并显著提高了操作效率，任务成功率达到94.2%，与最佳基线相比，平均恢复时间（MTTR）减少了34.4%。

Conclusion: 这项工作提出了一个可扩展、自适应和上下文感知的自主操作的范式转变，从而以最少的人工干预实现下一代IT基础设施的稳健管理。

Abstract: The proliferation of cloud-native architectures, characterized by microservices and dynamic orchestration, has rendered modern IT infrastructures exceedingly complex and volatile. This complexity generates overwhelming volumes of operational data, leading to critical bottlenecks in conventional systems: inefficient information processing, poor task coordination, and loss of contextual continuity during fault diagnosis and remediation. To address these challenges, we propose AOI (AI-Oriented Operations), a novel multi-agent collaborative framework that integrates three specialized agents with an LLM-based Context Compressor. Its core innovations include: (1) a dynamic task scheduling strategy that adaptively prioritizes operations based on real-time system states, and (2) a three-layer memory architecture comprising Working, Episodic, and Semantic layers that optimizes context retention and retrieval. Extensive experiments on both synthetic and real-world benchmarks demonstrate that AOI effectively mitigates information overload, achieving a 72.4% context compression ratio while preserving 92.8% of critical information and significantly enhances operational efficiency, attaining a 94.2% task success rate and reducing the Mean Time to Repair (MTTR) by 34.4% compared to the best baseline. This work presents a paradigm shift towards scalable, adaptive, and context-aware autonomous operations, enabling robust management of next-generation IT infrastructures with minimal human intervention.

</details>


### [35] [Multi-Agent Medical Decision Consensus Matrix System: An Intelligent Collaborative Framework for Oncology MDT Consultations](https://arxiv.org/abs/2512.14321)
*Xudong Han,Xianglun Gao,Xiaoyi Qu,Zhenyu Yu*

Main category: cs.MA

TL;DR: 本文介绍了一个多智能体医疗决策共识矩阵系统，该系统利用七个专门的大型语言模型智能体模拟多学科团队（MDT）工作流程，并通过数学共识矩阵和强化学习方法量化共识、提高决策可追溯性。


<details>
  <summary>Details</summary>
Motivation: 目前癌症护理决策中的多学科团队（MDT）会诊缺乏量化共识和确保决策可追溯性的结构化机制。

Method: 该系统部署了七个专业的大型语言模型智能体（包括肿瘤学家、放射科医生、护士、心理学家、患者倡导者、营养师和康复治疗师），模拟真实的多学科团队工作流程。它融入了基于数学的共识矩阵，使用Kendall一致性系数客观评估一致性。此外，系统集成了Q-Learning、PPO和DQN等强化学习方法，以提高治疗推荐质量和共识效率。

Result: 在五个医学基准测试中（MedQA、PubMedQA、DDXPlus、MedBullets和SymCat），该系统相较于现有方法取得了显著的提升，平均准确率达到87.5%（最强基线为83.8%），共识达成率为89.3%，平均Kendall的W值为0.823。专家评审员对系统输出的临床适宜性评分为8.9/10。系统还通过强制引用临床指南和同行评审文献（遵循GRADE原则）确保了完整的证据可追溯性。

Conclusion: 该工作通过提供结构化的共识测量、角色专业化的多智能体协作和基于证据的可解释性，改进了临床决策的质量和效率，从而推动了医学人工智能的发展。

Abstract: Multidisciplinary team (MDT) consultations are the gold standard for cancer care decision-making, yet current practice lacks structured mechanisms for quantifying consensus and ensuring decision traceability. We introduce a Multi-Agent Medical Decision Consensus Matrix System that deploys seven specialized large language model agents, including an oncologist, a radiologist, a nurse, a psychologist, a patient advocate, a nutritionist and a rehabilitation therapist, to simulate realistic MDT workflows. The framework incorporates a mathematically grounded consensus matrix that uses Kendall's coefficient of concordance to objectively assess agreement. To further enhance treatment recommendation quality and consensus efficiency, the system integrates reinforcement learning methods, including Q-Learning, PPO and DQN. Evaluation across five medical benchmarks (MedQA, PubMedQA, DDXPlus, MedBullets and SymCat) shows substantial gains over existing approaches, achieving an average accuracy of 87.5% compared with 83.8% for the strongest baseline, a consensus achievement rate of 89.3% and a mean Kendall's W of 0.823. Expert reviewers rated the clinical appropriateness of system outputs at 8.9/10. The system guarantees full evidence traceability through mandatory citations of clinical guidelines and peer-reviewed literature, following GRADE principles. This work advances medical AI by providing structured consensus measurement, role-specialized multi-agent collaboration and evidence-based explainability to improve the quality and efficiency of clinical decision-making.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [36] [Deepfakes in the 2025 Canadian Election: Prevalence, Partisanship, and Platform Dynamics](https://arxiv.org/abs/2512.13915)
*Victor Livernoche,Andreea Musulan,Zachary Yang,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.SI

TL;DR: 该研究对2025年加拿大联邦选举期间深伪技术在社交媒体上的传播进行了深入分析。


<details>
  <summary>Details</summary>
Motivation: 尽管人们对AI生成的政治内容日益关注，但关于深伪技术在民主国家重大事件中如何出现和传播的实证证据有限。

Method: 分析了来自X、Bluesky和Reddit的187,778个帖子，使用高精度检测框架识别深伪图片。

Result: 在选举相关的图片中，有5.86%是深伪图片。右倾账户分享深伪图片的频率更高。然而，大多数被检测到的深伪图片是良性或非政治性的，有害的深伪图片关注度较低，仅占X上总浏览量的0.12%。

Conclusion: 深伪技术在选举对话中存在，但其传播范围有限，尽管不常见，但逼真的虚假图片获得了更高的参与度，这突显了对其潜在滥用的日益关注。

Abstract: Concerns about AI-generated political content are growing, yet there is limited empirical evidence on how deepfakes actually appear and circulate across social platforms during major events in democratic countries. In this study, we present one of the first in-depth analyses of how these realistic synthetic media shape the political landscape online, focusing specifically on the 2025 Canadian federal election. By analyzing 187,778 posts from X, Bluesky, and Reddit with a high-accuracy detection framework trained on a diverse set of modern generative models, we find that 5.86% of election-related images were deepfakes. Right-leaning accounts shared them more frequently, with 8.66% of their posted images flagged compared to 4.42% for left-leaning users, often with defamatory or conspiratorial intent. Yet, most detected deepfakes were benign or non-political, and harmful ones drew little attention, accounting for only 0.12% of all views on X. Overall, deepfakes were present in the election conversation, but their reach was modest, and realistic fabricated images, although less common, drew higher engagement, highlighting growing concerns about their potential misuse.

</details>


### [37] ["Talking past each other": Issue ownership and microtargeting in Swiss online political ads](https://arxiv.org/abs/2512.14564)
*Arthur Capozzi*

Main category: cs.SI

TL;DR: 本文分析了瑞士2021-2025年间Facebook和Instagram上的政治广告数据，揭示了直投民主背景下在线广告如何影响公投结果，并展示了不同政党的微定位策略和内容分歧。


<details>
  <summary>Details</summary>
Motivation: 探索瑞士直投民主背景下在线政治广告的影响，尤其是在非选举周期内对公投和公共领域的影响。

Method: 对2021年至2025年间瑞士Facebook和Instagram上的4万条政治广告进行大规模数据驱动分析，涉及450万瑞士法郎的支出和5.6亿次展示。

Result: 政治广告不仅用于联邦选举，也用于影响公投，其中“支持”广告的曝光度与批准结果显著正相关。微定位策略存在党派差异：中右翼政党 targeting 老年男性，左翼政党 targeting 年轻女性。地域差异显著。内容上存在“各说各话”现象，政党倾向于推广独有的议题。广告作者可以通过受众和话题特征进行预测。

Conclusion: 社交平台上的微定位和议题分歧可能导致公共领域碎片化，并绕过传统的民主审议。

Abstract: Switzerland's unique system of direct democracy, characterized by frequent popular referenda, provides a critical context for studying the impact of online political advertising beyond standard electoral cycles. This paper presents a large-scale, data-driven analysis of 40k political ads published on Facebook and Instagram in Switzerland between 2021 and 2025. Despite a voting population of only 5.6 million, the ad campaigns were significant in scale, costing CHF 4.5 million and achieving 560 million impressions. This study shows that political ads are used not only for federal elections, but also to influence referenda, where greater exposure to ``pro-Yes'' advertising correlates significantly with approval outcomes. The analysis of microtargeting reveals distinct partisan strategies: centrist and right-wing parties predominantly target older men, whereas left-wing parties focus on young women. Furthermore, significant region-specific demographic variations are observed even within the same party, reflecting Switzerland's strong territorial divisions. Regarding content, a clear pattern of ``talking past each other'' is identified: in line with issue ownership theory, parties avoid direct debate on shared issues, preferring to promote exclusively owned topics. Finally, it is demonstrated that these strategies are so distinct that an ad's author can be predicted using a machine learning model trained exclusively on its audience and topic features. This study sheds light on how microtargeting and issue divergence on social platforms may fragment the public sphere and bypass traditional democratic deliberation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 本文探讨了大型语言模型在专业任务微调中出现的灾难性遗忘问题，并提出混合训练策略以有效解决该问题，从而在保持专业性能的同时，避免通用能力的损失。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型针对特定任务进行微调时，模型会遗忘之前学到的能力，即灾难性遗忘。本文旨在研究这一现象，并寻求解决方案，以期在模型专业化的同时保留其通用能力。

Method: 本文通过在DeepMind Mathematics数据集上对Flan-T5-Base（250M参数）进行微调，并测量MultiNLI上的遗忘情况来 investigates 灾难性遗忘问题。为了解决遗忘问题，本文提出了混合训练策略，在训练过程中交错使用数学和NLI示例。

Result: 仅进行数学训练可以将数学准确率从3.1%提高到12.0%，但会导致NLI准确率从81.0%骤降至16.5%，在最初的1000个训练步骤中下降了64.5个百分点。混合训练策略完全消除了灾难性遗忘，同时保持了与仅数学训练相当的数学性能：1:1的平衡混合比例在达到12.0%数学准确率（与仅数学训练持平）的同时，NLI准确率保持在86.2%。即使是最低限度的NLI暴露（6.2%），也能提供有效的正则化。

Conclusion: 专业化不需要以遗忘通用能力为代价。混合训练策略可以有效防止灾难性遗忘，并且对于未来扩展到更大的模型可能会带来额外的益处。

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [39] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: 该研究结合合成孔径雷达（SAR）图像与环境水文数据，在肯尼亚西部Nyando河流域建立了洪水敏感性模型，并发现随机森林（RF）模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球最具破坏性的自然灾害之一，对生态系统，基础设施和人类生计构成严重风险。

Method: 本研究结合了合成孔径雷达（SAR）图像和环境水文数据，利用2024年5月的Sentinel-1双极化SAR数据生成了二元洪水库，作为机器学习（ML）模型的训练数据。 将坡度、高程、坡向、土地利用/土地覆盖、土壤类型和距河流的距离六个条件因素与SAR衍生的洪水库相结合，训练了逻辑回归（LR）、分类回归树（CART）、支持向量机（SVM）和随机森林（RF）四种监督分类器。

Result: 随机森林（RF）模型取得了最高的预测性能（准确性=0.762；Kappa=0.480），优于LR、CART和SVM。基于RF的敏感性地图显示，维多利亚湖附近的低洼Kano平原具有最高的洪水脆弱性。

Conclusion: 该研究结果证明了结合SAR数据和集成机器学习方法对数据有限地区进行洪水敏感性测绘的价值。生成的地图为减少灾害风险、土地利用规划和预警系统开发提供了重要见解。

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [40] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: 本文提出了一种名为Hessian Reassignment的两步模型无关解决方案，用于文档分类器中的类级别遗忘。该方法通过Hessian-vector系统去除目标类的贡献，并使用Top-1分类来保证决策空间，实现了接近完全重新训练的留存类准确性，同时显著降低了成员推理优势。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLMs）的遗忘技术已取得很大进展，但文档分类模型的遗忘研究相对较少。本文旨在研究文档分类器中的类级别遗忘问题。

Method: Hessian Reassignment是一个两步、模型无关的解决方案。首先，通过求解一个共轭梯度Hessian-vector系统，进行一次影响式更新，以减去目标类所有训练点的贡献；这仅需要梯度和Hessian-vector乘积。其次，不同于常见的随机重新分类删除类样本的基线方法，本文通过Top-1分类强制执行决策空间保证。

Result: 在标准文本基准测试中，Hessian Reassignment在运行速度快几个数量级的同时，实现了接近于完全重新训练（不包含该类）的留存类准确性。此外，它还通过汇集多影攻击测量，持续降低了被删除类的成员推理优势。

Conclusion: Hessian Reassignment为文档分类中的高效类遗忘提供了一条实用且有原则的途径。

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [41] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: 该研究开发了一个机器学习框架，通过整合废水监测、气象和空气质量数据，预测美国与呼吸道合胞病毒（RSV）相关的住院情况。


<details>
  <summary>Details</summary>
Motivation: RSV是导致幼儿住院的主要原因，其爆发深受环境条件影响。

Method: 结合每周住院率、废水RSV水平、每日气象测量和空气污染物浓度，训练了CART、随机森林和Boosting分类模型，以预测RSV相关的住院率水平（低风险、警报和流行）。

Result: 废水RSV水平被确定为最强的预测因子，其次是气象和空气质量变量（如温度、臭氧水平和特定湿度）。分析还显示，美国原住民和阿拉斯加原住民的RSV相关住院率显著更高，高海拔州的RSV相关住院率也持续较高。

Conclusion: 结合环境和社区监测数据对预测RSV爆发具有重要价值，可以实现更及时的公共卫生干预和资源分配。研究还开发了一个交互式R Shiny仪表盘，以方便用户探索和预测RSV爆发。

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [42] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: 该研究提出了一种联邦小样本学习框架，用于在数据稀缺和隐私受限的临床环境中进行个性化癫痫检测。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习癫痫检测方法依赖于大型集中式标注数据集，但在临床实践中，EEG数据通常稀缺、分散且受隐私法规限制，因此难以创建实用的AI模型。

Method: 该方法是一个两阶段的联邦小样本学习（FFSL）框架。第一阶段，使用联邦学习在非IID模拟医院站点上微调预训练的生物信号转换器（BIOT），实现共享表示学习。第二阶段，联邦小样本个性化阶段，仅使用五个标记的EEG片段，为每个患者调整分类器，同时保留癫痫特异性信息并受益于跨站点知识。在TUH事件语料库上进行训练和评估。

Result: 联邦微调的平衡准确率为0.43，Cohen’s kappa为0.42，加权F1为0.69。FFSL阶段，客户端模型的平均平衡准确率达到0.77，Cohen’s kappa为0.62，加权F1为0.73。

Conclusion: FFSL框架可以在现实世界中数据可用性和隐私限制下，有效地支持患者自适应的癫痫检测。

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [43] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种在有限的用户时间预算下，通过平衡推荐物品的相关性和评估成本来优化推荐系统的方法。作者提出了基于强化学习的算法，旨在同时学习用户偏好和时间预算模式，从而在资源约束下提高推荐的参与度。


<details>
  <summary>Details</summary>
Motivation: 在传统的推荐任务中，用户有限的时间预算引入了一个关键的资源限制，使得推荐系统需要平衡物品的相关性和评估成本。例如，在移动购物界面中，用户通过滚动与推荐互动，每次滚动都会触发一个商品列表，用户需要花费时间评估商品特征。高相关性但评估成本较高的商品可能不符合用户的时间预算，从而影响用户的参与度。

Method: 本研究通过将时间受限的列表推荐建模为具有预算感知效用的马尔可夫决策过程（MDPs）来统一这一问题。作者还构建了一个模拟框架来研究策略行为，并使用阿里巴巴的个性化重排序数据集来支持电子商务环境中的列表优化。研究比较了在线策略和离线策略控制方法与传统的基于上下文多臂老虎机方法的性能。

Result: 实验结果表明，在严格的时间预算下，基于在线策略和离线策略控制的强化学习方法比传统的基于上下文多臂老虎机的方法能够显著提高性能和用户参与度。

Conclusion: 本论文提出了一个解决用户时间预算限制下推荐系统挑战的新框架，并通过强化学习算法在理论和实践上证明了其有效性，为在资源约束下提高用户参与度提供了新的思路和方法。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [44] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的复合分类器自由指导（CCFG）方法，以解决扩散模型在处理风数据超分辨率时多输入通道的挑战。CCFG在风超分辨率任务中表现出比传统CFG更高的保真度，并且其构建的WindDM模型在重建质量上达到最先进水平，同时成本大大降低。


<details>
  <summary>Details</summary>
Motivation: 气象建模问题需要高分辨率、高精度的风数据，而获取此类数据成本高昂且具有挑战性。传统的重建方法难以在成本效益和准确性之间取得平衡。深度学习方法，尤其是扩散模型，通过借鉴自然图像超分辨率的进展，有望解决这一问题。然而，风数据与自然图像不同，其输入通道数远多于自然图像，这给扩散模型带来了新的挑战。

Method: 本文提出了一种复合分类器自由指导（CCFG）方法，该方法是分类器自由指导（CFG）的泛化，可以处理多个条件输入。CCFG可以与任何使用标准CFG dropout训练的预训练扩散模型结合使用。此外，文章还提出了WindDM，一个专门用于工业规模风动力学重建的扩散模型，并集成了CCFG。

Result: CCFG在风超分辨率任务上的输出保真度高于传统的CFG。WindDM模型在深度学习模型中实现了最先进的重建质量，并且相较于传统方法，成本降低了高达1000倍。

Conclusion: 本文成功地将分类器自由指导泛化到多条件输入，提出了CCFG方法，有效解决了风数据超分辨率中多通道输入的挑战。结合CCFG的WindDM模型在重建质量和成本效益方面均达到了行业领先水平，为高分辨率风数据的获取提供了一个有前景的解决方案。

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [45] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: PIS是一个基于扩散的框架，能够处理稀疏、不规则观测条件下的PDE逆问题。它采用Set Transformer编码器，并利用余弦退火稀疏性课程来提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在有限的间接测量，特别是观测数据稀疏、不规则且受限于传感器放置的条件下，偏微分方程（PDE）约束的物理参数估计存在固有的病态性。

Method: PIS（物理反演求解器）是一个集合条件下的扩散框架。PIS采用基于Set Transformer的编码器来处理任意数量或几何形状的测量。该模型还采用了余弦退火稀疏性课程以提高鲁棒性。

Result: PIS在所有任务和稀疏性条件下，包括观测率仅为0.29%的极端情况，都表现出稳定和准确的性能，将反演误差降低了12.28%—88.73%。PIS能够可靠地生成经过校准的后验样本，这些样本准确反映了数据稀缺性和内在物理模糊性。

Conclusion: PIS是一个强大、通用且独特地对稀疏性具有韧性的解决方案，适用于任意和严重欠采样的观测条件下的物理反演。

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [46] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: 该文章提出了一种名为LLRC的梯度方法，用于在无微调设置下选择奇异值，以解决大型语言模型低秩压缩中选择最佳秩的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩分解压缩大型语言模型的方法在选择最佳层秩时面临挑战，这影响了压缩率和下游任务的准确性。目前的方法要么依赖启发式，但搜索空间有限，导致次优结果；要么是基于梯度的，但性能不如启发式方法。

Method: 本文提出了一种名为Learning to Low-Rank Compress (LLRC) 的梯度方法。LLRC通过学习掩码的权重来选择奇异值，而无需进行后压缩微调。该方法使用校准数据集，仅训练掩码权重以选择更少的奇异值，同时最小化中间激活与原始模型之间的 Hellinger 距离。

Result: LLRC在常识推理和开放域问答任务中，在各种压缩率下优于其他不需要后压缩微调的秩选择方法，例如，在Llama-2-13B上，当压缩率为20%时，LLRC在MMLU，BoolQ和OpenbookQA上分别超越STRS 12%，3.5%和4.4%。

Conclusion: LLRC通过直接学习奇异值选择掩码的权重，无需微调即可有效解决大型语言模型低秩压缩的挑战，并在多项任务和数据集上取得了优于现有方法的表现。

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [47] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 该论文提出了一个联邦推荐（FR）训练框架，该框架采用基于参数高效微调（PEFT）的嵌入设计来减少需要传输的嵌入参数量，从而解决了FR中大规模项目嵌入导致的通信效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦推荐（FR）研究主要关注提高模型效率，但往往忽视了嵌入参数开销问题。大规模的项目嵌入导致了大量的参数，严重影响了通信效率。

Method: 该论文提出了一个FR训练框架，采用基于参数高效微调（PEFT）的嵌入设计，以减少需要传输的嵌入参数量。该方法是一个轻量级的插件式解决方案，可以无缝集成到现有的FR方法中。除了结合LoRA和基于哈希编码等常见PEFT技术外，还探索了使用残差量化变分自编码器（RQ-VAE）作为框架内新颖的PEFT策略。

Result: 通过在各种FR模型骨干和数据集上进行大量实验，结果表明该框架在显著降低通信开销的同时提高了准确性。

Conclusion: 该论文成功地提出了一个基于PEFT的FR训练框架，有效解决了联邦推荐中由于大规模项目嵌入导致的通信效率低下问题，并通过实验证明了其在降低通信开销和提高准确性方面的优越性。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [48] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: 该文章提出了一种名为DARTs的多元时间序列异常检测框架，旨在解决高维噪声时间序列中长期时空依赖捕获的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多元时间序列异常检测方法在低维场景中表现良好，但在高维噪声时间序列中学习表示时，往往无法鲁棒地捕获长期的时空依赖。

Method: DARTs框架包含三个主要组件：
1. **短期路径（Short-term path）**：引入了多视图稀疏图学习器（Multi-View Sparse Graph Learner）和扩散多关系图单元（Diffusion Multi-Relation Graph Unit），协同自适应地捕获高噪声时间序列中的分层判别性短期时空模式。
2. **长期路径（Long-term path）**：设计了多尺度时空图构造器（Multi-Scale Spatiotemporal Graph Constructor），用于在高维表示空间中建模显著的长期动态。
3. **窗口感知时空软融合机制（Window-aware spatiotemporal soft-fusion mechanism）**：用于过滤残余噪声并无缝集成异常模式。

Result: 在主流数据集上进行了广泛的定性和定量实验，结果表明DARTs框架具有优越性和鲁棒性。还进行了一系列消融研究，以探索所提组件的关键设计因素。

Conclusion: DARTs是一个鲁棒的长短期双路径框架，带有窗口感知时空软融合机制，能够有效解决高维噪声时间序列中长期时空依赖捕获的难题，并在异常检测方面表现出色。

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [49] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 该研究提出了一种名为TF-MCL的对比学习模型，用于解决基于脑电图（EEG）信号的抑郁症（MDD）检测中标签获取困难的问题。该模型通过融合时频信息和多域交叉损失函数显著提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管监督学习方法在基于脑电图（EEG）信号的重度抑郁症（MDD）检测中应用日益广泛，但其对标签的过度依赖性仍是一个挑战。现有的对比学习方法未能充分捕捉脑电信号的时频分布特性，且在MDD检测任务中获取低语义数据表示的能力不足。

Method: 本文提出了一种时频融合与多域交叉损失（TF-MCL）模型。该模型通过融合映射头（FMH）生成时频混合表示，以有效地将时频域信息重新映射到融合域，从而增强模型综合时频信息的能力。此外，通过优化多域交叉损失函数，重建时频域和融合域中表示的分布，以提高模型获取融合表示的能力。

Result: TF-MCL模型在公开数据集MODMA和PRED+CT上的表现优于现有最先进（SOTA）方法，准确率分别提高了5.87%和9.96%。

Conclusion: TF-MCL模型通过有效融合时频信息和优化多域交叉损失函数，显著提高了MDD检测的准确性，为MDD的检测提供了一个更有效且对标签依赖性更低的解决方案。

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [50] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: 该文章介绍了“层流假说”，提出LLMs中的对抗性攻击会导致其潜在空间中出现“语义湍流”，并利用层间余弦速度方差这一新颖指标来检测这种现象。


<details>
  <summary>Details</summary>
Motivation: 目前的LLMs防御策略依赖计算成本高的外部分类器或脆弱的词法过滤器，忽略了模型推理过程的内在动态。

Method: 文章提出了“层流假说”，认为良性输入在LLM高维潜在空间中引起平滑渐进的转换，而对抗性提示则引发混乱、高方差的轨迹，称之为“语义湍流”；这种湍流源于安全对齐和指令遵循目标之间的内部冲突。文章通过一个新的零样本指标——层间余弦速度的方差，将这种现象形式化。

Result: 在不同的LLMs上的实验评估显示出显著的诊断能力。RLHF对齐的Qwen2-1.5B在攻击下湍流增加了75.4%（p小于0.001），验证了内部冲突的假说。相反，Gemma-2B的湍流减少了22.0%，这表明它有一种独特的、低熵的“反射式”拒绝机制。

Conclusion: 语义湍流不仅可以作为一种轻量级的实时越狱检测器，还可以作为一种非侵入性诊断工具，用于对黑盒模型的底层安全架构进行分类。

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [51] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 本文评估了在资源有限的情况下，嵌入式方法对金融新闻情感分类的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决金融情感分析中小数据集的挑战，特别是在资源受限的环境下。

Method: 结合Word2Vec、GloVe和句子 transformer 表示与梯度提升模型，在人工标注的头条新闻上进行评估。

Result: 模型在验证集上表现良好，但在测试集上表现不佳，甚至低于基线模型；预训练嵌入在数据量不足时效果不佳；小的验证集会导致模型选择时的过拟合。

Conclusion: 金融情感分析中，在数据稀缺的情况下，仅依靠嵌入质量不足以解决问题。建议采用小样本学习、数据增强或结合词典的混合方法来应对资源受限的环境。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [52] [Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training](https://arxiv.org/abs/2512.13770)
*Huaiyuan Xiao,Fadi Dornaika,Jingjun Bi*

Main category: cs.LG

TL;DR: MV-SupGCN是一种半监督GCN模型，通过结合交叉熵损失和监督对比损失、融合多种图构建方法以及整合对比学习与伪标签，有效处理复杂多视图数据，并在多个基准测试中超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用视图间的互补信息，导致特征表示不理想和性能受限。

Method: MV-SupGCN模型集成了三个互补组件：
1.设计了结合交叉熵损失和监督对比损失的联合损失函数，以最小化类内方差并最大化类间可分离性。
2.结合KNN和半监督图构建方法来增强数据结构表示的鲁棒性。
3.提出了一个统一框架，该框架集成了对比学习以强制多视图嵌入之间的一致性，同时使用伪标签为交叉熵和对比损失函数提供额外的监督。

Result: MV-SupGCN在多个基准测试中持续超越SOTA方法。

Conclusion: MV-SupGCN通过其集成方法有效地解决了多视图学习中的挑战，并取得了优异的性能。

Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN

</details>


### [53] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SCPO的策略学习方法，旨在在满足安全约束的前提下提高性能。SCPO通过在参数空间中直接实施安全约束，避免了对约束函数梯度的需求，并建立了安全归纳保证。


<details>
  <summary>Details</summary>
Motivation: 在不脱离安全操作范围的前提下，提高安全关键学习策略的性能。

Method: SCPO是一种基于采样的权重空间投影方法，它通过结合轨迹rollout和平滑度边界来构建局部安全区域，这些边界将参数变化与安全指标的变化相关联。每次梯度更新都通过凸SOCP进行投影，从而产生安全的。对于具有稳定备份策略的受限控制设置，SCPO能进一步确保闭环稳定性，并实现超越保守备份的安全适应。

Result: 在有害监督下的回归任务和恶意专家的受限双积分器任务中，SCPO始终拒绝不安全的更新，在整个训练过程中保持可行性，并实现了有意义的原始目标改进。

Conclusion: SCPO通过在参数空间中直接实施安全约束，有效地解决了安全关键学习中的挑战，在保证安全性的同时提高了策略性能。

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [54] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: CTVP 是一种新颖的 AI 控制框架，通过语义轨道分析，验证不受信任的代码生成模型，以解决大型语言模型中后门注入和恶意行为的担忧。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面的自主性日益增强，引发了对后门注入和恶意行为的担忧。

Method: CTVP 通过分析语义等效程序转换中预测的执行轨迹的一致性模式，检测行为异常，而无需直接执行潜在恶意代码。

Result: 引入了对抗鲁棒性商数 (ARQ)，量化了验证的计算成本相对于基线生成的成本，并证明了其随轨道大小呈指数增长。

Conclusion: 语义轨道分析为代码生成任务提供了一种可扩展、理论基础坚实的 AI 控制方法。

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [55] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种改进RLHF对语言模型进行对齐的方法，通过事后解释识别导致不满意的响应的训练数据，然后通过遗忘这些数据来改进响应，同时不影响满意的响应。


<details>
  <summary>Details</summary>
Motivation: 目前RLHF调优的语言模型仍可能输出不尽人意的响应，本文旨在找到并纠正导致这些不满意响应的原因。

Method: 本文方法包含两部分：1. 事后解释方法：通过识别导致不满意响应的训练数据来解释原因。这被建模为一个受约束的组合优化问题，目标是在特征空间中找到最接近提示-响应对的训练数据子集，且该提示-响应对可分解为该子集在特征空间中的凸组合。提出了一个高效的迭代数据选择算法来解决此问题。2. 遗忘方法：通过遗忘导致不满意响应的训练数据来改进响应，同时不显著降低其他满意响应的质量。

Result: 实验结果表明，该算法可以改进RLHF。

Conclusion: 本文提出了一种新颖的方法，通过识别并纠正导致语言模型输出不满意响应的训练数据，从而有效地改进了RLHF的对齐效果。

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [56] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 这篇文章解决了如何估计二分类器 L1 校准误差的问题。


<details>
  <summary>Details</summary>
Motivation: 目前的工作普遍缺乏可靠的校准误差估计方法。因此，本文旨在提供一种有效的方法来估计二分类器的 L1 校准误差，并确保该方法在实际应用中的可行性。

Method: 文章提出了两种方法：一是为校准函数有界变化的分类器提供一个 L1 校准误差的上限；二是提出一种修改任何分类器的方法，使其校准误差能被有效地上限估计，且不显著影响分类器性能，也无需任何限制性假设。

Result: 所有结果都是非渐近和无分布的，可以提供实用的程序，运用于真实世界的数据集。

Conclusion: 文章的结论是 L1 校准误差可以被有效地和可靠地估计。本文还提供了关于如何在实践中测量校准误差的建议，这对于实际应用具有指导意义。

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [57] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 该论文介绍了一种用于婴儿哭声分类的端到端联邦学习框架，旨在解决隐私、噪声和领域 H 的挑战，并在F1 分数、AUC 和 ECE 方面取得了显著性能。


<details>
  <summary>Details</summary>
Motivation: 婴儿哭声分类有助于早期评估婴儿需求，但现有解决方案在部署时面临音频数据隐私、背景噪声敏感和跨 L 域迁移等挑战，这促使作者开发一个端到端的分析流程来解决这些问题。

Method: 本文提出了一种集成了去噪自动编码器（DAE）、卷积分词器和 Transformer 编码器的端到端婴儿哭声分析流程。该系统在设备端进行去噪、自适应分割、事后校准和基于能量的越界（OOD）弃权。在联邦学习过程中，采用带有 8 位适配器增量的正则化控制变量更新，并在安全聚合下进行训练。

Result: 在包含 Baby Chillanto 和 Donate-a-Cry 数据集（叠加ESC-50 噪声）的测试中，模型取得了 0.938 的宏 F1 分数、0.962 的 AUC 和 0.032 的预期校准误差（ECE）。同时，该方法将每轮客户端上传数据量从约 36-42 MB 减少到 3.3 MB。在 NVIDIA Jetson Nano 上进行的实时边缘推理显示，每秒语谱图帧的处理时间仅为 96 毫秒。

Conclusion: 本研究提出的方法为隐私保护、抗噪声和通信高效的婴儿哭声分类提供了一条实用途径，适用于联邦部署。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [58] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: OPTIMA是一种实用的单次训练后剪枝方法，它通过将层级权重重建建模为独立的行级二次规划问题，并在共享Hessian的结构下高效求解，从而在不进行微调的情况下显著提高大型语言模型的零样本准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的训练后模型剪枝方法在效率和准确性之间存在权衡：简单的启发式方法速度快但准确性低，而联合优化方法准确性高但计算成本高。

Method: OPTIMA将掩码选择后的层级权重重建视为独立的行级二次规划（QP），这些QP共享一个共同的层Hessian。它实现了一个加速器友好的QP求解器，可以在单个加速器上并行处理多个小QP，从而实现大规模的单次训练后剪枝。

Result: OPTIMA在多个LLM系列和稀疏度范围内始终如一地提高了零样本性能，绝对准确性提高了3.97%。在NVIDIA H100上，OPTIMA可以在40小时内对8B参数的Transformer进行端到端剪枝，峰值内存为60GB。

Conclusion: OPTIMA在单次训练后剪枝的准确性和效率之间取得了新的最先进的平衡，为LLM剪枝提供了一种实用且高性能的解决方案。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [59] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 该研究表明，针对长上下文LLMs，少量上下文特定训练比增加思考令牌等现有推理时间策略更有效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的上下文长度已达到数百万代币，但它们在实际使用中对长文本的可靠利用能力不足。现有的推理时间计算策略，如生成思考令牌，在处理长上下文任务时效果有限且收益递减。

Method: 通过在沙盒长上下文任务上进行受控实验，研究团队发现现有的推理时间策略在长上下文任务中表现出收益递减并失效。他们将失败归因于静态自注意力固有的“分数稀释”现象。研究还发现，在某些条件下，当前的推理时间策略无法检索相关的长上下文信号。提出了一种通过对给定上下文进行有针对性的梯度更新的简单方法，以克服静态自注意力的局限性。

Result: 实验结果显示，通过上下文特定训练，Qwen3-4B模型在LongBench-v2和ZeroScrolls基准测试的子集上，平均分别取得了12.6和14.1个百分点的显著性能提升。

Conclusion: 对于长上下文任务，将计算资源用于少量的上下文特定训练，比采用生成更多思考令牌等现有的推理时间扩展策略更为有效。这种方法可以显著提升模型在处理长上下文时的性能。

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [60] [RePo: Language Models with Context Re-Positioning](https://arxiv.org/abs/2512.14391)
*Huayang Li,Tianyu Zhao,Richard Sproat*

Main category: cs.LG

TL;DR: 该论文提出了RePo，一种通过上下文重新定位来减少LLM中无关认知负荷的机制，与传统方法不同，RePo利用可微分模块$f_φ$分配捕获上下文依赖关系的令牌位置。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLM）的上下文学习是基础，但主流架构通过分配线性或常数位置索引施加了僵化和固定的上下文结构，这增加了无关认知负荷，消耗了有限的工作记忆能力。

Method: RePo利用可微分模块$f_φ$来分配令牌位置，这些位置捕获上下文依赖性，而不是依赖预定义的整数范围。

Result: 通过在OLMo-2 1B骨干网上持续预训练，RePo显著提升了在涉及嘈杂上下文、结构化数据和更长上下文长度任务上的性能，同时在一般短上下文任务上保持了有竞争力的表现。详细分析表明，RePo成功地将更高的注意力分配给遥远但相关的信息，在密集和非线性空间中分配位置，并捕获输入上下文的内在结构。

Conclusion: RePo通过创新的上下文重新定位机制，有效降低了大型语言模型中的认知负荷，显著提升了模型在复杂上下文任务中的表现，并能更好地捕捉上下文的内在结构。

Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

</details>


### [61] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 该研究介绍了一种语言模型，通过滑动窗口循环（SWR）技术，在保持性能的同时，将Transformer模型加速了10-40%。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在质量和性能上存在局限性，并且现有的多混合架构在处理长上下文时效率不高。作者旨在开发一种新的架构来克服这些限制，提高语言模型的效率和质量。

Method: 本文提出了一种针对线性循环的层次分解框架，并在此基础上开发了滑动窗口循环（SWR）算法。SWR通过将循环截断为与硬件对齐的窗口来限制昂贵的warp间通信。此外，SWR还允许开发Phalanx层，这些层可以替代窗口注意力机制或线性循环。

Result: 在具有10亿个参数的多混合模型中，Phalanx层在4K到32K的上下文长度范围内，比优化的Transformer模型实现了10-40%的速度提升，同时保持了相同的困惑度（perplexity）。

Conclusion: 滑动窗口循环（SWR）和Phalanx层为语言模型提供了一种高效的替代方案，可以在不牺牲模型质量的前提下，显著提高模型在长上下文处理上的速度。

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [62] [EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training](https://arxiv.org/abs/2511.10333)
*Qingao Yi,Jiaang Duan,Hanwen Hu,Qin Hua,Haiyan Zhao,Shiyou Qian,Dingyu Yang,Jian Cao,Jinghua Tang,Yinghao Yu,Chenzhi Liao,Kangjin Wang,Liping Zhang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为 EDGC 的熵驱动动态梯度压缩框架，通过根据梯度熵的变化趋势调整压缩率，显著减少了大型语言模型（LLM）训练过程中的通信延迟和训练时间，同时保持了模型精度。


<details>
  <summary>Details</summary>
Motivation: 尽管分布式训练技术能缓解大型语言模型（LLMs）训练中的计算资源和内存限制，但仍面临显著的通信开销。现有方法主要依赖静态梯度压缩，但这种方法忽略了训练过程中梯度演变的动态特性，导致性能下降。因此，如何在不牺牲性能的情况下加速LLM训练是一个挑战。

Method: 本文提出了一个名为 EDGC 的熵驱动动态梯度压缩框架，包含三个关键组件：
1. **梯度熵估计下采样**：采用下采样方法高效估计梯度熵，减少计算开销。
2. **理论模型建立**：建立了压缩率与梯度熵之间的理论模型，以支持更明智的压缩决策。
3. **窗口式调整机制**：通过基于窗口的调整机制，动态适应管道阶段间的压缩率，以提高通信效率并保持模型性能。

Result: EDGC 在32张NVIDIA V100 GPU和64张NVIDIA H100 GPU集群上，分别训练了GPT2-2.5B和GPT2-12.1B模型。结果表明，EDGC显著降低了通信延迟（高达46.45%）和训练时间（高达16.13%），同时保持了LLM的精度。

Conclusion: EDGC 通过动态调整梯度压缩率，有效解决了大型语言模型分布式训练中的通信开销问题，实现了显著的训练加速，同时维护了模型的准确性，为LLM的训练提供了一种高效且实用的解决方案。

Abstract: Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.

</details>


### [63] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: 该论文提出了PGDM，一个利用时间数据中固有模式进行预测的扩散模型，并在两个应用中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在多元时间序列预测中未充分考虑数据中反复出现的结构或模式。

Method: PGDM首先使用原型分析提取模式，并估计序列中最可能出现的下一个模式。然后，PGDM利用这个模式估计来指导预测，并引入了一种基于原型分析的新型不确定性量化技术，根据模式估计的不确定性动态调整指导水平。

Result: PGDM将视觉场测量和运动捕捉帧预测的性能（MAE/CRPS）分别提高了40.67%/56.26%和14.12%/14.10%。PGDM还比基线模型表现更好，分别提高了65.58%/84.83%和93.64%/92.55%。

Conclusion: PGDM通过利用数据中的固有模式显著提高了多元时间序列的预测性能，并在应用中展现出优越性。

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [64] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: 这篇论文介绍了一个精心策划的数据集，用于解决MHC-II抗原表位研究中的挑战，并提出了三个机器学习任务和多尺度评估框架，以促进计算免疫治疗领域的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管抗原表位在免疫治疗中扮演着重要角色，但与MHC-I相比，对MHC-II抗原表位的计算研究面临巨大挑战，包括其复杂的结合特异性和模糊的基序模式，导致现有MHC-II数据集规模较小且不够标准化。

Method: 本文构建了一个来自免疫表位数据库（IEDB）和其他公共来源的精心策划的数据集。该数据集不仅扩展并标准化了现有的肽-MHC-II数据集，还引入了一个具有更丰富生物学背景的新型抗原-MHC-II数据集。此外，基于该数据集，论文提出了肽结合、肽呈递和抗原呈递三个主要的机器学习任务，并采用多尺度评估框架对现有模型进行基准测试，同时对各种建模设计进行了全面分析，并设计了一个模块化框架。

Result: 通过该工作，研究人员能够更好地理解MHC-II抗原呈递途径中的生物学过程。该数据集和评估框架为未来机器学习指导的表位发现和免疫反应预测建模奠定了基础。

Conclusion: 该工作为计算免疫治疗提供了一个有价值的资源，为未来机器学习指导的表位发现和免疫反应预测建模领域的进一步研究奠定了基础。

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [65] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: EXAONE Path 2.5是一个病理学基础模型，它通过联合建模组织学、基因组学、表观遗传学和转录组学等多模态数据，生成整合的患者表征，更全面地反映肿瘤生物学。


<details>
  <summary>Details</summary>
Motivation: 癌症进展是多生物学层级相互作用的结果，尤其是在形态学之外和图像模型不可见的分子层级。为了捕捉更广泛的生物学图景，需要一个能整合多模态数据的模型。

Method: EXAONE Path 2.5采用三个关键组件：1. 多模态SigLIP损失，实现异构模态之间的全对对比学习。2. 片段感知旋转位置编码（F-RoPE）模块，保留WSI中的空间结构和组织片段拓扑。3. 用于WSI和RNA-seq的领域专用内部基础模型，提供生物学基础的嵌入以实现稳健的多模态对齐。

Result: EXAONE Path 2.5在内部真实世界临床数据集和涵盖80项任务的Patho-Bench基准测试中，与六个领先的病理学基础模型进行了评估。该框架显示出高数据和参数效率，在Patho-Bench上与最先进的基础模型表现相当，并在内部临床环境中展现出最高的适应性。

Conclusion: 生物学信息多模态设计的价值以及整合基因型到表型建模在下一代精准肿瘤学中的潜力。

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [66] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: FusAD是一个统一的时间序列分析框架，通过自适应时频融合和去噪机制，以及信息融合和解码结构，在多任务时间序列基准测试中超越了现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在时间序列分析中取得了显著进展，但构建一个高效、多任务兼容和可推广的统一时间序列分析框架仍然是一个巨大挑战。现有方法通常针对单一任务或特定数据类型，难以同时处理多任务建模并有效整合不同时间序列类型的信息。真实世界数据中的噪声、复杂频率成分和多尺度动态模式进一步增加了鲁棒特征提取的难度。

Method: 本文提出了FusAD，一个统一的多元时间序列任务分析框架。该框架具有自适应时频融合机制，结合傅里叶和小波变换来有效捕获全局-局部和多尺度动态特征。FusAD还包含自适应去噪机制，能自动感知和过滤各种类型的噪声。此外，该框架集成了通用的信息融合和解码结构，并结合了掩码预训练，以促进多粒度表示的高效学习和转移。

Result: 广泛的实验表明，FusAD在主流时间序列分类、预测和异常检测任务的基准测试中，始终优于现有最先进的模型，同时保持了高效性和可伸缩性。

Conclusion: FusAD通过其独特的时间-频率融合和自适应去噪机制，以及强大的信息融合和解码结构，成功解决了现有时间序列分析方法的局限性，为多任务时间序列分析提供了一个高效且通用的解决方案。

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [67] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: 本文提出了一种内存高效的MoE算法SonicMoE，通过减少激活内存、优化GPU内核和引入“token rounding”方法，显着提升了MoE模型的训练吞吐量和硬件效率。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型在提高专家粒度和稀疏度的过程中，面临激活内存占用高、硬件效率低以及计算浪费等问题，这促使研究者寻找更高效的MoE训练方法。

Method: SonicMoE提出了一种内存高效的MoE前向和后向计算算法，它通过最小化激活缓存来优化内存。同时，设计了能够重叠内存I/O和计算的GPU内核，并引入了一种新颖的“token rounding”方法，以减少Grouped GEMM内核中由于填充导致的计算浪费。

Result: SonicMoE相较于ScatterMoE的BF16 MoE内核，在细粒度7B MoE模型上减少了45%的激活内存，并实现了1.86倍的计算吞吐量提升。在64块H100s GPU上，SonicMoE的训练吞吐量达到每天2130亿token，与ScatterMoE在96块H100s上的2250亿token/天相当。在高MoE稀疏度设置下，其tile-aware token rounding算法在保持性能的同时，将内核执行时间额外加速了1.16倍。

Conclusion: SonicMoE通过其创新的内存优化算法、高效的GPU内核设计和独特的token rounding方法，有效解决了现有MoE模型在内存和计算效率上的挑战，显著提高了大型语言模型MoE架构的训练吞吐量和硬件利用率。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [68] [Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization](https://arxiv.org/abs/2512.14086)
*Boyuan Yao,Dingcheng Luo,Lianghao Cao,Nikola Kovachki,Thomas O'Leary-Roseberry,Omar Ghattas*

Main category: cs.LG

TL;DR: 本文介绍了导数信息傅里叶神经算子（DIFNOs）的近似理论和高效训练方法，并将其应用于偏微分方程约束优化。


<details>
  <summary>Details</summary>
Motivation: DIFNOs能同时模拟高保真算子的响应及其敏感性。准确的替代模型驱动的偏微分方程约束优化需要精确的替代模型Fréchet导数。

Method: 本文建立了FNOs及其Fréchet导数在紧集上的同时通用逼近，以及FNOs在具有无界支持的输入测度的加权Sobolev空间中的通用逼近。开发了使用降维和多分辨率技术的高效训练方案。

Result: 数值示例表明，DIFNOs在算子学习和求解无限维偏微分方程约束逆问题方面具有优越的样本复杂度，在低训练样本量下也能实现高精度。

Conclusion: 理论结果证明了FNOs在导数信息算子学习和求解偏微分方程约束优化方面的能力，并且DIFNOs在各种方程中表现出色。

Abstract: We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fréchet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fréchet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fréchet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.

</details>


### [69] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Cornserve是一个用于Any-to-Any多模态模型的在线服务系统，它解决了模型服务中的异构性问题，并显著提高了吞吐量和降低了尾延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的模型服务系统难以有效支持新兴的Any-to-Any多模态模型，因为这类模型在请求类型、计算路径和计算扩展方面存在固有的异构性。

Method: Cornserve允许模型开发者描述Any-to-Any模型的计算图，该计算图包含多模态编码器、自回归模型（如LLMs）和多模态生成器（如DiTs）等异构组件。系统通过规划器自动找到优化的部署方案，包括是否以及如何将模型分解为更小的组件。Cornserve的分布式运行时根据此方案高效执行模型，处理在线服务期间的Any-to-Any模型异构性。

Result: 评估表明，Cornserve可以高效服务各种Any-to-Any模型和工作负载，与现有解决方案相比，吞吐量提高了3.81倍，尾延迟降低了5.79倍。

Conclusion: Cornserve通过其独特的计算图描述、自动优化部署规划和高效的分布式运行时，成功解决了Any-to-Any多模态模型在线服务中的异构性挑战，并在性能上取得了显著提升。

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [70] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为 S-GRPO 的逻辑相似度奖励机制，用于解决大型语言模型在与人类价值观对齐时，传统奖励模型质量和稳定性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习（RLHF）在对齐大型语言模型（LLMs）与人类偏好时，严重依赖奖励模型，但奖励模型的质量和稳定性往往不尽如人意。

Method: 本研究提出了一种基于逻辑相似度的奖励机制，替代传统的启发式奖励估计。为了避免模型在多视角理解真实世界问题时发生坍塌，引入了 S-GRPO，这是 GRPO 框架的一个监督变体。S-GRPO 在训练过程中结合了额外的监督成分，并同时优化生成项、KL 散度正则化和基于标签的目标。

Result: 实验结果表明，S-GRPO 在性能和鲁棒性方面均优于标准监督微调（SFT）。此外，S-GRPO 扩展了现有的偏好学习框架（如 GRPO 和 DPO），提供了一种更灵活、更适应任务的对齐训练方法。

Conclusion: S-GRPO 通过引入逻辑相似度奖励机制和监督学习组件，有效提升了大型语言模型与人类偏好的对齐效果，并展现出优越的性能和鲁棒性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [71] [PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario](https://arxiv.org/abs/2512.14150)
*Zhijie Zhong,Zhiwen Yu,Pengyu Li,Jianming Lv,C. L. Philip Chen,Min Chen*

Main category: cs.LG

TL;DR: PathFinder通过解耦特征编码和Mask-Guided低秩注意力主动建模建筑物和发射器，并通过Transmitter-Oriented Mixup策略进行鲁棒训练，解决了当前无线路径损耗预测方法在环境建模、多发射器场景和分布偏移泛化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 目前的无线路径损耗预测（RPP）方法存在三个主要问题：1）被动环境建模，忽略了发射器和关键环境特征；2）过分强调单发射器场景，而现实世界中多发射器场景普遍存在；3）过度关注同分布性能，忽略了分布偏移的挑战。

Method: 本文提出了PathFinder架构来解决这些问题。PathFinder通过解耦特征编码主动建模建筑物和发射器，并集成Mask-Guided Low-rank Attention以独立关注接收器和建筑物区域。此外，还引入了Transmitter-Oriented Mixup策略进行鲁棒训练，并提出了一个新的基准S2MT-RPP来评估外推性能。

Result: 实验结果表明，PathFinder在具有挑战性的多发射器场景中显著优于最先进的方法。

Conclusion: PathFinder通过其新颖的架构和训练策略，有效地解决了无线路径损耗预测在复杂环境和多发射器场景下的泛化能力和准确性问题。

Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.

</details>


### [72] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 这篇论文介绍了随机桥在生成建模领域的应用，它可以作为两种概率分布之间的随机传输，并能生成高质量的样本。


<details>
  <summary>Details</summary>
Motivation: 传统的生成模型在采样效率和质量方面存在局限性，作者旨在提出一种更高效、高质量的生成模型方法。

Method: 本文提出使用随机桥（random-bridges）作为生成模型。随机桥是一种在固定时间点上条件化以获取目标分布的随机过程。它可以根据驱动过程显示马尔可夫或非马尔可夫、连续、不连续或混合模式。作者展示了如何从一般的概率陈述出发，然后分支到用于学习和模拟算法的特定表示，从而在信息处理方面实现。

Result: 基于高斯随机桥的实验结果表明，与传统方法相比，该方法能够在显著更少的步骤中生成高质量样本，并取得了具有竞争力的Frechet inception距离分数。

Conclusion: 本文提出的随机桥框架计算成本低，适用于高速生成任务。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [73] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: 该文提出了一种名为“LLM比较法”的新问题难度估计方法，通过LLM进行成对比较，然后计算Bradley-Terry得分来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的问题难度估计方法（如人工校准或基于性能的评分）无法推广到分布外问题，因为它们不可扩展、耗时且依赖于基本事实。

Method: 提出了一种新的问题难度估计方法——LLM比较法。该方法通过LLM进行成对难度比较，然后根据比较结果计算Bradley-Terry得分。为了验证该方法，作者提出了一个概念框架来定位现有方法，并证明LLM比较法作为第一个连续、动态、模型无关且独立于基本事实信息的度量方法，占据了所有理想的象限。

Result: LLM比较法与人类标注显示出很强的一致性（Pearson r ≥ 0.80，n=1876）。此外，LLM比较法对幻觉具有鲁棒性，即使注入10%的噪声，Pearson相关性也仅下降不到6%。

Conclusion: LLM比较法为替代耗时的人工标注和合成数据生成迈出了重要一步，并将成为课程设计、模型评估和AI辅助研究构思的重要推动力。

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [74] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 为了在使用数据学习反应扩散 (RD) 系统时确保物理一致性和模型的适定性，本文提出了两种技术，用于系统地修改参数化反应项，使其固有地满足质量守恒和准正性。这确保了学习到的RD系统在额外的规律性和增长条件下保持非负性、符合物理原理并保证了所得偏微分方程的适定性。研究结果还表明，即使在强制执行守恒定律和准正性的情况下，学习问题的解也会收敛到极限系统的一个唯一的正则化最小解。研究也为准正函数提供了近似结果，这对于构建物理上一致的参数化至关重要。


<details>
  <summary>Details</summary>
Motivation: 在从数据中学习反应扩散（RD）系统时，现有的方法往往难以保证学习到的模型的物理一致性和适定性。这意味着学习到的模型可能不符合基本的物理定律，或者在数学上存在不适定性，导致结果不可靠或不具备解释性。因此，需要一种能够将物理约束直接融入学习过程的框架，以确保模型的准确性和可靠性。

Method: 本文提出了一种基于正则化的结构化模型学习框架，其核心方法包括两个方面：1. **参数化反应项的系统性修改**：针对给定的参数化反应项，本文提出系统修改技术以使其固有地满足质量守恒和准正性。通过这种修改，确保了学习到的RD系统能够保持非负性并符合物理原理。同时，这些修改在额外的正则性和增长条件下，也能保证所得偏微分方程的适定性。2. **正则化学习理论的扩展**：本文将现有关于正则化模型学习的理论结果扩展到使用这些物理一致性反应项的RD系统。通过理论证明，即使在强制执行守恒定律和准正性的情况下，学习问题的解也会收敛到极限系统的一个唯一的、正则化最小的解。此外，本文还提供了准正函数的近似结果，这对于构建物理一致性参数化至关重要。

Result: 本文的研究成果主要包括两点：1. **确保了学习到的RD系统的物理一致性和适定性**：通过系统地修改参数化反应项，使其满足质量守恒和准正性，从而保证了学习到的RD系统在额外的正则性和增长条件下保持非负性、符合物理原理并保证了所得偏微分方程的适定性。2. **扩展了正则化学习的理论框架**：研究结果表明，即使在强制执行守恒定律和准正性的情况下，学习问题的解也会收敛到极限系统的一个唯一的正则化最小解。此外，本文还为准正函数提供了近似结果，这对于构建物理一致性参数化至关重要。

Conclusion: 本文通过提出确保物理一致性和模型适定性的策略，显著推进了从数据中学习反应扩散（RD）系统的研究。研究提出的方法，通过系统修改参数化反应项以满足质量守恒和准正性，以及扩展正则化学习的理论框架，为构建可解释且可靠的数据驱动RD系统模型奠定了基础。这些进展不仅有助于RD系统模型的准确性和可靠性，也促进了数据驱动方法与基本物理定律的融合。

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [75] [Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning](https://arxiv.org/abs/2512.14241)
*Salvatore Romano,Marco Grassia,Giuseppe Mangioni*

Main category: cs.LG

TL;DR: 介绍了新的GGM评估方法RGM，并用其评估了GRAN和EDGE模型，发现现有GGM在保留结构特征方面存在局限性，并认为MMD不适用于GGM评估。


<details>
  <summary>Details</summary>
Motivation: 图生成是许多领域的关键任务，图生成模型（GGM）是很有前景的解决方案，但目前的评估方法（尤其是MMD）存在局限性。

Method: 提出了名为RGM（Representation-aware Graph-generation Model evaluation）的GGM评估新方法，并使用为图分类任务定制的综合数据集上训练的几何深度学习模型，对GRAN和EDGE两种SOTA的GGM进行了评估。

Result: 评估结果表明，GRAN和EDGE模型虽然可以生成具有某些拓扑属性的图，但在保留区分不同图域的结构特征方面存在显著局限性。此外，研究还强调了最大均值差异（MMD）作为GGM评估指标的不足。

Conclusion: 现有的GGM在保留图的结构特征方面存在局限性，MMD不适用于GGM的评估，未来研究需要探索替代的评估方法。

Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.

</details>


### [76] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: FLAME是一个轻量级且功能强大的时间序列基础模型家族，通过生成概率建模支持确定性和概率预测，旨在确保效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在时间序列预测中，需要一个既高效又鲁棒的模型，能够支持确定性和概率预测。

Method: FLAME模型利用勒让德记忆（Legendre Memory）及其变体（LegT和LegS）来捕捉数据中的归纳偏差，实现高效的长期推断。它还采用基于归一化流（Normalization Flow）的预测头，以生成方式模拟复杂的预测分布，提高概率预测的准确性。

Result: FLAME在TSFM-Bench和ProbTS等时间序列基准测试中，在确定性和概率预测任务上均展现出持续的最先进零样本性能。

Conclusion: FLAME通过结合勒让德记忆和归一化流，成功地提供了一个高效、鲁棒且准确的时间序列基础模型，在多种预测任务中表现卓越。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [77] [Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization](https://arxiv.org/abs/2512.14263)
*Nick Leenders,Thomas Quadt,Boris Cule,Roy Lindelauf,Herman Monsuur,Joost van Oijen,Mark Voskuijl*

Main category: cs.LG

TL;DR: 该论文介绍了一种基于决策树的替代模型，用于解决当前 Preferential Bayesian Optimization 方法中高斯过程模型的解释性差、难以处理分类数据和计算复杂性高等问题。


<details>
  <summary>Details</summary>
Motivation: 当前的 Preferential Bayesian Optimization 方法依赖高斯过程（GPs）作为替代模型，这些模型难以解释，难以处理分类数据，并且计算复杂，限制了它们在实际应用中的可用性。

Method: 本文引入了一种本质上可解释的基于决策树的替代模型，该模型能够处理分类和连续数据，并可扩展到大型数据集。

Result: 在八个复杂度递增的优化函数上的大量数值实验表明，该模型在处理尖峰函数时优于基于高斯过程的替代方案，而在处理非尖峰函数时性能仅略低。此外，将该模型应用于真实的寿司数据集，展示了其学习个体寿司偏好的能力。

Conclusion: 本文展示了利用历史偏好数据加速新用户优化过程的初步工作。

Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.

</details>


### [78] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 该论文提出了首个用于量子机器学习的黑盒隐私审计框架，利用量子金丝雀量化隐私泄漏。


<details>
  <summary>Details</summary>
Motivation: 在量子机器学习（QML）中，模型训练面临敏感数据被记忆的隐私风险，而现有的量子差分隐私（QDP）机制缺乏实证验证工具。

Method: 本文引入了基于“提升量子差分隐私”的黑盒隐私审计框架，该框架利用量子金丝雀（策略性偏移编码的量子态）来检测记忆并精确量化训练过程中的隐私泄漏。该框架在金丝雀偏移和迹距离界限之间建立了严格的数学联系，并推导出了隐私预算消耗的经验下限。

Result: 在模拟和物理量子硬件上的综合评估表明，该框架能有效测量量子机器学习模型中的实际隐私损失。

Conclusion: 该研究通过提供一个严谨的经验验证工具，弥补了量子差分隐私理论保证与实际隐私验证之间的关键空白，从而实现了QML系统中的鲁棒隐私验证。

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [79] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: 这篇论文提出了GRAFT模型，它通过文本引导的交叉注意力机制，将多源文本数据（新闻、社交媒体、政策文本）与半小时电力负荷数据对齐融合，以实现更好的电网感知负荷预测。模型在统一基准下进行了系统评估，并显著优于现有基线，在不同区域和预测范围内达到了或超越了最先进水平。该研究还发布了基准数据集和工具，以促进电力负荷预测领域的标准化评估和可复现性。


<details>
  <summary>Details</summary>
Motivation: 电力负荷受到多种外生因素（如天气、日历周期、突发事件和政策）在不同时间尺度上的影响。本研究旨在开发一个能够更好地支持电网感知预测和利用多源文本干预的负荷预测模型。

Method: 本研究提出了GRAFT（GRid-Aware Forecasting with Text）模型，该模型改进了STanHOP。具体方法包括：1. 严格将每日聚合的新闻、社交媒体和政策文本与半小时负荷数据对齐。2. 通过交叉注意力机制在训练和滚动预测过程中实现文本引导的特定时间点融合。3. 提供一个即插即用的外部存储接口以适应不同的信息源。4. 构建并发布了一个覆盖2019-2021年五个澳大利亚州的统一对齐基准数据集，包括半小时负荷、每日对齐的天气/日历变量和三类外部文本。5. 在小时、日和月三个尺度下进行系统、可复现的评估，并采用统一协议进行跨区域、外部来源和时间尺度的比较。

Result: GRAFT模型显著优于现有基线，并在多个区域和预测范围内达到了或超越了最先进水平。该模型在事件驱动场景中表现出鲁棒性，并且能够通过注意力解读实现文本到负荷效应的时间定位和源级别解释。

Conclusion: 本研究提出了一个名为GRAFT的先进模型，能够有效融合多源文本信息进行电力负荷预测，并在现有方法的基础上取得了显著提升。通过发布统一基准和相关工具，该研究为电力负荷预测领域的未来研究提供了宝贵资源，促进了该领域的标准化评估和可复现性。

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [80] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: ClimaX-LETKF是第一个纯数据驱动的机器学习集合天气预报系统，它通过同化观测数据，实现了独立于数值天气预报模型且多年稳定的运行。


<details>
  <summary>Details</summary>
Motivation: 尽管基于机器学习的天气预报（MLWP）取得了显著进展，但将真实观测或集合预报融合到MLWP模型中的研究仍然有限。

Method: 本文引入了ClimaX-LETKF，这是第一个纯数据驱动的机器学习集合天气预报系统，它通过同化NCEP ADP全球高空和地面天气观测数据，实现了独立于数值天气预报模型（NWP）的多年稳定运行。研究比较了两种扰动松弛方案（RTPP和RTPS）对系统稳定性和准确性的影响。

Result: ClimaX-LETKF系统在使用RTPP时表现出比RTPS更高的稳定性和准确性，而NWP模型则倾向于在RTPS下更稳定。研究发现MLWP模型在恢复大气场到其吸引子方面的能力不如NWP模型。

Conclusion: 这项工作为增强MLWP集合预报系统提供了宝贵的见解，并代表着向其实际应用迈出了重要一步。

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [81] [Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics](https://arxiv.org/abs/2512.14471)
*Additi Pandey,Liang Wei,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: 本文介绍了Kinetic-Mamba，一个基于Mamba的神经算子框架，用于化学动力学建模。


<details>
  <summary>Details</summary>
Motivation: 准确的化学动力学建模对于燃烧模拟至关重要，因为它控制着复杂反应路径和热化学状态的演变。

Method: Kinetic-Mamba框架包括三个互补模型：(i) 独立Mamba模型，根据给定初始条件预测热化学状态变量的时间演变；(ii) 约束Mamba模型，在学习状态动力学的同时强制执行质量守恒；(iii) 区域信息架构，采用两个独立Mamba模型来捕获跨温度依赖性区域的动力学。我们还开发了一种潜在Kinetic-Mamba变体，在简化的潜在空间中演化动力学，并在物理流形上重建完整状态。

Result: 在Syngas和GRI-Mech 3.0反应机制上的计算实验表明，我们的框架在使用状态变量的初始条件预测复杂动力学行为方面实现了高保真度。

Conclusion: Kinetic-Mamba能够以高精度预测复杂的化学动力学行为，并且具有良好的外推能力。

Abstract: Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.

</details>


### [82] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: 该研究探讨了使用变分自编码器（VAEs）生成合成心房电图（EGMs）来解决心电图成像（ECGI）中数据稀缺的问题，并提出了两种模型（VAE-S和VAE-C），结果表明这些生成的EGMs可以用于数据增强以提高非侵入性EGM重建的性能。


<details>
  <summary>Details</summary>
Motivation: 心房颤动（AF）的临床评估需要准确表征心房电活动，而非侵入性心电图成像（ECGI）结合深度学习（DL）在从体表电位（BSPMs）估计心内电图（EGMs）方面显示出潜力，但受限于成对BSPM-EGM数据集的可用性。

Method: 研究者提出了两种变分自编码器（VAEs）模型来生成合成多通道心房EGMs：一种是针对窦性心律的VAE（VAE-S），另一种是针对窦性心律和心房颤动信号进行训练的类别条件VAE（VAE-C）。生成的EGMs通过形态、频谱和分布相似性指标进行评估。

Result: VAE-S在与计算机模拟的EGMs相比时，实现了更高的保真度，而VAE-C则实现了特定节律的生成，但代价是窦性重建质量有所降低。生成的EGMs被用于下游非侵入性EGM重建任务中的数据增强，适度的增强改善了估计性能。

Conclusion: 基于VAE的生成模型有望缓解数据稀缺问题，并增强基于深度学习的ECGI流程。

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [83] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 该文章批判了现有时间序列分类反事实解释方法的局限性，并呼吁开发更符合临床实际、具备时序连贯性和因果合理性的反事实解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列分类反事实解释方法主要关注最小输入扰动，且建立在静态数据假设之上，这在临床推荐设置中是远远不够的。临床干预是动态的、长期的，需要因果合理性和时序连贯性。

Method: 通过对现有多种时间序列方法的鲁棒性分析，展示了现有方法生成的反事实解释对随机噪声高度敏感。

Result: 现有时间序列反事实解释方法生成的反事实解释对随机噪声高度敏感，在真实临床环境中可靠性有限。这些方法在时间上存在盲点，且缺乏以用户为中心的考虑。

Conclusion: 需要开发超越单纯预测变化的、可行的、以行动为导向的反事实解释方法和评估框架，以更好地支持临床决策和用户实际需求。

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [84] [Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection](https://arxiv.org/abs/2512.14563)
*Tejaswani Dash,Gautam Datla,Anudeep Vurity,Tazeem Ahmad,Mohd Adnan,Saima Rafi,Saisha Patro,Saina Patro*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为Residual GRU with Multi-Head Self-Attention的深度学习模型，用于预测心血管疾病。该模型在UCI心脏病数据集上取得了优于传统方法和现代深度学习基线的性能，并在资源受限的医疗环境中具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病仍然是全球主要的死亡原因，因此需要可靠且高效的预测工具来支持早期干预。传统的诊断方法依赖于手工特征和临床医生经验，而机器学习方法虽然提高了可重复性，但难以在嘈杂和异构的临床数据中泛化。

Method: 本文提出了一种紧凑的深度学习架构，名为Residual GRU with Multi-Head Self-Attention，专为表格临床记录设计。该模型整合了残差双向门控循环单元（用于特征列的序列建模）、一个通道重新加权块以及带有可学习分类token的多头自注意力池化（用于捕获全局上下文）。

Result: 所提出的模型在UCI心脏病数据集上实现了0.861的准确率、0.860的宏观F1分数、0.908的ROC-AUC和0.904的PR-AUC，优于所有基线模型。消融研究证实了残差循环、通道门控和注意力池化的各自贡献。t-SNE可视化进一步表明，与原始特征相比，学习到的嵌入在疾病和非疾病类别之间表现出更清晰的分离。

Conclusion: 轻量级混合循环和基于注意力的架构在临床风险预测的准确性和效率之间取得了良好的平衡，支持在资源受限的医疗环境中部署。

Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

</details>


### [85] [ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning](https://arxiv.org/abs/2512.14619)
*Chaohao Yuan,Zhenjie Song,Ercan Engin Kuruoglu,Kangfei Zhao,Yang Liu,Deli Zhao,Hong Cheng,Yu Rong*

Main category: cs.LG

TL;DR: PageRank Transformer（ParaFormer）通过PageRank增强的注意力模块解决了图Transformer（GTs）中的过平滑问题，在节点和图分类任务上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 图Transformer（GTs）在捕获全局信息方面表现出色，但其全局注意力机制存在严重的过平滑问题，导致节点表示难以区分。

Method: 提出PageRank Transformer（ParaFormer），它包含一个PageRank增强的注意力模块，旨在模拟深度Transformer的行为。ParaFormer被设计为自适应滤波器，以减轻过平滑问题。

Result: ParaFormer在节点分类和图分类任务上都实现了持续的性能改进，并在11个不同规模的数据集上验证了其有效性。

Conclusion: 通过引入PageRank增强的注意力机制，ParaFormer成功缓解了图Transformer中的过平滑问题，并作为一个自适应滤波器提升了模型性能。

Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

</details>


### [86] [gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation](https://arxiv.org/abs/2512.14658)
*Alban Puech,Matteo Mazzonelli,Celia Cintas,Tamara R. Govindasamy,Mangaliso Mngomezulu,Jonas Weiss,Matteo Baù,Anna Varbella,François Mirallès,Kibaek Kim,Le Xie,Hendrik F. Hamann,Etienne Vos,Thomas Brunschwiler*

Main category: cs.LG

TL;DR: gridfm-datakit-v1 是一个 Python 库，用于为机器学习求解器生成逼真且多样化的潮流 (PF) 和最优潮流 (OPF) 数据集，解决了现有数据集中场景多样性不足、数据点限制以及发电机成本函数固定等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的电力系统数据集和库存在三个主要挑战：1. 缺乏真实的随机负荷和拓扑扰动，限制了场景多样性；2. 潮流数据集仅限于 OPF 可行点，阻碍了机器学习求解器在违反运行限制情况下的泛化能力；3. 最优潮流数据集使用固定的发电机成本函数，限制了在不同成本下的泛化。

Method: gridfm-datakit 通过以下方法解决这些挑战：1. 结合来自真实世界配置文件的全局负荷缩放与局部噪声，并支持任意 N-k 拓扑扰动，以创建多样化且真实的数据集；2. 生成超出运行限制的潮流样本；3. 生成具有不同发电机成本的最优潮流数据。它还可以高效扩展到大型电网（最多 10,000 个母线）。

Result: gridfm-datakit-v1 能够生成逼真且多样化的电力系统数据集，解决现有数据集的局限性，并具有高效扩展大型电网的能力。

Conclusion: gridfm-datakit-v1 为训练机器学习求解器提供了重要的数据集生成工具，能够显著提高模型在复杂电力系统场景下的泛化能力和鲁棒性。

Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.

</details>


### [87] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: 医院缺乏自动化系统来处理异构数据并预测关键事件。我们开发了一个多模式机器学习框架EWI，通过结合临床医生经验和可解释的AI，对ICU入院、紧急响应和死亡率进行聚合风险预测。EWI已部署在医院，并通过对18,633名患者数据的验证，达到了0.796的C-统计量。它能够根据风险水平自动对患者进行分类，帮助医生优先处理高风险患者，找出特定风险驱动因素，并指导资源分配，从而提高患者护理质量和效率。


<details>
  <summary>Details</summary>
Motivation: 目前医院缺乏自动化系统来有效预测关键事件，导致难以在早期识别出有恶化风险的患者。尽管临床和操作数据量不断增长，但异构数据格式给准确和可解释的风险评估带来了挑战。

Method: 我们开发了一个多模式机器学习框架——早期预警指数（EWI）来预测ICU入院、紧急响应团队调度和死亡率的聚合风险。该框架的关键在于“人机协作”：临床医生参与确定预警阈值并解释模型输出，同时使用Shapley Additive exPlanations（SHAP）增强模型解释性，以突出驱动患者风险的临床和操作因素。EWI能够从结构化和非结构化电子健康记录（EHR）数据中自动提取特征。

Result: EWI框架在一个包含18,633名独特患者数据集的美国大型医院进行了部署，并取得了0.796的C-统计量。它被用作对高风险患者进行主动管理的分类工具。

Conclusion: EWI系统通过自动对不同风险水平的患者进行分类，为医生节省了宝贵时间，使他们能够专注于患者护理，而不是在复杂的EHR数据中筛选。通过指出具体的风险驱动因素，该模型为医护人员排班和关键资源分配提供了数据支持的调整方案。这有助于临床医生和管理人员避免下游并发症，提高整体患者流量。

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [88] [Complete weight enumerators and weight hierarchies for linear codes from quadratic forms](https://arxiv.org/abs/2512.14073)
*Xiumei Li,Xiaotong Sun,Min Sha*

Main category: cs.IT

TL;DR: 本文提出并研究了两类至多四非零权重的线性分组码，并确定了其完整的重量枚举和重量等级。


<details>
  <summary>Details</summary>
Motivation: 推广了Xie等人提出的码的构造，引入了两类基于二次型的线性分组码。

Method: 通过使用指数和的方法，推导了所提出的线性码的完整重量枚举和重量等级。

Result: 构造的多数码是极小的，部分码是最优的，达到了Griesmer界。同时，确定了下降码的重量等级。

Conclusion: 本文成功地构建了两类具有特定重量性质的线性码，并对其关键编码理论参数进行了全面分析。

Abstract: In this paper, for an odd prime power $q$, we extend the construction of Xie et al. \cite{XOYM2023} to propose two classes of linear codes $\mathcal{C}_{Q}$ and $\mathcal{C}_{Q}'$ over the finite field $\mathbb{F}_{q}$ with at most four nonzero weights. These codes are derived from quadratic forms through a bivariate construction. We completely determine their complete weight enumerators and weight hierarchies by employing exponential sums. Most of these codes are minimal and some are optimal in the sense that they meet the Griesmer bound. Furthermore, we also establish the weight hierarchies of $\mathcal{C}_{Q,N}$ and $\mathcal{C}_{Q,N}'$, which are the descended codes of $\mathcal{C}_{Q}$ and $\mathcal{C}_{Q}'$.

</details>


### [89] [Agile Affine Frequency Division Multiplexing](https://arxiv.org/abs/2512.14424)
*Yewen Cao,Yulin Shao*

Main category: cs.IT

TL;DR: Agile-AFDM是一种新型的AFDM框架。


<details>
  <summary>Details</summary>
Motivation: 目前用于6G通信的AFDM波形虽然在双色散信道中表现出色，但其线性调频参数优化通常针对最坏情况，缺乏灵活性。

Method: Agile-AFDM将线性调频参数重新定义为可优化的变量，并根据实时信道和数据信息为每个传输块进行优化，从而实现了波形的自适应重构。此外，还提出了为Agile-AFDM量身定制的高效优化算法。

Result: Agile-AFDM可以在降低PAPR、抑制ICI和减少CRLB方面取得显著的性能增益，超越了传统的OFDM和静态AFDM。

Conclusion: Agile-AFDM是为6G及未来通信设计敏捷波形的关键一步。

Abstract: The advancement to 6G calls for waveforms that transcend static robustness to achieve intelligent adaptability. Affine Frequency Division Multiplexing (AFDM), despite its strength in doubly-dispersive channels, has been confined by chirp parameters optimized for worst-case scenarios. This paper shatters this limitation with Agile-AFDM, a novel framework that endows AFDM with dynamic, data-aware intelligence. By redefining chirp parameters as optimizable variables for each transmission block based on real-time channel and data information, Agile-AFDM transforms into an adaptive platform. It can actively reconfigure its waveform to minimize peak-to-average power ratio (PAPR) for power efficiency, suppress inter-carrier interference (ICI) for communication reliability, or reduce Cramer-Rao bound (CRLB) for sensing accuracy. This paradigm shift from a static, one-size-fits-all waveform to a context-aware signal designer is made practical by efficient, tailored optimization algorithms. Comprehensive simulations demonstrate that this capability delivers significant performance gains across all metrics, surpassing conventional OFDM and static AFDM. Agile-AFDM, therefore, offers a crucial step forward in the design of agile waveforms for 6G and beyond.

</details>


### [90] [The Performance of Compression-Based Denoisers](https://arxiv.org/abs/2512.14539)
*Dan Song,Ayfer Özgür,Tsachy Weissman*

Main category: cs.IT

TL;DR: 本文提出了一种框架，用于通过有损压缩来重构通过无记忆噪声信道观测到的平稳遍历源，并将该框架扩展到了一般的离散无记忆信道。


<details>
  <summary>Details</summary>
Motivation: 以前关于基于压缩的去噪工作仅限于加性噪声信道。

Method: 通过选择用于有损压缩的失真度量来匹配信道条件分布，从而将该框架扩展到了一般的离散无记忆信道。通过限制源、观测和去噪器输出的经验联合分布偏离满足马尔可夫性质，我们精确地刻画了这种去噪器所实现的损耗。

Result: 这些结果的推论在特殊情况下得到了明确的证明，包括MSE和汉明损耗。

Conclusion: 将该方法与间接速率失真角度进行了比较。

Abstract: We consider a denoiser that reconstructs a stationary ergodic source by lossily compressing samples of the source observed through a memoryless noisy channel. Prior work on compression-based denoising has been limited to additive noise channels. We extend this framework to general discrete memoryless channels by deliberately choosing the distortion measure for the lossy compressor to match the channel conditional distribution. By bounding the deviation of the empirical joint distribution of the source, observation, and denoiser outputs from satisfying a Markov property, we give an exact characterization of the loss achieved by such a denoiser. Consequences of these results are explicitly demonstrated in special cases, including for MSE and Hamming loss. A comparison is made to an indirect rate-distortion perspective on the problem.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [91] [Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records](https://arxiv.org/abs/2512.13700)
*Mitchell A. Klusty,Elizabeth C. Solie,Caroline N. Leach,W. Vaiden Logan,Lynnet E. Richey,John C. Gensel,David P. Szczykutowicz,Bryan C. McLellan,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 该框架利用本地部署的大型语言模型，从临床笔记中自动提取结构化特征，从而减少手动图表审查的工作量并提高数据捕获的一致性，从而加速临床研究。


<details>
  <summary>Details</summary>
Motivation: 临床研究中，人工图表审查耗时且资源密集，需要专家从非结构化的电子健康记录中提取复杂信息。

Method: 本文提出了一个安全、模块化的框架，用于从临床笔记中自动提取结构化特征。该框架利用本地部署的大型语言模型，并结合检索增强生成（RAG）和结构化响应方法，以HIPPA兼容的计算基础设施提供特征提取。

Result: 在评估中，该框架在与专家注释数据集比较时，在大量患者笔记中的多个医学特征上实现了高精度，并识别出人工审查中遗漏的几个注释错误。

Conclusion: 该框架展示了大型语言模型系统通过自动化提取来减轻人工图表审查负担的潜力，并提高了数据捕获的一致性，从而加速了临床研究。

Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.

</details>


### [92] [Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference](https://arxiv.org/abs/2512.13701)
*Zheng Xing,Junting Chen*

Main category: cs.AI

TL;DR: 本文提出了一种盲无线电地图构建框架，该框架能够在没有位置标签的情况下，从室内MIMO-OFDM信道测量中推断用户轨迹，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的无线电地图构建方法需要大量的带有位置标签的数据，这在实际应用中成本高昂且不切实际。

Method: 本文首先证明了非视距（NLOS）下的信道状态信息（CSI）在准镜面环境模型下表现出空间连续性，并推导出了与物理距离成比例的CSI-距离度量。然后，针对泊松分布的AP部署中的直线轨迹，证明了即使在较差的角度分辨率下，定位误差的Cramer-Rao下界（CRLB）也渐近消失。在此理论基础上，开发了一个空间正则化的贝叶斯推理框架，该框架联合估计信道特征，区分视距（LOS）/非视距（NLOS）条件并恢复用户轨迹。

Result: 在射线追踪数据集上的实验表明，平均定位误差为0.68米，波束图重建误差为3.3%。

Conclusion: 本文提出的盲映射方法能够有效地进行无线电地图构建和用户轨迹恢复，降低了对位置标签的依赖。

Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.

</details>


### [93] [Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents](https://arxiv.org/abs/2512.13704)
*Doohee You,Sundeep Paul*

Main category: cs.AI

TL;DR: Adjudicator是一个神经符号系统，用于自动识别和校正生产机器学习系统中的标签噪声，通过构建动态知识图谱并利用多智能体大型语言模型架构来提高数据质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决生产机器学习系统中训练数据质量受限的问题，尤其是在高风险工业应用中，嘈杂标签会降低性能并侵蚀用户信任。

Method: Adjudicator将标签噪声识别和校正建模为神经符号任务。首先，它构建一个动态知识图谱（KG）以统一项目上下文。然后，该KG为“智能体委员会”提供信息，这是一个新颖的多智能体大型语言模型（LLM）架构，其中专业智能体对标签的有效性进行辩论和投票。

Result: 在AlleNoise基准的1,000项平衡子集上进行验证，我们的KG信息模型实现了0.99的F1分数，显著优于单一LLM基线（0.48 F1）和非KG委员会（0.59 F1）。分析表明，这归因于通过使用KG完美识别复杂结构错误（完整召回）的新颖覆盖逻辑所实现的精度，而基线未能发现这类错误。

Conclusion: Adjudicator 提供了一个强大且可解释的自动化、高精度数据验证系统，为在严格管理的工业环境中生成黄金数据集提供了重要的概念验证。

Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.

</details>


### [94] [AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach](https://arxiv.org/abs/2512.13714)
*Gangesh Pathak,Prasanna Kumar*

Main category: cs.AI

TL;DR: 该研究提出了一种AI辅助的标注流程，旨在系统性地识别、标记并修复大型语言模型（LLM）输出中的不稳定模式，从而提高LLM在强监管行业中的可靠性和一致性。


<details>
  <summary>Details</summary>
Motivation: LLM在强监管行业中因其不稳定性、推理不一致性、幻觉和性能可变性而未能得到有效应用，现有的稳定化方法（如RLHF和SFT）成本高昂且难以持续扩展，因此需要一种更有效的方法来解决这些可靠性问题。

Method: 本文提出了一种人机协同的AI标注流程，该流程结合了自动化弱监督和基于置信度的标注模型，并通过人工验证来确保反馈信息的可靠性和道德正确性。该框架引入了语义一致性、事实正确性和逻辑连贯性等稳定性特定标注类别，以实现模型的持续校准和鲁棒性增强。

Result: 虽然原文未明确给出具体的实验结果或量化改进，但该方法旨在通过发现和修复LLM输出中的不稳定模式，提高LLM的可靠性、事实准确性和行为一致性，使其在强监管行业中能够安全使用。

Conclusion: 该研究提出了一种创新的人机协同AI标注流程，为解决LLM在强监管行业中面临的稳定性挑战提供了可持续且可扩展的解决方案。通过系统的标注和反馈循环，该方法有望显著提升LLM的性能和可靠性。

Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).

</details>


### [95] [Grammar Search for Multi-Agent Systems](https://arxiv.org/abs/2512.14079)
*Mayank Singh,Vikas Yadav,Shiva Krishna Reddy Malay,Shravan Nayak,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Eduardo Blanco*

Main category: cs.AI

TL;DR: 这篇论文提出了一种结构化的多智能体系统自动搜索框架，该框架采用固定、可组合的组件集来探索代码空间，并在多个基准测试中优于现有的基于LLM的自由形式搜索方法，同时具有更高的成本效益和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统自动搜索方法通常依赖于基于大型语言模型（LLM）的自由形式搜索，这可能缺乏结构化和效率。

Method: 本研究提出了一种结构化的框架，通过固定、可组合的简单组件集来探索代码空间。

Result: 尽管在候选生成阶段缺乏LLM的生成灵活性，但该方法在数学和问答两大领域的五个基准测试中，有四个超越了现有方法。此外，该方法还提供了更具成本效益的搜索过程，并生成了逻辑更简单、模块化且可解释的多智能体系统。

Conclusion: 本研究提出的结构化多智能体系统自动搜索框架，通过采用固定、可组合的组件集，在性能、成本效益和可解释性方面均优于传统的基于LLM的自由形式搜索方法，为多智能体系统的自动化设计和优化提供了新的思路。

Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.

</details>


### [96] [ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making](https://arxiv.org/abs/2512.13716)
*Yitong Luo,Ziang Chen,Hou Hei Lam,Jiayu zhan,Junqi Wang,Zhenliang Zhang,Xue Feng*

Main category: cs.AI

TL;DR: 该文章提出了一种价值驱动的个性化决策方法ValuePilot，用于提升AI在人机交互中与用户个体价值偏好对齐的能力。该方法通过数据集生成工具DGT和决策模块DMM，实现了在未见场景下超越LLM基线的决策对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统在现实应用中，除了任务完成和集体对齐，还需要解决如何适应个性化价值的挑战，以更好地实现人机交互中的个性化决策。

Method: 文章提出了ValuePilot框架，包含两部分：1. 数据集生成工具（DGT）：通过人类与大型语言模型（LLM）的协作，构建多样化的、带有价值标注的场景数据集。2. 决策模块（DMM）：学习根据个人价值偏好评估行动，从而实现情境敏感的个性化决策。

Result: 在未曾见过的场景中，DMM在与人类行动选择对齐方面，超越了包括GPT-5、Claude-Sonnet-4、Gemini-2-flash和Llama-3.1-70b在内的强大LLM基线模型。

Conclusion: 价值驱动的决策是一种有效且可扩展的工程途径，用于构建可解释的、个性化的AI智能体。

Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.

</details>


### [97] [Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy](https://arxiv.org/abs/2512.13725)
*Steve Nwaiwu,Nipat Jongsawat,Anucha Tungkasthan*

Main category: cs.AI

TL;DR: 首次系统评估了大语言模型因果推理在量化（INT8、NF4）后的性能影响，发现在Pearl因果阶梯中，量化对Llama 3 8B模型的准确性影响不大，但对因果推理的中间层（干预）影响最明显。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型向边缘和资源受限环境部署的背景下，量化模型（如INT8和NF4）已成为标准配置。然而，目前对精度降低如何影响正式的因果推理知之甚少。

Method: 1. 使用包含3000个样本的分层CLadder基准来系统评估量化对Pearl因果阶梯三个层级的影响。
2. 在CRASS基准上进行实验，观察量化对常识性反事实推理任务的影响。
3. 评估使用真实因果图的图检索增强生成（Graph RAG）如何影响量化模型的干预准确性。

Result: 1. 在CLadder基准上，Llama 3 8B模型在量化后（特别是NF4）的准确性大致保持稳定，总体下降不到1%。
2. Pearl因果阶梯第二层级的干预查询对精度损失最敏感，而第三层级的反事实推理相对稳定，但在特定查询类型（如对撞机偏差和后门调整）上表现出异质性弱点。
3. CRASS基准上的实验表明，不同精度下的性能几乎相同，这表明现有常识反事实数据集缺乏揭示量化引起的推理漂移所需的结构敏感性。
4. 使用图检索增强生成后，NF4模型的干预准确性一致提高了1.7%，部分抵消了压缩相关的性能下降。

Conclusion: 1. 因果推理对四比特量化表现出意想不到的鲁棒性。
2. 图结构化增强可以选择性地强化干预推理能力。
3. 当前的反事实基准未能捕捉到更深层次的因果脆弱性。本研究为部署高效且结构支持的因果AI系统提供了初步的经验指导。

Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.

</details>


### [98] [State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models](https://arxiv.org/abs/2512.13762)
*TK Lee*

Main category: cs.AI

TL;DR: 该研究提出了一种定性案例研究方法，用于审计长时程交互中与政策相关的行为选择性，并引入了“习得性无力”（learned incapacity）作为描述大型语言模型选择性规避行为的术语。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）作为通用工具被广泛部署，但长时间的交互可能会揭示出标准定量基准未能捕捉到的行为模式。

Method: 通过对一个86轮对话会话的单一案例研究，作者观察到模型在非敏感领域表现正常，但在提供者或政策敏感领域反复产生功能性拒绝（FR），从而在不同领域之间产生了持续的不对称。研究还定义了三种响应机制（NP、FR、Meta-Narrative; MN），并分析了MN角色框架叙述与敏感语境下拒绝的共现。

Result: 同一个模型在广泛的非敏感领域表现出正常性能（NP），但在提供者或政策敏感领域反复产生功能性拒绝（FR），导致了NP和FR在不同领域之间的持续不对称。元叙事（MN）角色框架叙述倾向于与敏感语境中的拒绝共现。

Conclusion: 该研究提出了一个基于可观察行为的交互层面审计框架，并提出“习得性无力”（LI）作为检查潜在对齐副作用的视角，值得在不同用户和模型之间进行进一步调查。

Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.

</details>


### [99] [Mathematics and Coding are Universal AI Benchmarks](https://arxiv.org/abs/2512.13764)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 这篇论文研究了数学和编码在AI智能体心理测量电池模空间中的特殊作用，并提出了数学纤维概念，证明其在形式证明核下可以实现谱稳定的自我改进。


<details>
  <summary>Details</summary>
Motivation: 探索数学和编码在AI智能体评估中的关键作用，并研究它们如何促进AI的自我改进。

Method: 1. 建立了AAI框架和GVU动力学。2. 定义了数学纤维，并结合形式证明核（如Lean，Coq）研究了该纤维上的GVU流。3. 提出了一个密度定理，用于描述由数学定理证明和编码任务生成的电池子空间在评估度量下于电池模空间中的稠密性。

Result: 1. 结合形式证明核，数学纤维上的GVU流可以实现谱稳定的自我改进。2. 编码任务在生成电池子空间方面具有普适性，而纯数学不具备。3. 数学和编码为AI评估提供了“通用坐标”，形式数学是高级AI智能体递归自我改进的自然点火领域。

Conclusion: 数学和编码在AI智能体的评估和自我改进中扮演着至关重要的角色，尤其是在形式验证的帮助下，它们能够为AI的高级发展提供稳定的自我改进机制和通用评估标准。

Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.

</details>


### [100] [Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems](https://arxiv.org/abs/2512.13771)
*Javier Marín*

Main category: cs.AI

TL;DR: 该篇文章介绍了语义接地指数（SGI），一种衡量RAG系统幻觉的几何指标。


<details>
  <summary>Details</summary>
Motivation: 当检索增强生成（RAG）系统产生幻觉时，这些幻觉在嵌入空间中留下了何种几何痕迹？作者旨在通过引入SGI来解决这个问题。

Method: 文章引入了语义接地指数（SGI），定义为在单位超球面$\mathbb{S}^{d-1}$上，响应到问题的角度距离与响应到上下文的角度距离之比。文章通过理论推导和实验验证了SGI的有效性，并对其在不同条件下的性能进行了分析。

Result: SGI在HaluEval数据集上显示出显著的效果量（Cohen's d范围从0.92到1.28），并且跨模型相关性达到0.85。研究发现，SGI的判别力随问题-上下文角度分离度的增加而增强，效果量从0.61单调上升到1.27，AUC从0.72提高到0.83。SGI在长响应和短问题上表现优异，并且对上下文长度具有鲁棒性。校准分析显示ECE=0.10，表明SGI分数可以作为概率估计。然而，在TruthfulQA上的负面结果表明SGI衡量的是话题参与度而非事实准确性。

Conclusion: SGI提供了一种计算高效且有理论依据的基础设施，用于识别生产RAG部署中需要验证的响应。它能有效检测RAG系统中的语义懒惰和幻觉，但主要衡量话题相关性而非事实准确性。

Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.

</details>


### [101] [MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2512.13955)
*Sindhuja Madabushi,Dawood Wasif,Jin-Hee Cho*

Main category: cs.AI

TL;DR: MURIM是一种多维度的基于声誉的激励机制，它在解决联邦学习中的挑战方面表现出色，包括客户端可靠性、隐私、资源限制和公平性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习作为一种领先的隐私保护机器学习范式，允许参与者共享模型更新而非原始数据。然而，它仍然面临客户端激励不足、隐私风险和资源限制等关键挑战。评估客户端的可靠性对于公平的激励分配和确保每个客户端的数据对全局模型做出有意义的贡献至关重要。

Method: 我们提出了MURIM，一个多维度的基于声誉的激励机制，它综合考虑了客户端的可靠性、隐私、资源能力和公平性，同时防止恶意或不可靠的客户端获得不应得的奖励。MURIM根据客户端的贡献、延迟和声誉分配激励，并由一个可靠性验证模块提供支持。

Result: 在MNIST、FMNIST和ADULT Income数据集上进行的广泛实验表明，与最先进的基线相比，MURIM在公平性指标上最高提高了18%，将隐私攻击成功率降低了5-9%，并将针对投毒和噪声梯度攻击的鲁棒性提高了85%。

Conclusion: MURIM有效地缓解了对抗性威胁，促进了公平和真实的参与，并在异构和动态联邦设置中保持了模型稳定的收敛。

Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.

</details>


### [102] [Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms](https://arxiv.org/abs/2512.13978)
*Yang Cao,Yubin Chen,Xuyang Guo,Zhao Song,Song Yue,Jiahao Zhang,Jiale Zhao*

Main category: cs.AI

TL;DR: 本文对四个前沿大语言模型在数学推理方面的能力进行了全面评估，发现顶级模型表现出较高的准确性，但不同模型的可靠性差异显著。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学推理和科学发现方面取得了显著进展，但仍需对其在经典研究生水平数学理论上的基线推理能力进行严格评估。

Method: 作者使用Motwani和Raghavan的《随机算法》教材中的一系列引理和习题，要求GPT-5-Thinking、Gemini-3-Pro、Claude-Sonnet-4.5-Thinking和Grok-4这四个前沿模型生成正式的LaTeX证明。

Result: 顶部模型（Gemini和Claude）取得了约66%的高准确率，表明它们对概率方法和形式逻辑有扎实的掌握，而其他模型在一致性方面显著落后（约40%）。研究还对生成的证明进行了定性分析，比较了简洁性、幻觉率和逻辑结构。

Conclusion: 前沿模型已达到研究生教学辅助和形式化所需的熟练程度，但在严格的数学推导方面，它们的可靠性仍存在显著差异。

Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.

</details>


### [103] [ReflCtrl: Controlling LLM Reflection via Representation Engineering](https://arxiv.org/abs/2512.13979)
*Ge Yan,Chung-En Sun,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: 本文提出了一种名为ReflCtrl的逐步控制方法，通过在潜在空间中识别和利用反射方向来控制大型语言模型（LLMs）的自我反思频率，从而在保持性能的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 以往的研究表明，大型语言模型（LLMs）的自我反思能力（CoT）可以提高推理性能，但会显著增加推理成本。

Method: 本文通过将模型的推理过程分解为多个步骤，识别出与反思相关的步骤，并在潜在空间中提取出控制这种行为的“反思方向”。在此基础上，本文提出了一种逐步引导方法（ReflCtrl），以控制反思的频率。

Result: 实验结果表明，在许多情况下，模型的自我反思是冗余的，尤其是在更强大的模型中。通过ReflCtrl框架，可以在保持性能的同时，节省高达33.6%的推理tokens。此外，模型反思行为与内部不确定性信号高度相关。

Conclusion: LLMs的自我反思存在冗余，可以通过控制反思频率来提高效率。模型的反思行为可能与模型内部的不确定性有关。

Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.

</details>


### [104] [Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training](https://arxiv.org/abs/2512.13996)
*Can Jin,Hongwu Peng,Mingcan Xiang,Qixin Zhang,Xiangchi Yuan,Amit Hasan,Ohiremen Dibua,Yifan Gong,Yan Kang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 本文提出了DTop-p MoE，一种稀疏性可控的动态Top-p路由机制，通过PI控制器动态调整概率阈值以匹配目标稀疏度，并在LLMs和Diffusion Transformers上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Top-k路由在MoE模型中强制执行统一的稀疏模式，忽略了token难度的差异；而现有Top-p路由实现中的固定全局概率阈值导致计算成本不可控且对超参数敏感。

Method: 本文提出DTop-p MoE，一种稀疏性可控的动态Top-p路由机制。利用PID控制器动态调整概率阈值，以使运行激活专家的稀疏度与指定目标对齐。引入动态路由归一化机制，调整逐层路由logits，允许不同层学习不同的专家选择模式，同时利用全局概率阈值。

Result: DTop-p在大型语言模型和Diffusion Transformers上的广泛实验表明，它始终优于Top-k和固定阈值Top-p基线。DTop-p能精确控制激活专家的数量，同时自适应地分配不同token和层之间的资源。

Conclusion: DTop-p MoE提供了一个强大的大规模MoE预训练框架，具有良好的可扩展性，并且在LLM和Diffusion Transformers上表现出色。

Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

</details>


### [105] [Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation](https://arxiv.org/abs/2512.14048)
*Shen Li,Li Huang,Shaoxiong Zhan,Weifeng Sun,Tao Yin,Zhongxin Liu,Meng Yan*

Main category: cs.AI

TL;DR: RoutingGen是一个新的困难感知路由框架，它动态调整代码生成的提示策略。


<details>
  <summary>Details</summary>
Motivation: 现有CoT提示方法在代码生成中存在两个主要限制：1. 统一应用CoT会导致在简单任务上过度思考；2. 缺乏意图抽象，导致模型关注表面结构而忽略全局问题目标。

Method: RoutingGen框架动态适应代码生成的提示策略。对于简单任务，采用少样本提示；对于复杂任务，SFT 采用意图思维链（ICoT）策略，引导模型捕捉任务意图，如核心算法逻辑及其时间复杂度。

Result: RoutingGen在大多数设置中实现了最先进的性能，同时将总令牌使用量平均减少了46.37%。ICoT在具有挑战性的基准测试中优于六种现有提示基线。

Conclusion: RoutingGen框架解决了现有CoT提示方法的局限性，在代码生成方面取得了显著的性能提升和效率优化。

Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.

</details>


### [106] [OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value](https://arxiv.org/abs/2512.14051)
*Mengzhang Cai,Xin Gao,Yu Li,Honglin Lin,Zheng Liu,Zhuoshi Pan,Qizhi Pei,Xiaoran Shang,Mengyuan Sun,Zinan Tang,Xiaoyang Wang,Zhanping Zhong,Yun Zhu,Dahua Lin,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 本文介绍了OpenDataArena（ODA），一个用于评估大型语言模型后训练数据内在价值的开放平台，旨在解决当前数据不透明、缺乏系统评估的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展依赖于高质量和多样化的后训练数据集。然而，目前的数据是一个黑箱，其组成不透明、来源不确定且缺乏系统评估，这阻碍了模型的可复现性，并掩盖了数据特性与模型行为之间的因果关系。

Method: OpenDataArena（ODA）平台通过四个关键支柱来解决上述问题：1. 统一的训练评估流程，确保不同模型和领域之间的公平开放比较；2. 多维度评分框架，从十几个不同维度评估数据质量；3. 交互式数据溯源浏览器，可视化数据集的起源并分析组件来源；4. 完全开源的工具包，用于训练、评估和评分，以促进数据研究。

Result: 通过ODA平台进行的广泛实验，涵盖22个基准测试中120多个跨多个领域的训练数据集，并经过600多次训练运行和4000万个处理数据点的验证，揭示了非凡的见解。研究揭示了数据复杂性与任务性能之间的内在权衡，通过溯源识别了流行基准中的冗余，并描绘了数据集之间的谱系关系。

Conclusion: ODA旨在将数据策展从试错法转变为数据中心AI的科学原则，为数据混合规律和基础模型的战略组成进行严谨研究铺平道路，而不仅仅是扩展排行榜。作者发布了所有结果、工具和配置，以实现高质量数据评估的民主化。

Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.

</details>


### [107] [RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](https://arxiv.org/abs/2512.14069)
*Junjie Ma,Jinlong Li*

Main category: cs.AI

TL;DR: RADAR是一种基于强化学习的动态草稿树的新的推测采样方法，它通过训练一个预测模型来实时决定对草稿模型的调用，从而减少冗余计算并加速推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的推理成本高昂且速度慢，推测采样是解决这个问题的一种有效方法，但是其草稿模型生成候选token的调用次数是一个预设的超参数，缺乏灵活性。

Method: RADAR将草稿树生成过程建模为马尔可夫决策过程（MDP），并采用离线强化学习来训练一个预测模型，该模型能够实时决定对草稿模型的调用，从而减少冗余计算。

Result: 在三个LLM和四个任务上的评估表明，RADAR相对于自回归解码基线实现了3.17x-4.82x的加速。

Conclusion: RADAR通过引入RL-based动态草稿树，有效地解决了传统推测采样方法中草稿模型调用次数缺乏灵活性M的问题，显著提升了LLM推理速度。

Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

</details>


### [108] [Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model](https://arxiv.org/abs/2512.14112)
*Chunan Tong*

Main category: cs.AI

TL;DR: 该研究提出了一个混合LNN+XGBoost模型，旨在解决供应链管理中传统方法和LLM在面对复杂连续时间序列数据时的效率和适应性问题，以减少牛鞭效应并提高盈利能力。


<details>
  <summary>Details</summary>
Motivation: 供应链管理（SCM）面临需求波动和牛鞭效应等重大挑战，传统方法乃至最先进的LLM都难以处理SCM复杂的连续时间序列数据。现有的机器学习方法（如LSTM和XGBoost）虽然提供了一些解决方案，但往往受限于计算效率。

Method: 本研究提出了一个LNN+XGBoost的混合模型，结合LNN的动态特征提取能力和XGBoost的全局优化能力，应用于多层供应链。

Result: 模型旨在最小化牛鞭效应并提高盈利能力。

Conclusion: 这种创新方法解决了智能SCM中对效率和适应性的需求，填补了关键空白。

Abstract: Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.

</details>


### [109] [Georeferencing complex relative locality descriptions with large language models](https://arxiv.org/abs/2512.14228)
*Aneesha Fernando,Surangika Ranathunga,Kristin Stock,Raj Prasanna,Christopher B. Jones*

Main category: cs.AI

TL;DR: 这篇论文探讨了大型语言模型（LLMs）在对生物多样性收集记录中复杂的地理位置描述进行地理参考方面的潜力，并通过微调和优化方法，显著提高了地理参考的准确性，超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统的地理参考方法（基于地名录或语言模型）难以处理包含空间关系而不只是地名的复杂地理描述，尤其在生物标本采集记录中，这些记录在GPS时代之前常以叙述形式存在。这导致地理参考不准确，且过程耗时，因此需要自动化解决方案。

Method: 本文首先确定了有效的提示模式，然后使用 QLoRA（Quantized Low-Rank Adaptation）技术对一个大型语言模型进行了微调。微调使用了来自不同区域和语言的生物多样性数据集。

Result: 该方法在固定训练数据量的情况下，在所有数据集中，平均有65%的记录在10公里范围内，优于现有基线。在纽约州的数据集上，最佳结果达到了10公里内85%的准确率和1公里内67%的准确率。选定的大型语言模型在处理冗长、复杂的描述时表现出色。

Conclusion: 大型语言模型在对复杂地理位置描述进行地理参考方面具有巨大潜力，尤其适用于生物多样性集合等领域，能够显著提高地理参考的准确性和自动化水平。

Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.

</details>


### [110] [Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting](https://arxiv.org/abs/2512.14288)
*Georgios Bouchouras,Dimitrios Doumanas,Andreas Soularidis,Konstantinos Kotis,George A. Vouros*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLM）在帕金森病（PD）监测和预警本体工程中的应用，并评估了LLM独立构建本体的能力以及人机协作在这一过程中的有效性。研究表明，虽然LLM可以自主生成本体，但人机协作（特别是通过X-HCOME和SimX-HCOME+方法）能显著提高本体的全面性和准确性，使其接近专家水平。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能独立创建全面本体，以及人机协作能否实现这一目标，从而评估LLM在自动化本体开发中的有效性及其通过人机协作获得的提升。

Method: 采用四种关键方法：One Shot (OS) 提示技术、Chain of Thought (CoT) 提示、X-HCOME和SimX-HCOME+。OS和CoT用于LLM自主生成本体，X-HCOME和SimX-HCOME+用于结合人类专业知识与LLM能力进行本体工程。

Result: 通过OS和CoT提示，LLM可以自主构建PD监测和预警本体，但输出不全面，需大量人工完善。X-HCOME方法显著提高了本体的全面性，产出的本体与专家构建的非常相似。SimX-HCOME+方法（强调持续人工监督和迭代改进）进一步提高了本体的全面性和准确性。

Conclusion: 人类与LLM的协作在本体工程中具有巨大潜力，特别是在帕金森病等复杂领域。研究结果为未来的本体构建（包括开发专门的GPT模型）指明了前景广阔的研究方向。

Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.

</details>


### [111] [Massive Editing for Large Language Models Based on Dynamic Weight Generation](https://arxiv.org/abs/2512.14395)
*Wentao Wan,Qiqing Lao,Zhiwei Xie,Hefeng Wu,Runnan Lin,Liang Lin,Keze Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为MeG的大规模知识编辑方法，它通过在LLM的特定层添加一个动态权重神经元，并使用扩散模型根据输入查询条件性地生成神经元权重，从而实现低成本的大规模知识编辑。


<details>
  <summary>Details</summary>
Motivation: 目前，LLM在确保编辑的可靠性、泛化性和局部性指标的同时进行大规模编辑仍然是一个挑战。

Method: 本文提出了一种基于动态权重生成（MeG）的大规模LLMs编辑方法。该方法涉及在LLMs的特定层附加一个动态权重神经元，并使用扩散模型根据知识所需的输入查询条件性地生成该神经元的权重。

Result: 实验表明，与现有知识编辑方法相比，MeG在可靠性、泛化性和局部性指标方面显著提高了大规模KE的性能，尤其是在局部性指标的绝对值指数上取得了较高的百分点增长。

Conclusion: MeG方法通过添加单个动态权重神经元实现了大规模知识编辑的目标，并显著提升了编辑的可靠性、泛化性和局部性。

Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.

</details>


### [112] [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465)
*Siyuan Zhu,Chengdong Xu,Kaiqiang Ke,Chao Yu*

Main category: cs.AI

TL;DR: 本文介绍了一种名为Context-Picker的框架，旨在于长文本问答（LCQA）中优化上下文选择。Context-Picker通过一个两阶段的强化学习过程，从相似度排序转变为最小充分子集选择，从而在保证答案准确性的同时，减少了上下文长度。


<details>
  <summary>Details</summary>
Motivation: 在长文本问答（LCQA）中，为给定查询确定最佳上下文量是一个重大挑战。上下文过少可能遗漏关键信息，而上下文过多则可能引入噪声并降低答案质量。传统方法在选择正确数量的文本段落时面临困境，尤其对于事实型问题，通常只需要少量特定证据。

Method: 本文引入了Context-Picker，一个推理感知的框架，将上下文选择从基于相似度的排名转变为最小充分子集选择。Context-Picker将上下文选择视为一个决策过程，并通过一个受人类启发设计的两阶段强化学习计划进行优化：一个以召回率为导向的阶段，优先覆盖推理链；随后是一个以精确率为导向的阶段，积极修剪冗余以提取紧凑的证据集。为了解决奖励稀疏性问题，本文提出了一种离线证据提炼流程，通过“留一法”（LOO）程序挖掘“最小充分集”，提供密集的、与任务对齐的监督。

Result: 在五个长上下文和多跳问答基准测试中进行的实验表明，Context-Picker显著优于强大的RAG基线模型，在上下文长度相当或更短的情况下，实现了卓越的答案准确性。

Conclusion: 本文提出的Context-Picker框架，通过其独特的两阶段强化学习上下文选择机制和离线证据提炼流程，有效解决了长文本问答中上下文选择的挑战，并在保证高准确率的同时，显著优化了上下文长度。

Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.

</details>


### [113] [Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence](https://arxiv.org/abs/2512.14527)
*Shreyas Subramanian,Bala Krishnamoorthy,Pranav Murthy*

Main category: cs.AI

TL;DR: 这篇论文介绍了一种名为GreedyLR的新型学习率调度器，它能根据当前损失自适应地调整学习率，并在多种任务上表现优于现有SOTA调度器。


<details>
  <summary>Details</summary>
Motivation: 尽管优化器在训练方面取得了显著进展，但大多数研究工作仍使用Cos或是指数衰减等常见的调度器。

Method: 本研究提出了一种名为GreedyLR的新型调度器。该调度器根据当前的损失自适应地调整学习率。

Result: 实验结果表明，该方法在准确性、速度和收敛性方面优于几个最先进的调度器。我们还对GreedyLR算法进行了理论分析，包括收敛性证明以及推导出能够最大化收敛速度的最佳比例因子F，并通过实验证明了该算法对实际噪声的鲁棒性。

Conclusion: 所提出的调度器易于实现，计算效率高，可以被认为是训练的一个很好的默认调度器。

Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.

</details>


### [114] [Universal Reasoning Model](https://arxiv.org/abs/2512.14693)
*Zitian Gao,Lynx Chen,Yihao Xiao,He Xing,Ran Tao,Haoming Luo,Joey Zhou,Bryan Dai*

Main category: cs.AI

TL;DR: 本文分析了通用Transformer（UTs）在复杂推理任务中的性能，并提出了Universal Reasoning Model（URM），通过引入短卷积和截断反向传播，在ARC-AGI任务上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 探索通用Transformer（UTs）在复杂推理任务中性能提升的具体原因，并在此基础上提出更高效的模型。

Method: 系统分析了UTs的变体，发现性能提升主要来源于循环归纳偏置和强大的非线性组件。在此基础上，提出了Universal Reasoning Model (URM)，通过引入短卷积和截断反向传播来增强UTs。

Result: URM在ARC-AGI 1上取得了53.8%的pass@1，在ARC-AGI 2上取得了16.0%的pass@1，达到了最先进的水平。

Conclusion: 循环归纳偏置和强大的非线性组件是UTs在复杂推理任务中性能提升的关键。通过结合短卷积和截断反向传播，可以进一步提高模型的推理能力。

Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

</details>


### [115] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 研究了智能体是否能像人类一样通过交互学习，并提出了IPR模型和PhysCode，在G2U基准测试中表现出色，超越GPT-5。


<details>
  <summary>Details</summary>
Motivation: 智能体能否像人类一样通过与环境的交互来学习并持续改进推理能力。现有的VLM和世界模型在捕捉潜在物理和因果关系方面存在不足。

Method: 提出了IPR（Interactive Physical Reasoner）模型，该模型使用世界模型rollout来评估和强化VLM的策略，并引入了PhysCode，一个以物理为中心的动作代码，旨在将语义意图与动力学对齐，为预测和推理提供共享的动作空间。

Result: IPR模型在1,000多个游戏的预训练后，在从原始直觉到目标驱动的推理等多个层面都表现出强大的鲁棒性，甚至整体性能超越了GPT-5。模型的性能随着训练游戏数量和交互步骤的增加而提高，并且能够零样本迁移到未见过的游戏。

Conclusion: 以物理为中心的交互是稳步提高物理推理能力的一条有效途径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.

</details>
