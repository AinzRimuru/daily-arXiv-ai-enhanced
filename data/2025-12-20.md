<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: 本文提出 GraFT 框架，通过引入语法约束减少语言模型的输出空间，显著提升了从自然语言到时序逻辑翻译的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言（NL）转时序逻辑（TL）方法在原子命题（AP）提取、指代解析以及小样本学习方面表现不佳，主要原因是搜索空间过大。

Method: 提出 GraFT 框架，利用语法约束（Grammar Forced Translation）在翻译的每一步限制 LLM 的可选 Token 范围，将全词表预测缩减为极小集合的预测。

Result: 在 CW、GLTL 和 Navi 基准测试中，GraFT 的端到端翻译准确率平均提升了 5.49%，而其在域外（Out-of-domain）翻译准确率显著提升了 14.06%。

Conclusion: GraFT 通过减少搜索空间极大地提高了模型在有限数据下的学习效率，是解决 NL 到 TL 翻译任务中复杂性和跨领域泛化问题的有效方案。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [2] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文通过信息论方法量化了语音韵律与文本在传递语义信息上的差异，发现韵律在传达讽刺和情感方面远比文本重要。


<details>
  <summary>Details</summary>
Motivation: 旨在量化韵律（语音的旋律）中蕴含的、未被文本捕捉的关键信息，并明确这些信息的具体指向。

Method: 提出一种基于信息论的方法，利用大规模语音和语言模型来估计语音含义的特定维度（如情感）与不同通信渠道（音频或文本）之间的互信息。

Result: 在讽刺和情感表达方面，音频渠道（韵律）传递的信息量比纯文本高出一个数量级（在缺乏长期上下文时）；但在判断是否为疑问句时，韵律提供的额外信息相对较少。

Conclusion: 该信息论方法可以推广到更多语义维度、通信渠道和语言的研究中，为量化分析语音的多模态信息贡献了有效框架。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [3] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: 本文提出 LLMCache，一种跨模型、跨架构的层级化缓存框架，通过语义相似性重用中间激活值，在保持高精度的同时将推理速度提升达 3.1 倍。


<details>
  <summary>Details</summary>
Motivation: Transformer 模型的高推理延迟制约了其实时和大规模部署，且现有的 Token 级 KV 缓存机制在适用范围和提速效果上存在局限。

Method: 提出 LLMCache 框架，采用层级化缓存（layer-wise caching）重用中间激活值。核心技术包括：用于匹配语义相似输入的轻量级指纹机制，以及管理缓存过时性的自适应淘汰策略。

Result: 在 BERT 和 GPT-2 模型上的实验表明，LLMCache 在 SQuAD 等数据集上可实现高达 3.1 倍的推理加速，而准确率下降不到 0.5%。

Conclusion: LLMCache 是一种实用且通用的解决方案，能够显著优化 Transformer 模型在真实场景中的推理性能。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [4] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: 本文推出 MMRB2，首个针对多模态（图文交织）理解与生成的奖励模型综合基准，揭示了当前 AI 裁判与人类专家在偏好判断上的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管奖励模型在 LLM 训练中至关重要，但针对能处理图文交织序列的全能模型（Omni models）的奖励模型研究还很少，缺乏全面的评估基准。

Method: 构建评估基准 MM-RewardBench 2 (MMRB2)，涵盖文本到图像、图像编辑、交织生成和多模态推理四个任务，包含 4000 对专家标注的偏好数据。采用集成过滤策略确保数据质量，并评估了多种 LLM-as-a-judge 及训练后的奖励模型。

Result: Gemini 1.5 Pro 表现最佳（75-80% 准确率）；GPT-4o 和 Gemini 1.5 Flash 表现次之；开源模型 Qwen2-VL-72B 与 Gemini 1.5 Flash 相当（64%）。研究还证明 MMRB2 的表现与 Best-of-N 采样下的模型下游任务质量高度正相关。

Conclusion: MMRB2 为多模态奖励模型提供了标准化的评估基准。研究发现，尽管最先进的闭源模型（如 Gemini 1.5 Pro）在评估上表现出色，但与人类专家相比仍有约 15-20% 的准确率差距，开源模型（如 Qwen2-VL）也有巨大进步空间。

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [5] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 研究发现 Transformer 在处理变量化算术任务时，能通过学习“交换复制”和“单位元识别”等符号推理机制，实现对动态含义符号的高效代数运算与泛化。


<details>
  <summary>Details</summary>
Motivation: 探究当 Transformer 面对含义不固定（变量化）的算术序列时，其内部的学习机制及代数推理能力。

Method: 设计了一项新任务，其中符号与代数群元素的映射随序列变化（In-context）；通过控制数据分布进行因果测试，隔离并识别特定的注意力机制。

Result: 模型在此类高难度任务上达到近乎完善的准确率，并能泛化至未见过的代数群。识别出三种核心机制：交换复制（Commutative Copying）、单位元识别（Identity Recognition）和基于闭包的抵消（Closure-based Cancellation）。

Conclusion: 在动态变量映射的环境下，Transformer 不仅依赖几何嵌入，更倾向于开发具备泛化能力的符号化推理机制来解决代数问题。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 该研究通过对抗强化学习联合训练推理器和判别器，利用密集的步骤级奖励解决了 LLM 数学推理中的过程错误问题，大幅提升了模型在 AIME 等竞赛级数学任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理中仍存在过程错误（如计算错误、逻辑脆弱等），且传统的稀疏奖励信号（仅看答案正确与否）难以实现精准的信用分配。

Method: 提出 GAR（Generative Adversarial Reasoner）框架。通过一种计算高效的复核机制将推理链切片，利用判别器对每个切片的合理性给出结构化评价。推理器因逻辑严密且答案正确获得奖励，判别器因准确检测错误获得奖励，实现模型间的对抗式共同进化。

Result: 在多个数学评测集上取得显著提升。在 AIME24 上，DeepSeek-R1-Distill-Qwen-7B 准确率提高 7.3%，Llama-8B 提高 10.0%。该方法显著提升了样本效率和推理质量。

Conclusion: GAR 框架证明了生成模型与判别模型通过对抗强化学习共同进化，能有效减少复杂推理中的过程错误。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


### [7] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出TOGGLE，通过信号时序逻辑（STL）引导压缩过程，在大幅降低LLM计算开销和体积的同时，提供模型行为的形式化验证保证。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM压缩技术（如量化、剪枝）计算需求大，且往往会损坏关键的语言属性，由于缺乏形式化保证，难以确保压缩后模型的行为。

Method: 提出TOGGLE框架，利用信号时序逻辑（STL）形式化定义语言属性，并结合STL鲁棒性引导的贝叶斯优化，在不进行重训练和微调的情况下，自动探索层级量化和剪枝的最优配置。

Result: 在四种主流模型（GPT-2, DeepSeek-V2, LLaMA 3, Mistral）上评估，实现了高达3.3倍的FLOPs减少和68.8%的模型尺寸缩减，同时确保所有指定的语言形式化约束得到满足。

Conclusion: TOGGLE是首个将形式化方法引入LLM压缩的框架，解决了模型轻量化与语言特性保持之间的矛盾，为边缘设备的验证部署提供了新途径。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [8] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 本文提出了“社会责任栈（SRS）”，一个将社会伦理原则转化为可执行 AI 工程约束和闭环监控机制的六层架构框架。


<details>
  <summary>Details</summary>
Motivation: 现有的负责任 AI 治理工作多停留在规范性原则层面，缺乏在整个系统生命周期中可操作、可强制执行的工程机制。

Method: 通过一个六层架构框架将社会价值嵌入系统，并将其建模为社会技术系统的闭环监督控制问题。利用约束公式、安全包络（safety-envelope）和反馈机制，在设计和运行时实施监控。

Result: 通过在临床决策、协作自动驾驶和公共部门系统中的案例研究，证明了该框架能将抽象的规范目标转化为具体的工程和运行控制指标。

Conclusion: SRS 为构建可负责、可自适应且可审计的社会技术 AI 系统提供了实践基础，弥合了伦理、控制理论与治理之间的鸿沟。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies](https://arxiv.org/abs/2512.16876)
*Astrid Brull,Sara Aguti,Véronique Bolduc,Ying Hu,Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del-Rio,Oleksii Sliusarenko,Haiyan Zhou,Francesco Muntoni,Carsten G. Bönnemann,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 本研究通过联邦学习在不共享原始隐私数据的前提下，实现了跨国协作诊断罕见病，显著提升了COL6-RD疾病分类模型的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 罕见病（如COL6-RD）诊断面临数据稀缺且碎片化的问题，而跨机构的数据整合又受到隐私、法规和物流的严重限制。

Method: 利用Sherpa.ai联邦学习平台，在两个国际组织的分布式数据集上协同训练模型。该模型基于患者成纤维细胞培养物的胶原VI免疫荧光显微镜图像进行病理分类。

Result: 该模型成功将COL6-RD分类为三类主要致病机制，其F1分数达到0.82，显著优于各机构独立训练的模型（0.57-0.75）。

Conclusion: 联邦学习（FL）有效克服了罕见病研究中的数据鸿沟，为分布式隐私数据的协作诊断提供了可行的技术路径，不仅能辅助解释不确定意义的变异，还能指导基因测序策略。

Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.

</details>
