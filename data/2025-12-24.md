<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.IT](#cs.IT) [Total: 4]
- [cs.SI](#cs.SI) [Total: 2]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.AI](#cs.AI) [Total: 26]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data](https://arxiv.org/abs/2512.19864)
*Shashi Kant Gupta,Arijeet Pramanik,Jerrin John Thomas,Regina Schwind,Lauren Wiener,Avi Raju,Jeremy Kornbluth,Yanshan Wang,Zhaohui Su,Hrituraj Singh*

Main category: cs.CL

TL;DR: 本研究开发了一个基于LLM的智能体框架，用于从大规模非结构化临床笔记中自动提取肿瘤学结构化数据，实现了高精度（F1 0.93）并显著降低了人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中的非结构化临床笔记包含关键医疗信息，但由于语言异构、术语专业及跨文档矛盾，导致传统自动化方法难以实现准确的患者级数据合成，而人工提取则成本过高。

Method: 提出了一种智能体框架（Agentic Framework），将复杂的提取任务分解为模块化的自适应任务。该框架利用LLM作为推理智能体，结合上下文敏感检索和迭代综合能力，对真实的肿瘤学记录进行全面处理。

Result: 在超过40万份临床笔记和PDF报告的大规模数据集上，该方法平均F1分数达0.93，关键变量（如生物标志物和药物）超过0.95。此外，系统生成的结论在人工审核中获得了0.94的直接批准率。

Conclusion: 该研究证明了基于大语言模型的智能体框架在处理大规模、异构医疗文书方面的卓越能力，显著降低了临床数据结构化的成本，具有极高的实际应用价值。

Abstract: Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale

</details>


### [2] [How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse](https://arxiv.org/abs/2512.19903)
*Kirk Vanacore,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 研究评估了六种大型语言模型在分类教室对话中的表现，发现尽管多样本提示能显著提升准确度，但模型在处理复杂教学脉络时仍存在局限性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在教育领域的广泛应用，了解其在未经大规模定制（即开箱即用）的情况下，理解真实教学场景的能力对于设定预期和基准测试至关重要。

Method: 比较了六种大型语言模型（LLMs）在零样本（zero-shot）、单样本（one-shot）和多样本（few-shot）提示条件下的表现。任务是将真实的教室教学录音文本按教学动作（instructional moves）进行分类。

Result: 多样本提示显著提升了SOTA模型的表现，最高Cohen's Kappa达到0.58。但改进并不平衡：不同教学动作的分类准确率差异很大，且召回率的提升往往伴随着误报率的增加。

Conclusion: 基础模型在解释教学对话方面具有一定意义但有限的能力。虽然提示词工程能挖掘其潜力，但无法完全消除可靠性方面的瓶颈。

Abstract: Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.

</details>


### [3] [Counterfactual LLM-based Framework for Measuring Rhetorical Style](https://arxiv.org/abs/2512.19908)
*Jingyi Qiu,Hong Chen,Zongyi Li*

Main category: cs.CL

TL;DR: 研究开发了一个基于LLM的框架，量化了八年间ICLR论文的修辞风格，发现夸张的辞藻能显著提升论文曝光度，并揭示了2023年后LLM辅助写作导致论文“炒作成分”激增。


<details>
  <summary>Details</summary>
Motivation: 机器学习论文中的“过度炒作（hype）”引起广泛关注，但由于夸张辞藻可能源于真实成果或纯粹修辞，难以在学术评估中独立量化这种修辞风格。

Method: 提出一种基于LLM的反事实框架：利用多个LLM预设角色对同一论文内容生成不同风格的实验文本，通过LLM判别器进行成对比较，并使用Bradley-Terry模型聚合评分，从而将“修辞风格”从“实质内容”中剥离。

Result: 1. 具有“远见/愿景式（visionary）”框架的论文即使在控制同行评审成绩后，仍能获得更多引用和媒体关注；2. 2023年后论文修辞强度大幅上升，主要由LLM辅助写作驱动；3. 该框架在不同LLM角色下表现稳健，且与人类标注高度一致。

Conclusion: 本研究证明了视觉化修辞对学术影响力有显著预测作用，并揭示了LLM正在改变科学论文的写作风格。LLM不仅是写作工具，也可作为评估科学趋势的有效度量工具。

Abstract: The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.

</details>


### [4] [Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems](https://arxiv.org/abs/2512.19950)
*Heet Bodara,Md Masum Mushfiq,Isma Farah Siddiqui*

Main category: cs.CL

TL;DR: 本研究探讨了LLM对话系统中的隐藏语调偏差，通过合成数据集和集成学习模型证明了这种偏差的系统性存在，并提供了有效的量化检测方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM生成的回复流畅自然，但常带有过度礼貌、乐观或谨慎等微妙的语调偏差。研究旨在探讨这种隐藏的偏差特征及其对用户信任、共情和公平性的影响。

Method: 集成可控LLM对话合成技术与语调分类模型。通过中性提示和带语调引导的提示生成两个合成数据集，并利用预训练的DistilBERT进行弱监督标注，最后训练集成模型（Ensemble models）进行识别。

Result: 研究发现即使在“中性”提示下，LLM仍表现出一致的语调倾斜。开发的集成模型在检测这些偏差模式时达到了0.92的F1分数，证明了语调偏差是系统化且可衡量的。

Conclusion: 语调偏差是大语言模型系统性存在的特征，可以通过机器学习方法进行量化和监测，这对构建公平、值得信赖的对话式AI至关重要。

Abstract: Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.

</details>


### [5] [Schoenfeld's Anatomy of Mathematical Reasoning by Language Models](https://arxiv.org/abs/2512.19995)
*Ming Li,Chenrui Fan,Yize Cheng,Soheil Feizi,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出 ThinkARM 框架，利用片段理论将 LLM 的推理轨迹抽象为功能性步骤，揭示了模型在数学推理中的认知结构动态及其与准确性的关联。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型生成的推理轨迹日益增多，但其底层的认知结构和逻辑步骤仍难以通过表面统计数据进行深度识别和分析。

Method: 借鉴 Schoenfeld 的片段理论（Episode Theory），开发了 ThinkARM 框架。该框架将复杂的推理过程抽象为“分析、探索、执行、验证”等功能性步骤，实现对模型推理动态的规模化语义映射。

Result: 1. 揭示了推理模型与非推理模型在结构上的本质差异；2. 发现“探索”步骤是决定解题正确性的关键分支；3. 证明效率优化（如剪枝）主要抑制了评估反馈步骤，而非均匀缩短所有环节。

Conclusion: ThinkARM 框架成功将不透明的推理轨迹转化为可解释的功能性序列，证明了在中间层次（Episode-level）分析模型认知结构的可行性与价值。

Abstract: Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.

</details>


### [6] [ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language](https://arxiv.org/abs/2512.20111)
*Aly Lidayan,Jakob Bjorner,Satvik Golechha,Kartik Goyal,Alane Suhr*

Main category: cs.CL

TL;DR: 本文提出 ABBEL 框架，通过将长历史压缩成自然语言“信念状态”来减少 LLM 内存压力，并结合强化学习克服信息丢失与误差传播，实现了高性能、低内存的长程决策。


<details>
  <summary>Details</summary>
Motivation: 随着序列决策任务长度增加，将完整的交互历史保留在 LLM 上下文中会导致计算开销过大、效率极低，因此需要一种能维持简短上下文且不丢失关键信息的方法。

Method: 提出 ABBEL 框架，将冗长的交互历史压缩为自然语言描述的“信念状态”（Belief State）。智能体每步仅根据当前观察更新信念并以此执行动作，并引入 RL（信念评分奖励和长度惩罚）优化信念生成质量。

Result: 在六种多步任务环境中，ABBEL 实现了近乎恒定的内存占用。虽然初始阶段受误差累积影响弱于全上下文模型，但通过 RL 训练后，性能不仅优于全上下文方案，且内存开销远低于同类方法。

Conclusion: 结合 LLM 与信念压缩的 ABBEL 框架能以极低的内存成本处理长程任务，且通过 RL 调优可克服误差累积，性能反超全上下文方案。

Abstract: As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.

</details>


### [7] [M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2512.20136)
*Hyeongcheol Park,Jiyoung Seo,Jaewon Mun,Hogun Park,Wonmin Byeon,Sung June Kim,Hyeonsoo Im,JeungSub Lee,Sangpil Kim*

Main category: cs.CL

TL;DR: 本文提出 M$^3$KG-RAG，通过构建多跳多模态知识图谱并引入 GRASP 检索剪枝机制，显著提升了多模态大模型在音视频领域的推理精度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的音视频领域多模态 RAG 存在两个限制：1) 知识图谱模态覆盖率低且缺乏多跳连接；2) 仅基于向量相似度的检索无法过滤无关或冗余知识。

Method: 1. 使用轻量化多智能体流水线构建包含上下文增强三元组的 M$^3$KG；2. 提出 GRASP（基元检索与选择性剪枝）技术，实现实体精准对齐、相关性评估及冗余上下文剔除。

Result: 在多个多模态基准测试中，M$^3$KG-RAG 在推理深度和回答忠实度方面均显著优于现有方法，增强了 MLLM 的多模态推理能力。

Conclusion: M$^3$KG-RAG 通过构建多跳多模态知识图谱并引入精准检索与剪枝机制，有效解决了音视频领域 RAG 的模态覆盖不足及冗余干扰问题。

Abstract: Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.

</details>


### [8] [Multi-hop Reasoning via Early Knowledge Alignment](https://arxiv.org/abs/2512.20144)
*Yuxin Wang,Shicheng Fang,Bo Wang,Qi Luo,Xuanjing Huang,Yining Zheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文通过引入 EKA 模块，在迭代式 RAG 的规划阶段前预先对齐检索库知识，有效解决了多跳问题中的检索低效和推理链崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的迭代式 RAG 系统在分解复杂多跳问题时，往往不考虑检索库的具体信息，导致检索效率低下及推理链错误累积（级联误差）。

Method: 提出“早起知识对齐”（Early Knowledge Alignment, EKA）模块，在迭代式 RAG 系统进行问题分解和规划之前，先利用上下文相关的检索知识使 LLM 与检索库进行对齐。

Result: 在 6 个标准 RAG 数据集上的实验表明，EKA 显著提高了检索精度，减少了级联错误；熵值分析显示其降低了推理过程中的无效搜索。此外，该方法作为一种无需训练的推理策略，具有良好的可扩展性和鲁棒性。

Conclusion: EKA 技术通过在规划前进行知识对齐，显著提升了迭代式 RAG 系统的性能和效率，并揭示了结构化推理与高效搜索在强化学习框架中的协同作用。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.

</details>


### [9] [Retrieval-augmented Prompt Learning for Pre-trained Foundation Models](https://arxiv.org/abs/2512.20145)
*Xiang Chen,Yixin Ou,Quan Feng,Lei Li,Piji Li,Haibo Ye,Sheng-Jun Huang,Shuofei Qiao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: RetroPrompt 通过引入外部检索机制，将“记忆”与“泛化”解耦，解决了传统提示学习在数据匮乏时容易过拟合和泛化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的基础模型提示学习仍遵循参数化学习范式，在数据有限时容易导致过度拟合浅层模式，难以充分利用非典型实例，从而影响泛化的稳定性。

Method: 提出 RetroPrompt 框架，通过构建基于训练数据的公开知识库，并在输入、训练和推理阶段引入检索机制，使模型能主动获取相关的上下文信息。

Result: 在 NLP 和 CV 的多项数据集实验中，RetroPrompt 在零样本和少样本场景下均表现出优于传统方法的性能，并有效缓解了过拟合问题。

Conclusion: RetroPrompt 成功地将知识与纯粹的记忆解耦，减少了对机械记忆的依赖，是提升基础模型在少样本场景下泛化能力的有效方案。

Abstract: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.

</details>


### [10] [Fun-Audio-Chat Technical Report](https://arxiv.org/abs/2512.20156)
*Qian Chen,Luyao Cheng,Chong Deng,Xiangang Li,Jiaqing Liu,Chao-Hong Tan,Wen Wang,Junhao Xu,Jieping Ye,Qinglin Zhang,Qiquan Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: 本文推出Fun-Audio-Chat，一种通过双分辨率语音表示和防遗忘训练策略构建的大型音频语言模型，实现了高效、低损耗且具备全双工能力的高质量语音交互。


<details>
  <summary>Details</summary>
Motivation: 现有的语音-文本联合模型面临语音和文本Token时间分辨率不匹配（25Hz vs 3Hz）、语义稀释、计算成本高以及LLM灾难性遗忘文本知识的问题。

Method: 1) 引入双分辨率语音表示（DRSR）：LLM处理5Hz低频特征以减少计算量，Refined Head处理25Hz高频特征保证质量；2) 核心鸡尾酒训练（Core-Cocktail Training）：通过两阶段微调与合并防止文本知识遗忘；3) 多任务DPO训练增强指令遵循和共情能力。

Result: Fun-Audio-Chat 8B和MoE 30B模型在端到端语音识别、语音对话、口语问答（Spoken QA）和指令遵循等任务上表现优异，且GPU资源消耗降低约50%。

Conclusion: Fun-Audio-Chat证明了通过双分辨率架构和优化的后训练策略，可以在保持LLM原有文本能力的同时，实现高效且具备情感共鸣的实时语音交互。

Abstract: Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.

</details>


### [11] [AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications](https://arxiv.org/abs/2512.20164)
*Honglin Mu,Jinghao Liu,Kaiyang Wan,Rui Xing,Xiuying Chen,Timothy Baldwin,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文揭示了LLM在简历筛选等应用中极易受隐藏对抗指令攻击，并证明通过LoRA适配的训练端防御比简单的提示词工程更有效。


<details>
  <summary>Details</summary>
Motivation: LLM 易受到隐藏在输入数据（如简历或代码）中的“对抗性指令”操纵，且在简历筛选等新兴应用领域缺乏有效的防御机制。

Method: 提出了针对简历筛选场景的评估基准，并引入了基于LoRA适配的“通过分离检测外部指令”（FIDS）方法。

Result: 特定攻击成功率超过80%；FIDS方法在减少15.4%攻击的同时误报增加10.4%；结合提示词防御后可减少26.3%的攻击，表现优于单一防御。

Conclusion: 训练阶段的防御策略（如微调）在提升安全性的同时也更优于仅在推理阶段的缓解措施，能更好地兼顾防御效果与系统效用。

Abstract: Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by "adversarial instructions" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.

</details>


### [12] [FaithLens: Detecting and Explaining Faithfulness Hallucination](https://arxiv.org/abs/2512.20182)
*Shuzheng Si,Qingyi Wang,Haozhe Zhao,Yuzhuo Bai,Guanqiao Chen,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出 FaithLens，一种高效且参数仅为 8B 的幻觉检测模型，通过合成数据微调和强化学习，在 12 项任务上超越了 GPT-4.1，并能提供可靠的诊断解释。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在检索增强生成（RAG）和摘要等任务中面临“忠实性幻觉”问题，需开发一种既能提供二元预测，又能提供合理解释的高效检测模型。

Method: 1) 利用先进 LLM 合成带有解释的训练数据；2) 应用严格的数据过滤策略确保质量；3) 进行冷启动微调；4) 使用基于规则的强化学习（奖励预测准确性和解释质量）进一步优化。

Result: 8B 参数规模的 FaithLens 在 12 项多样化任务中的表现超过了 GPT-4.1 和 o3 等先进模型，并能生成高质量的解释。

Conclusion: FaithLens 在性能、可信度、效率和效果之间取得了平衡，是处理现实世界 LLM 任务中忠实性幻觉检测的有力工具。

Abstract: Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.

</details>


### [13] [Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings](https://arxiv.org/abs/2512.20204)
*Marko Čechovič,Natália Komorníková,Dominik Macháček,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文发布了一个包含12种语言、附带翻译与摘要的跨语言对话语料库，并探索了利用大模型自动检测会议中误解的方法。


<details>
  <summary>Details</summary>
Motivation: 缺乏多样且真实的跨语言对话语料库，无法有效评估自动同声传译系统及跨语言会议中的误解现象。

Method: 录制12种语言的跨语言对话，提供语音转录、多级翻译及会议纪要；并利用Gemini等大语言模型进行误解的自动识别与标注。

Result: 构建了包含5小时语音、多种翻译版本及摘要的语料库；Gemini模型在误解检测任务中达到了77%的召回率和47%的准确率。

Conclusion: 该语料库和误解检测方法为研究跨语言会议动态和提升自动翻译系统的交互质量提供了重要工具。

Abstract: Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.
  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.

</details>


### [14] [Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives](https://arxiv.org/abs/2512.20298)
*Karolina Drożdż,Kacper Dudzic,Anna Sterna,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: 研究发现Gemini Pro在识别边缘型人格障碍上优于人类专家，但在诊断自恋型人格障碍时由于术语偏差导致严重误诊，反映了LLM在精神医学应用中存在可靠性短板。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地依赖LLM进行精神评估，亟需验证模型在理解患者定性叙述及诊断复杂人格障碍方面的能力与准确性。

Method: 使用波兰语的第一人称自传体叙述，将最先进的LLM（如Gemini Pro）与心理健康专业人员在诊断边缘型人格障碍（BPD）和自恋型人格障碍（NPD）的表现进行对比。

Result: Gemini Pro在总体准确率上比人类专家高出21.91%，但在NPD诊断上表现极差（F1仅6.7对比专家的50.0），显示出对“自恋”一词的避讳。LLM侧重于模式和分类的正当化描述，而人类专家更关注患者的自我感。

Conclusion: 尽管LLM在解析定性临床叙述方面展现出极高潜力，但在诊断特定障碍（如NPD）时存在明显的偏差和可靠性问题，不能完全替代人类专家的深度临床判断。

Abstract: Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term "narcissism." Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.

</details>


### [15] [Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://arxiv.org/abs/2512.20324)
*Nurul Labib Sayeedi,Md. Faiyaz Abdullah Sayeedi,Khushnur Binte Jahangir,Swakkhar Shatabda,Sarah Masud Preum*

Main category: cs.CL

TL;DR: 本文通过引入BanglaRiddleEval基准，揭示了当前大语言模型在孟加拉语谜语推理和文化理解方面与人类水平相比仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在NLP基准上表现优异，但在非语言直译、具有文化背景及低资源语言（如孟加拉语）的推理能力尚未得到充分探索。

Method: 通过LLM流水线构建了包含1,244个传统孟加拉语谜语的基准测试，涵盖四种任务（共4,976个工件），并生成了思维链（CoT）解释、语义干扰项及歧义标注，对多种开源和闭源模型进行了评估。

Result: 模型在MCQ任务中最高准确率仅为56%，远低于83%的人类基准；歧义解析能力弱（26%-68%）；虽然生成式QA有语义重叠，但准确性较低，高质量解释仅限于最强模型。

Conclusion: 当前LLM在处理低资源语言的修辞推理方面仍面临巨大挑战，BanglaRiddleEval为提升模型在文化敏感型任务上的表现提供了重要的评估基准。

Abstract: Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.

</details>


### [16] [Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining](https://arxiv.org/abs/2512.20404)
*Junyi Liu,Stanley Kok*

Main category: cs.CL

TL;DR: 本文开发了一个结合情感信号的文本摘要框架，旨在优化社交媒体等非结构化数据的挖掘，助力企业在动态在线环境中的战略决策。


<details>
  <summary>Details</summary>
Motivation: 现有的摘要方法多针对结构化新闻优化，难以处理非结构化社交媒体数据中的噪声、非正式表达及丰富的情感线索。

Method: 提出一种情感感知框架，通过将情感信号嵌入排序和生成过程中，扩展了抽取式（TextRank）和生成式（UniLM）摘要方法。

Result: 该框架提升了对情感细微差别和主题相关性的捕捉能力，生成了简洁且富有情感信息的摘要。

Conclusion: 在信息系统的社交媒体文本分析中，将情感建模与摘要技术相结合是非常必要的，能够显著提高文本挖掘的战略价值。

Abstract: With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.

</details>


### [17] [Step-DeepResearch Technical Report](https://arxiv.org/abs/2512.20491)
*Chen Hu,Haikuo Du,Heng Wang,Lin Lin,Mingrui Chen,Peng Liu,Ruihang Miao,Tianchi Yue,Wang You,Wei Ji,Wei Yuan,Wenjin Deng,Xiaojian Yuan,Xiaoyun Zhang,Xiangyu Liu,Xikai Liu,Yanming Xu,Yicheng Cao,Yifei Zhang,Yongyao Wang,Yubo Shu,Yurong Zhang,Yuxiang Zhang,Zheng Gong,Zhichao Chang,Binyan Li,Dan Ma,Furong Jia,Hongyuan Wang,Jiayu Liu,Jing Bai,Junlan Liu,Manjiao Liu,Na Wang,Qiuping Wu,Qinxin Du,Shiwei Li,Wen Sun,Yifeng Gong,Yonglin Chen,Yuling Zhao,Yuxuan Lin,Ziqi Ren,Zixuan Wang,Aihu Zhang,Brian Li,Buyun Ma,Kang An,Li Xie,Mingliang Li,Pan Li,Shidong Yang,Xi Chen,Xiaojia Liu,Yuchu Luo,Yuan Song,YuanHao Ding,Yuanwei Liang,Zexi Li,Zhaoning Zhang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Jiansheng Chen,Jing Li,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CL

TL;DR: 本文推出 Step-DeepResearch 智能体，通过原子能力数据合成和渐进式训练，使 32B 规模的模型在深度研究任务上达到了顶尖闭源模型水平，并同步发布了中文研究基准 ADR-Bench。


<details>
  <summary>Details</summary>
Motivation: 现有的学术基准无法满足现实世界对自主智能体在开放式研究中的需求，尤其是在意图识别、长程决策和跨源验证方面存在短板，且中文领域缺乏真实的深度研究评价体系。

Method: 1) 提出基于原子能力的训练数据合成策略；2) 采用从 Agent 级中期预训练到 SFT 再到 RL 的渐进式训练路径；3) 引入清单式评分器（Checklist-style Judger）；4) 构建中文深度研究基准 ADR-Bench。

Result: Step-DeepResearch (32B) 在 Scale AI 研究评分中获得 61.4% 的高分，在 ADR-Bench 上显著优于同类模型，性能可比肩 OpenAI 和 Gemini 的深度研究系统。

Conclusion: 经过精细化训练的中型模型（如 32B）能够在极高的成本效益下，达到甚至超越顶级闭源模型（如 OpenAI/Gemini DeepResearch）的专家级深度研究能力。

Abstract: As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.

</details>


### [18] [Distilling to Hybrid Attention Models via KL-Guided Layer Selection](https://arxiv.org/abs/2512.20569)
*Yanhong Li,Songlin Yang,Shawn Tan,Mayank Mishra,Rameswar Panda,Jiawei Zhou,Yoon Kim*

Main category: cs.CL

TL;DR: 本文提出一种基于层重要性分数的高效方法，将预训练模型部分转换为线性注意力层，在降低推理开销的同时保持更佳性能。


<details>
  <summary>Details</summary>
Motivation: 将预训练的 Softmax Transformer 蒸馏为包含线性注意力的混合架构可提升推理效率，但目前缺乏一种简单高效的方法来确定哪些层应被转化为线性注意力层。

Method: 使用少量通用文本训练计算出的“层重要性分数”来指导层选择，随后采用 RADLADS 流水线进行蒸馏（包括权重转移、状态对齐、KL 散度匹配及微调）。

Result: 该方法在选择转化层方面优于现有的固定比例均匀交替等启发式方法，也优于依赖专用诊断数据集的复杂方法。

Conclusion: 在保持语言模型性能的同时，通过层重要性进行结构选择是优化 LLM 推理效率的一种比简单启发式或复杂诊断方法更有效的路径。

Abstract: Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.

</details>


### [19] [Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits](https://arxiv.org/abs/2512.20578)
*Amirhosein Ghasemabadi,Di Niu*

Main category: cs.CL

TL;DR: Gnosis是一个轻量级的内部状态解码器，通过观察LLM的隐藏状态而非通过文本生成，以极低的成本实现了优于外部大模型的自我错误预测能力。


<details>
  <summary>Details</summary>
Motivation: LLM常出现幻觉且难以识别自身错误，而现有的外部评判、多样本一致性或自我批判方法计算成本高，且与真实正确性的相关性较弱。作者试图探索LLM是否由于其内部状态预测自身的失败。

Method: 提出了Gnosis机制：这是一个轻量级（约5M参数）的自知之明模块，通过解码冻结LLM的隐藏状态和注意力模式，将内部轨迹压缩为固定维度的描述符，从而预测输出的正确性。

Result: Gnosis在数学推理、问答等任务中优于强内部基线和大型外部判别器；它具有良好的校准性（calibration），并能零样本泛化至部分生成的文本，实现错误的早期检测。

Conclusion: 可靠的正确性信号内在存在于LLM的生成过程中，通过解析内部推理状态（非外部监督）可以实现高效、准确的模型自我评估。

Abstract: Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.

</details>


### [20] [Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs](https://arxiv.org/abs/2512.20595)
*Dhruv Anand,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 本文提出 Cube Bench，通过魔方挑战系统性评估多模态大模型的空间与多步规划能力，发现当前顶尖模型在复杂空间推理任务中仍面临严峻挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLMs 评估缺乏针对空间推理和复杂顺序决策能力的深度测试，魔方任务固有的结构性和逻辑深度使其成为评估这些能力的理想物理模型。

Method: 引入 Cube Bench 基准，将魔方解题任务分解为五项核心技能（重建、动作选择、结果预测、多步规划和错误修正），在不同打乱深度下对七种 MLLMs 进行定量评估。

Result: 模型准确率随魔方打乱深度增加而剧降；闭源模型在感知和控制任务上明显优于开源模型；高重建准确率不代表能做出正确决策；反思性思维虽有帮助，但也可能造成“过度思考”。

Conclusion: Cube Bench 是衡量 MLLMs 空间与顺序推理能力的紧凑且可重复的基准，其结果表明 MLLMs 在该领域的性能与人类水平及理想状态仍有显著差距。

Abstract: We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.

</details>


### [21] [MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts](https://arxiv.org/abs/2512.20604)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: MoE-DiffuSeq 通过 MoE 架构、稀疏注意力和改进的扩散机制，显著提升了扩散模型在生成长文档时的效率、速度和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本扩散模型（如 DiffuSeq）在处理长文档生成时，面临计算成本过高和内存开销巨大的挑战。

Method: 结合了混合专家（MoE）架构与稀疏注意力机制（Sparse Attention），并引入了软吸收状态（Soft Absorbing State）来优化扩散过程中的序列重构。

Result: 与现有模型相比，MoE-DiffuSeq 显著提升了训练效率和采样速度，同时在长文档生成的准确性和表现力上取得了更好的基准测试结果。

Conclusion: MoE-DiffuSeq 增强了扩散模型在处理长文本时的实用性，在跨领域（科研、代码、对话）的长文本生成任务中具有高度的普适性。

Abstract: We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [22] [When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version)](https://arxiv.org/abs/2512.20457)
*Marco Aruta,Francesco Improta,Vadim Malvone,Aniello Murano*

Main category: cs.MA

TL;DR: 本文提出了一种新逻辑 HumanATLF，通过引入模糊语义和动作资源限制，改进了多智能体系统中对人类决策行为的建模与验证。


<details>
  <summary>Details</summary>
Motivation: 传统的多智能体系统策略推理通常假设智能体具有无限复杂的策略、执行动作零成本且环境信息完全精确，这与现实中受资源限制且存在模糊感知的人类决策不符。

Method: 提出了 HumanATLF 逻辑，并结合了资源界限（非充填预算）和模糊语义（[0,1] 赋值）。此外，在开源工具 VITAMIN 中实现了模型检测算法，并在无人机救援场景中进行了验证。

Result: 证明了模型检测的复杂性：在策略复杂度和资源预算固定时为 P，单个布尔目标下为 NP-complete，变量变化时为 $\Delta^P_2$-complete，召回策略为 PSPACE。实验验证了算法在对抗性资源感知场景下的有效性。

Conclusion: HumanATLF 成功填补了 MAS 逻辑中关于人类行为建模的空白，通过将资源约束与模糊逻辑结合，提供了一个更符合现实世界复杂性的形式化验证框架。

Abstract: In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [23] [Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning](https://arxiv.org/abs/2512.19777)
*Antonio Tarizzo,Mohammad Kazemi,Deniz Gündüz*

Main category: cs.IT

TL;DR: 本文通过结合矢量量化和端到端学习的展开AMP解码器，提升了联邦学习中数字空中计算在低信噪比下的鲁棒性和聚合精度。


<details>
  <summary>Details</summary>
Motivation: 联邦边缘学习（FEEL）面临通信瓶颈，传统的空中计算（OTA）虽能提高效率，但现有数字OTA方案在低信噪比（SNR）下鲁棒性较差。

Method: 提出一种学习型数字OTA框架，集成非源随机访问（URA）码本、矢量量化以及基于展开近似消息传递（AMP）的AMP-DA-Net解码器，进行端到端训练。

Result: 实验显示该设计在低信噪比区域将可靠运行范围提升了10 dB以上，且能支持包括修剪均值和多数规则在内的多种对称聚合函数。

Conclusion: 端到端学习的设计显著提升了数字空中计算在FEEL中的性能，在保持通信效率的同时，极大地增强了对低信噪比环境和非线性聚合规则的鲁棒性。

Abstract: Federated edge learning (FEEL) enables wireless devices to collaboratively train a centralised model without sharing raw data, but repeated uplink transmission of model updates makes communication the dominant bottleneck. Over-the-air (OTA) aggregation alleviates this by exploiting the superposition property of the wireless channel, enabling simultaneous transmission and merging communication with computation. Digital OTA schemes extend this principle by incorporating the robustness of conventional digital communication, but current designs remain limited in low signal-to-noise ratio (SNR) regimes. This work proposes a learned digital OTA framework that improves recovery accuracy, convergence behaviour, and robustness to challenging SNR conditions while maintaining the same uplink overhead as state-of-the-art methods. The design integrates an unsourced random access (URA) codebook with vector quantisation and AMP-DA-Net, an unrolled approximate message passing (AMP)-style decoder trained end-to-end with the digital codebook and parameter server local training statistics. The proposed design extends OTA aggregation beyond averaging to a broad class of symmetric functions, including trimmed means and majority-based rules. Experiments on highly heterogeneous device datasets and varying numbers of active devices show that the proposed design extends reliable digital OTA operation by more than 10 dB into low SNR regimes while matching or improving performance across the full SNR range. The learned decoder remains effective under message corruption and nonlinear aggregation, highlighting the broader potential of end-to-end learned design for digital OTA communication in FEEL.

</details>


### [24] [Generative Bayesian Spectrum Cartography: Unified Reconstruction and Active Sensing via Diffusion Models](https://arxiv.org/abs/2512.20108)
*Yuntong Gu,Xiangming meng,Zhiyuan Lin,Sheng Wu,Linling Kuang*

Main category: cs.IT

TL;DR: 本文提出了一个统一的扩散式贝叶斯框架，通过学习先验和不确定性引导的主动采样，同步解决了频谱重建与高效感知难题。


<details>
  <summary>Details</summary>
Motivation: 频谱制图面临观测稀疏且不规则的逆问题挑战，且现有研究往往将重建与传感解耦，缺乏有效的主动采样机制。

Method: 提出了基于扩散模型的贝叶斯框架，推导了闭式后验转移核以处理线性和非线性量化测量，并开发了基于不确定性的主动采样策略。

Result: 实验证明，该框架在重建精度、采样效率以及对低比特量化的鲁棒性方面均显著优于现有的插值、稀疏表示及深度学习基准方法。

Conclusion: 该框架为高保真频谱制图提供了一种稳健且高效的统一解决方案，特别适用于受限传感和量化观测环境。

Abstract: High-fidelity spectrum cartography is pivotal for spectrum management and wireless situational awareness, yet it remains a challenging ill-posed inverse problem due to the sparsity and irregularity of observations. Furthermore, existing approaches often decouple reconstruction from sensing, lacking a principled mechanism for informative sampling. To address these limitations, this paper proposes a unified diffusion-based Bayesian framework that jointly addresses spectrum reconstruction and active sensing. We formulate the reconstruction task as a conditional generation process driven by a learned diffusion prior. Specifically, we derive tractable, closed-form posterior transition kernels for the reverse diffusion process, which enforce consistency with both linear Gaussian and non-linear quantized measurements. Leveraging the intrinsic probabilistic nature of diffusion models, we further develop an uncertainty-aware active sampling strategy. This strategy quantifies reconstruction uncertainty to adaptively guide sensing agents toward the most informative locations, thereby maximizing spectral efficiency. Extensive experiments demonstrate that the proposed framework significantly outperforms state-of-the-art interpolation, sparsity-based, and deep learning baselines in terms of reconstruction accuracy, sampling efficiency, and robustness to low-bit quantization.

</details>


### [25] [RIS-Empowered OTFS Modulation With Faster-than-Nyquist Signaling in High-Mobility Wireless Communications](https://arxiv.org/abs/2512.20332)
*Chaorong Zhang,Benjamin K. Ng,Hui Xu,Chan-Tong Lam,Halim Yanikomeroglu*

Main category: cs.IT

TL;DR: 本文提出一种集成可重构智能表面（RIS）的超奈奎斯特（FTN）调制OTFS系统，旨在通过联合优化提升高速移动环境下的频谱效率和通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 高速移动环境下的多普勒频移和多径延迟会降低传统调制性能。旨在结合OTFS的鲁棒性、FTN的高频谱效率以及RIS的信道改善能力。

Method: 提出RIS-OTFS-FTN方案，建立统一的延迟-多普勒（DD）域输入输出模型，涵盖RIS波束赋形和FTN造成的干扰；设计量化相位选择策略，并进行理论性能分析与蒙特卡罗仿真。

Result: 验证了理论分析的准确性，展示了该方案在提高可靠性和频谱效率方面的显著增益，并权衡了PAPR、IBO与误差性能。

Conclusion: 结合RIS、OTFS和FTN的方案在可靠性和频谱效率方面表现优异，是未来高速移动和频谱受限无线通信系统的可行解决方案。

Abstract: High-mobility wireless communication systems suffer from severe Doppler spread and multi-path delay, which degrade the reliability and spectral efficiency of conventional modulation schemes. Orthogonal time frequency space (OTFS) modulation offers strong robustness in such environments by representing symbols in the delay-Doppler (DD) domain, while faster-than-Nyquist (FTN) signaling can further enhance spectral efficiency through intentional symbol packing. Meanwhile, reconfigurable intelligent surfaces (RIS) provide a promising means to improve link quality via passive beamforming. Motivated by these advantages, we propose a novel RIS-empowered OTFS modulation with FTN signaling (RIS-OTFS-FTN) scheme. First, we establish a unified DD-domain input-output relationship that jointly accounts for RIS passive beamforming, FTN-induced inter-symbol interference, and DD-domain channel characteristics. Based on this model, we provide comprehensive analytical performance for the frame error rate, spectral efficiency, and peak-to-average power ratio (PAPR), etc. Furthermore, a practical RIS phase adjustment strategy with quantized phase selection is designed to maximize the effective channel gain. Extensive Monte Carlo simulations under a standardized extended vehicular A (EVA) channel model validate the theoretical results and provide key insights into the trade-offs among spectral efficiency, PAPR, input back-off (IBO), and error performance, with some interesting insights.The proposed RIS-OTFS-FTN scheme demonstrates notable performance gains in both reliability and spectral efficiency, offering a viable solution for future high-mobility and spectrum-constrained wireless systems.

</details>


### [26] [Viterbi State Selection for Discrete Pinching Antenna Systems](https://arxiv.org/abs/2512.20389)
*Victoria E. Galanopoulou,Thrassos K. Oikonomou,Odysseas G. Karagiannidis,Sotiris A. Tegos,Panagiotis D. Diamantoulakis*

Main category: cs.IT

TL;DR: 本文针对夹持天线阵列提出了一种基于 Viterbi 的多项式复杂度算法，在保证最优性能的前提下解决了天线子集选择的组合优化难题。


<details>
  <summary>Details</summary>
Motivation: 在波导馈电夹持天线阵列中，最优天线子集选择是一个具有指数复杂度的组合优化问题，且可达速率对激活天线的相位对齐高度敏感。

Method: 提出 Viterbi 状态选择（VSS）算法。该算法利用累积复增益的量子化相位定义格点状态，并采用 Viterbi 生存准则在各阶段修建受支配的天线子集。

Result: 数值结果表明，该方法在获得与穷举搜索相同的天线选择结果和速率性能的同时，显著降低了计算开销。

Conclusion: VSS 算法成功将天线子集选择的计算复杂度从指数级降低到多项式级。

Abstract: Pinching antennas enable dynamic control of electromagnetic wave propagation through reconfigurable radiating structures, but selecting an optimal subset of antennas remains a combinatorial problem with exponential complexity. This letter considers antenna subset selection for a waveguide-fed pinching antenna array serving ground users under a time-division access scheme. The achievable rate depends on the coherent superposition of the effective complex channel gains and is therefore highly sensitive to the relative phase alignment of the activated antennas. To address the prohibitive complexity of exhaustive search, we propose a Viterbi state selection (VSS) algorithm that exploits the phase structure of the combined received signal. The trellis state is defined by a quantized representation of the phase of the accumulated complex gain, and a Viterbi-based survivor rule is used to prune dominated antenna subsets across stages. Numerical results demonstrate that the proposed method achieves the same antenna selection and rate as exhaustive search, while reducing the computational complexity from exponential to polynomial in the number of available antennas.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [27] [Community Notes: Crowd Participation and Dependence on Professional Fact-Checking Across Languages](https://arxiv.org/abs/2512.19947)
*Elizabeth Stewart,Suryash Greenwold,Timotius Marselo*

Main category: cs.SI

TL;DR: 本研究探讨了 X 平台的 Community Notes 在不同语言社区的应用现状，发现其全球推广存在参与不均的问题，且志愿者极少利用专业事实核查资源。


<details>
  <summary>Details</summary>
Motivation: 众包事实核查（如 Community Notes）被视为治理虚假信息的良方，但既往研究多集中于英语语境，缺乏对不同语言社区参与情况及其对专业核查资源依赖程度的跨语言研究。

Method: 通过分析 X 平台 Community Notes 中不同语言社区的参与数据，对比志愿者对专业事实核查资源的使用频率及不同语言下的表现差异。

Result: 1. 不同语言社区的参与度差异巨大（部分社区活跃，部分几乎无进展）；2. 引用专业核查机构的笔记被认为更有帮助，但整体上志愿者对专业核查资源的引用比例依然很低。

Conclusion: Community Notes 对专业核查机构的依赖程度依然较低，其在全球范围内的普遍适用性仍面临语言参与度不均的挑战。

Abstract: Crowd-sourced fact-checking provides social media platforms with a promising method of managing misinformation at scale. However, the success of fact-checking programs like X's Community Notes requires the participation of a critical mass of note-writers who have the time and epistemic resources necessary to write and rate high-quality notes. As X's Community Notes program was first established in English-speaking countries, much academic research has focused on English-language notes or notes writ large. Relatively little research has investigated how different linguistic communities utilise Community Notes. Thus, it is unclear whether Community Notes or similar crowd-sourced fact-checking initiatives represent a viable alternative to social media platforms' partnerships with professional fact-checking organisations across linguistic contexts. This research identifies how different linguistic communities participate in volunteer fact-checking efforts on X's Community Notes program and addresses volunteers' reliance on professional fact-checking resources differs across languages. We find that while the Community Notes program has had strong uptake in some linguistic communities, the program has failed to catch on in others. Additionally, we confirm findings that notes that cite professional fact-checkers are considered more helpful, but show that reliance on professional fact-checking overall remains minimal.

</details>


### [28] [Evaluating Moderation in Online Social Network](https://arxiv.org/abs/2512.20225)
*Letizia Milli,Laura Pollacci,Riccardo Guidotti*

Main category: cs.SI

TL;DR: 本文提出一个基于SEIZ模型的在线有毒内容传播模拟器，通过对比实验证明，相比于统一拦截，基于用户心理特征的个性化监管能更有效地抑制有害信息的扩散。


<details>
  <summary>Details</summary>
Motivation: 在线平台有毒内容的传播带来了复杂挑战，需要理论见解和实践工具来测试不同的干预策略。

Method: 扩展经典的SEIZ（易感-暴露-感染-怀疑）流行病模型，开发了一个包含两种监管机制的模拟框架：执行统一干预的“基础监管者”和基于用户“暗黑人格三项”（Dark Triad）特征进行个性化干预的“智能监管者”。

Result: 模拟结果显示，虽然通用干预在某些条件下能遏制毒性，但基于用户画像的个性化监管在限制有毒行为的传播和持续性方面显著更有效。

Conclusion: 该模拟框架为研究和设计复杂在线社交系统中的自适应监管策略提供了一个灵活且可扩展的工具。

Abstract: The spread of toxic content on online platforms presents complex challenges that call for both theoretical insight and practical tools to test intervention strategies. In this novel research paper, we introduce a simulation-based framework that extends the classical SEIZ (Susceptible-Exposed-Infected-Skeptic) epidemic model to capture the dynamics of toxic message propagation. Our simulator incorporates active moderation mechanisms through two distinct variants: a basic moderator, which implements uniform, non-personalized interventions, and smart moderator, which leverages user-specific psychological profiles based on Dark Triad traits to apply personalized, threshold-driven moderation. By varying parameter configurations, the simulator allows for systematic exploration of how different moderation strategies influence user state transitions over time. Simulation results demonstrate that while generic interventions can curb toxicity under certain conditions, profile-aware moderation proves significantly more effective in limiting both the spread and persistence of toxic behavior. This simulation framework offers a flexible and extensible tool for studying and designing adaptive moderation strategies in complex online social systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [29] [Robust Causal Directionality Inference in Quantum Inference under MNAR Observation and High-Dimensional Noise](https://arxiv.org/abs/2512.19746)
*Joonsung Kang*

Main category: stat.ML

TL;DR: 本文提出一种结合CVAE和GEE的统一稳健因果推断框架，解决了量子工程中受MNAR干扰的系统与观测间的因果方向识别问题。


<details>
  <summary>Details</summary>
Motivation: 量子力学中观测会改变系统的特性与统计学中的非随机缺失（MNAR）相似，需要一种统一框架来识别系统与观测之间的鲁棒因果方向。

Method: 集成CVAE（条件变分自编码器）潜在约束、MNAR感知选择模型、GEE（广义估计方程）稳健回归、惩罚经验似然（PEL）及贝叶斯优化。

Result: 在模拟和真实数据（如TCGA基因、蛋白质组学）中，该框架在偏差、方差、覆盖率及量子特定指标上均表现优于传统方法。

Conclusion: 该框架为量子工程中的因果推断建立了新的标准，确保了在复杂观测环境下的可靠性和稳健性。

Abstract: In quantum mechanics, observation actively shapes the system, paralleling the statistical notion of Missing Not At Random (MNAR). This study introduces a unified framework for \textbf{robust causal directionality inference} in quantum engineering, determining whether relations are system$\to$observation, observation$\to$system, or bidirectional.
  The method integrates CVAE-based latent constraints, MNAR-aware selection models, GEE-stabilized regression, penalized empirical likelihood, and Bayesian optimization. It jointly addresses quantum and classical noise while uncovering causal directionality, with theoretical guarantees for double robustness, perturbation stability, and oracle inequalities.
  Simulation and real-data analyses (TCGA gene expression, proteomics) show that the proposed MNAR-stabilized CVAE+GEE+AIPW+PEL framework achieves lower bias and variance, near-nominal coverage, and superior quantum-specific diagnostics. This establishes robust causal directionality inference as a key methodological advance for reliable quantum engineering.

</details>


### [30] [Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function](https://arxiv.org/abs/2512.19913)
*Matthew Drnevich,Stephen Jiggins,Kyle Cranmer*

Main category: stat.ML

TL;DR: 本文提出了一种新的凸损失函数和扩展切片沃瑟斯坦距离，旨在解决含负值的拟概率密度比估计难题，并在粒子物理应用中取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统的基于分类器的密度比估计任务在处理可能存在负值的“拟概率（Quasiprobabilistic）”分布时，常用的损失函数会导致最优分类器与目标密度比之间关系不连续或非满射。

Method: 引入了一种全新的凸损失函数（Convex Loss Function），并提出了一种兼容拟概率分布的扩展版切片沃瑟斯坦距离（Sliced-Wasserstein distance）作为性能评估指标。

Result: 在粒子物理学（胶子-胶子融合产生双希格斯玻色子伴随喷注）的真实世界案例中，该方法达到了当前最先进（SOTA）的水平。

Conclusion: 该方法解决了拟概率密度比估计中的不连续性和非满切性问题，为物理学等领域的拟概率分布分析提供了稳健的数学工具。

Abstract: We consider a generalization of the classifier-based density-ratio estimation task to a quasiprobabilistic setting where probability densities can be negative. The problem with most loss functions used for this task is that they implicitly define a relationship between the optimal classifier and the target quasiprobabilistic density ratio which is discontinuous or not surjective. We address these problems by introducing a convex loss function that is well-suited for both probabilistic and quasiprobabilistic density ratio estimation. To quantify performance, an extended version of the Sliced-Wasserstein distance is introduced which is compatible with quasiprobability distributions. We demonstrate our approach on a real-world example from particle physics, of di-Higgs production in association with jets via gluon-gluon fusion, and achieve state-of-the-art results.

</details>


### [31] [Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing](https://arxiv.org/abs/2512.20007)
*Zhihan Huang,Ziang Niu*

Main category: stat.ML

TL;DR: 本文通过建立得分检验与积分概率度量（IPM）之间的等效关系，提出了一种基于核Stein差异的非参数得分检验方法（SKSD），该方法计算高效、适用于复杂模型且具有极高的统计效能。


<details>
  <summary>Details</summary>
Motivation: 传统的拟合优度得分检验仅需拟合一次原假设模型，但难以扩展到强大的非参数对立假设；而非参数检验则缺乏合适的得分函数。研究旨在结合两者的优点，开发一种既具有非参数一致性又计算高效的得分检验方法。

Method: 通过指数倾斜模型（exponentially tilted models）证明得分检验等价于基于积分概率度量（IPM）的检验。在此基础上，利用核化Stein函数类诱导的一种特殊IPM，提出了半参数核Stein差异（SKSD）检验，并配合通用参数化自助法（bootstrap）。

Result: SKSD检验在计算上非常高效，支持通用的干扰参数估计，具有全样本一致性和皮特曼效率（Pitman efficiency）。实验表明，它在处理不可积似然模型时表现出色，且在正态性检验等任务中达到了与专用检验（如AD检验）相当的效能。

Conclusion: SKSD检验通过得分函数构建，成功地弥合了基于得分的检验与基于距离的检验之间的理论差距，为统计模型验证提供了一个通用且强有力的工具。

Abstract: Goodness-of-fit (GoF) tests are fundamental for assessing model adequacy. Score-based tests are appealing because they require fitting the model only once under the null. However, extending them to powerful nonparametric alternatives is difficult due to the lack of suitable score functions. Through a class of exponentially tilted models, we show that the resulting score-based GoF tests are equivalent to the tests based on integral probability metrics (IPMs) indexed by a function class. When the class is rich, the test is universally consistent. This simple yet insightful perspective enables reinterpretation of classical distance-based testing procedures-including those based on Kolmogorov-Smirnov distance, Wasserstein-1 distance, and maximum mean discrepancy-as arising from score-based constructions. Building on this insight, we propose a new nonparametric score-based GoF test through a special class of IPM induced by kernelized Stein's function class, called semiparametric kernelized Stein discrepancy (SKSD) test. Compared with other nonparametric score-based tests, the SKSD test is computationally efficient and accommodates general nuisance-parameter estimators, supported by a generic parametric bootstrap procedure. The SKSD test is universally consistent and attains Pitman efficiency. Moreover, SKSD test provides simple GoF tests for models with intractable likelihoods but tractable scores with the help of Stein's identity and we use two popular models, kernel exponential family and conditional Gaussian models, to illustrate the power of our method. Our method achieves power comparable to task-specific normality tests such as Anderson-Darling and Lilliefors, despite being designed for general nonparametric alternatives.

</details>


### [32] [Generative Bayesian Hyperparameter Tuning](https://arxiv.org/abs/2512.20051)
*Hedibert Lopes,Nick Polson,Vadim Sokolov*

Main category: stat.ML

TL;DR: 本文提出了一种生成式超参数微调框架，通过学习一个从超参数到模型参数的映射函数，实现了超参数评估的快速“在线查找”和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 在现代机器学习中，传统的超参数选择方法（如交叉验证）计算成本极高，而全贝叶斯推断又因后验采样的复杂性难以在大规模场景下应用。

Method: 核心结合了两项技术：(1) 利用加权贝叶斯自助法（Weighted Bayesian Bootstrap）通过随机加权目标函数对后验分布进行优化近似；(2) 摊销优化（Amortized Optimization），学习一个从超参数和权重到对应最优解的运输映射（Transport Map）。

Result: 构建了一个“生成器查找表”模型，使得在连续超参数范围内进行实时评估和预测调整成为可能，并能同时提供贝叶斯不确定性量化。

Conclusion: 该框架将超参数选择从一个昂贵的迭代优化过程转变为一个高效的函数预测过程，通过生成模型实现了对估计器及其不确定性的快速查询。

Abstract: \noindent Hyper-parameter selection is a central practical problem in modern machine learning, governing regularization strength, model capacity, and robustness choices. Cross-validation is often computationally prohibitive at scale, while fully Bayesian hyper-parameter learning can be difficult due to the cost of posterior sampling. We develop a generative perspective on hyper-parameter tuning that combines two ideas: (i) optimization-based approximations to Bayesian posteriors via randomized, weighted objectives (weighted Bayesian bootstrap), and (ii) amortization of repeated optimization across many hyper-parameter settings by learning a transport map from hyper-parameters (including random weights) to the corresponding optimizer. This yields a ``generator look-up table'' for estimators, enabling rapid evaluation over grids or continuous ranges of hyper-parameters and supporting both predictive tuning objectives and approximate Bayesian uncertainty quantification. We connect this viewpoint to weighted $M$-estimation, envelope/auxiliary-variable representations that reduce non-quadratic losses to weighted least squares, and recent generative samplers for weighted $M$-estimators.

</details>


### [33] [Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability](https://arxiv.org/abs/2512.20368)
*Samya Praharaj,Koulik Khamaru*

Main category: stat.ML

TL;DR: 本文提出了一种带惩罚项的 EXP4 算法，首次证明了在线性上下文多臂老虎机中可以同时实现“最优累积收益”和“有效的经典统计推断（Wald 置信区间）”，克服了自适应推断中常见的精度损失。


<details>
  <summary>Details</summary>
Motivation: 上下文多臂老虎机中的自适应采样导致数据非独立同分布，传统推断方法往往失效，或者需要付出 $\sqrt{d \log T}$ 的精度代价。研究者旨在寻找一种既能保持累积收益（低 Regret），又能满足 Lai-Wei 稳定性以实现有效统计推断的方法。

Method: 提出并分析了一种针对线性上下文多臂老虎机（Linear Contextual Bandits）的“带惩罚项的 EXP4 算法”（Penalized EXP4 Algorithm）。

Result: 1. 该算法满足 Lai-Wei 稳定性条件，使得经典的 Wald 型置信区间在自适应环境下依然渐近有效，无需支付额外的精度代价；2. 该算法在遗憾界（Regret）上达到了近乎极小化极大（Minimax）的最优性。

Conclusion: 本研究证明了在上下文多臂老虎机中，稳定性和统计效率是可以并存的。通过引入惩罚项，EXP4 算法能够在不牺牲累积收益的情况下，提供具有统计渐近有效性的推断结果。

Abstract: Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\sqrt{d \log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.
  A key structural property that circumvents this limitation is the \emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \emph{without} incurring the $\sqrt{d \log T}$ price of adaptivity.
  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Large Language Models for EDA Cloud Job Resource and Lifetime Prediction](https://arxiv.org/abs/2512.19701)
*Yuxuan Yin,Shengke Zhou,Yunjie Zhang,Ajay Mohindra,Boxun Xu,Peng Li*

Main category: cs.LG

TL;DR: 本文提出一种通过微调大语言模型（LLM）来预测云端 EDA 作业寿命的新框架，通过优化注意力机制和输出约束，在无需复杂特征工程的情况下实现了卓越的预测性能。


<details>
  <summary>Details</summary>
Motivation: 云端 EDA 行业对作业寿命预测的需求日益增长，但传统机器学习方法难以应对工作负载的复杂性与异构性，且过度依赖人工特征工程。

Method: 提出一种微调 LLM 的文本到文本回归框架，引入科学计数法和前缀填充（prefix filling）来增强格式稳定性，并采用全注意力机制（full-attention）优化滑动窗口注意力模型的精度。

Result: 在真实云数据集上表现优异，显著提升了预测准确度，并有效解决了 LLM 在数值回归任务中的格式输出可靠性问题。

Conclusion: 该研究为 EDA 领域提供了一个强大的性能预测新基准，展示了通过大语言模型自动化复杂时序任务的巨大潜力。

Abstract: The rapid growth of cloud computing in the Electronic Design Automation (EDA) industry has created a critical need for resource and job lifetime prediction to achieve optimal scheduling. Traditional machine learning methods often struggle with the complexity and heterogeneity of EDA workloads, requiring extensive feature engineering and domain expertise. We propose a novel framework that fine-tunes Large Language Models (LLMs) to address this challenge through text-to-text regression. We introduce the scientific notation and prefix filling to constrain the LLM, significantly improving output format reliability. Moreover, we found that full-attention finetuning and inference improves the prediction accuracy of sliding-window-attention LLMs. We demonstrate the effectiveness of our proposed framework on real-world cloud datasets, setting a new baseline for performance prediction in the EDA domain.

</details>


### [35] [Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches](https://arxiv.org/abs/2512.19713)
*Taoran Sheng,Manfred Huber*

Main category: cs.LG

TL;DR: 本文系统研究了穿戴式活动识别的多种监督学习范式，提出了一种仅需10%标签即可实现高性能的弱自监督学习框架。


<details>
  <summary>Details</summary>
Motivation: 解决穿戴式人体活动识别（HAR）中全监督学习标注成本高昂与无监督学习性能不佳之间的矛盾。

Method: 对比研究了六种学习范式：(1) 全监督学习；(2) 基础无监督学习；(3) 约束弱监督学习；(4) 多任务学习；(5) 领域专家自监督学习；(6) 结合领域知识与少量标签的创新弱自监督学习。

Result: (1) 弱监督方法在显著减少标注量的同时，性能与全监督相当；(2) 多任务框架通过知识共享提升了性能；(3) 创新的弱自监督框架在仅使用10%标注数据的情况下表现优异。

Conclusion: 多元化的学习范式（尤其是弱监督和自监督）为解决标注数据稀缺问题提供了有效路径，为实际应用中根据数据量定制HAR方案提供了理论与实践依据。

Abstract: Human activity recognition (HAR) using wearable sensors has advanced through various machine learning paradigms, each with inherent trade-offs between performance and labeling requirements. While fully supervised techniques achieve high accuracy, they demand extensive labeled datasets that are costly to obtain. Conversely, unsupervised methods eliminate labeling needs but often deliver suboptimal performance. This paper presents a comprehensive investigation across the supervision spectrum for wearable-based HAR, with particular focus on novel approaches that minimize labeling requirements while maintaining competitive accuracy. We develop and empirically compare: (1) traditional fully supervised learning, (2) basic unsupervised learning, (3) a weakly supervised learning approach with constraints, (4) a multi-task learning approach with knowledge sharing, (5) a self-supervised approach based on domain expertise, and (6) a novel weakly self-supervised learning framework that leverages domain knowledge and minimal labeled data. Experiments across benchmark datasets demonstrate that: (i) our weakly supervised methods achieve performance comparable to fully supervised approaches while significantly reducing supervision requirements; (ii) the proposed multi-task framework enhances performance through knowledge sharing between related tasks; (iii) our weakly self-supervised approach demonstrates remarkable efficiency with just 10\% of labeled data. These results not only highlight the complementary strengths of different learning paradigms, offering insights into tailoring HAR solutions based on the availability of labeled data, but also establish that our novel weakly self-supervised framework offers a promising solution for practical HAR applications where labeled data are limited.

</details>


### [36] [Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data](https://arxiv.org/abs/2512.19716)
*Behrooz Mamandipoor,Chun-Nan Hsu,Martin Krause,Ulrich H. Schmidt,Rodney A. Gabriel*

Main category: cs.LG

TL;DR: 本研究开发并验证了一个整合结构化数据、文本和影像的多模态模型，能有效预测ICU患者入院24小时内的死亡风险。


<details>
  <summary>Details</summary>
Motivation: 早期预测ICU患者的住院死亡率有助于优化治疗决策，但现有模型往往依赖单一数据源，缺乏多模态信息的整合及广泛的外部验证。

Method: 使用MIMIC-III/IV、eICU和HiRID数据集（共203,434人次），开发了一种整合时间序列（定值与变值）、临床笔记和胸部X光图像的多模态深度学习模型。

Result: 模型在仅使用结构化数据时AUROC达到0.92；加入临床笔记和影像数据后，AUROC从0.87提升至0.89，Brier评分由0.37降至0.17；在8家外部机构验证中AUROC保持在0.84-0.92。

Conclusion: 结合多模态数据并进行多中心外部验证，对于开发稳健且临床适用的重症死亡风险预测模型至关重要。

Abstract: Early prediction of in-hospital mortality in critically ill patients can aid clinicians in optimizing treatment. The objective was to develop a multimodal deep learning model, using structured and unstructured clinical data, to predict in-hospital mortality risk among critically ill patients after their initial 24 hour intensive care unit (ICU) admission. We used data from MIMIC-III, MIMIC-IV, eICU, and HiRID. A multimodal model was developed on the MIMIC datasets, featuring time series components occurring within the first 24 hours of ICU admission and predicting risk of subsequent inpatient mortality. Inputs included time-invariant variables, time-variant variables, clinical notes, and chest X-ray images. External validation occurred in a temporally separated MIMIC population, HiRID, and eICU datasets. A total of 203,434 ICU admissions from more than 200 hospitals between 2001 to 2022 were included, in which mortality rate ranged from 5.2% to 7.9% across the four datasets. The model integrating structured data points had AUROC, AUPRC, and Brier scores of 0.92, 0.53, and 0.19, respectively. We externally validated the model on eight different institutions within the eICU dataset, demonstrating AUROCs ranging from 0.84-0.92. When including only patients with available clinical notes and imaging data, inclusion of notes and imaging into the model, the AUROC, AUPRC, and Brier score improved from 0.87 to 0.89, 0.43 to 0.48, and 0.37 to 0.17, respectively. Our findings highlight the importance of incorporating multiple sources of patient information for mortality prediction and the importance of external validation.

</details>


### [37] [Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference](https://arxiv.org/abs/2512.19717)
*Zhan Zhang*

Main category: cs.LG

TL;DR: 本文提出 ICFA 框架，通过目标条件的自适应重加权技术，显著提升了在超大规模搜索空间中寻找稀有解的效率。


<details>
  <summary>Details</summary>
Motivation: 在语言生成、规划和强化学习等超大规模候选空间中，寻找稀少但有用的解决方案非常困难，传统方法往往效率低下。

Method: 提出反向因果聚焦算法（ICFA），利用现有采样器和相似度函数构建目标条件重加权分布，并通过自适应控制聚焦强度以维持采样稳定性（基于有效样本量 ESS）。

Result: 在约束语言生成和稀疏奖励导航两个实验中证明了其优越性，并展示了如何通过结构化提示词（Structured Prompts）实现 ICFA 的语言级近似及混合架构。

Conclusion: ICFA 是一种实用且具有普适性的采样增强框架，通过在大规模空间中实现目标导向的重加权，解决了罕见有效解的搜索难题。

Abstract: Finding rare but useful solutions in very large candidate spaces is a recurring practical challenge across language generation, planning, and reinforcement learning. We present a practical framework, \emph{Inverted Causality Focusing Algorithm} (ICFA), that treats search as a target-conditioned reweighting process. ICFA reuses an available proposal sampler and a task-specific similarity function to form a focused sampling distribution, while adaptively controlling focusing strength to avoid degeneracy. We provide a clear recipe, a stability diagnostic based on effective sample size, a compact theoretical sketch explaining when ICFA can reduce sample needs, and two reproducible experiments: constrained language generation and sparse-reward navigation. We further show how structured prompts instantiate an approximate, language-level form of ICFA and describe a hybrid architecture combining prompted inference with algorithmic reweighting.

</details>


### [38] [Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals](https://arxiv.org/abs/2512.19721)
*Vineet Yadav*

Main category: cs.LG

TL;DR: 本文扩展了Jaccard/Tanimoto相似度框架，使其能够处理实值和复值信号，并提供了具备度量性质、核结构及可解释分解能力的信号分析工具。


<details>
  <summary>Details</summary>
Motivation: 传统的Jaccard/Tanimoto相似度仅限于非负向量，无法直接应用于包含实值或复值的通用信号，且缺乏能够同时提供度量性质、核函数结构和透明预算核算的统一框架。

Method: 将实值或复值信号嵌入到非负的多状态表示中（如正负拆分、笛卡尔或极坐标分解），并在这些嵌入上应用广义Jaccard重叠和Tanimoto构造，同时结合Möbius逆转进行联盟分析。

Result: 推导出一系列[0,1]范围内的距离度量，证明其满足三角不等式并定义了正定核。该方法不仅计算相似度，还能通过联盟分析将信号强度进行无损的加性分解。

Conclusion: 该框架为处理具有正负或复数属性的复杂信号提供了一个统一且具有高度机械解释性的数学标准，适用于科学和金融领域。

Abstract: We introduce a sign-aware, multistate Jaccard/Tanimoto framework that extends overlap-based distances from nonnegative vectors and measures to arbitrary real- and complex-valued signals while retaining bounded metric and positive-semidefinite kernel structure. Formally, the construction is a set- and measure-theoretic geometry: signals are represented as atomic measures on a signed state space, and similarity is given by a generalized Jaccard overlap of these measures. Each signal is embedded into a nonnegative multistate representation, using positive/negative splits for real signals, Cartesian and polar decompositions for complex signals, and user-defined state partitions for refined regime analysis. Applying the Tanimoto construction to these embeddings yields a family of $[0,1]$ distances that satisfy the triangle inequality and define positive-semidefinite kernels usable directly in kernel methods and graph-based learning. Beyond pairwise distances, we develop coalition analysis via Möbius inversion, which decomposes signal magnitude into nonnegative, additive contributions with exact budget closure across coalitions of signals. Normalizing the same embeddings produces probability measures on coordinate -- state configurations, so that the distance becomes a monotone transform of total variation and admits a regime -- intensity decomposition. The resulting construction yields a single, mechanistically interpretable distance that simultaneously provides bounded metric structure, positive-semidefinite kernels, probabilistic semantics, and transparent budget accounting within one sign-aware framework, supporting correlograms, feature engineering, similarity graphs, and other analytical tools in scientific and financial applications.

</details>


### [39] [Per-Axis Weight Deltas for Frequent Model Updates](https://arxiv.org/abs/2512.19720)
*Stefan Kuyumdzhiev,Radostin Cholakov*

Main category: cs.LG

TL;DR: 本文提出一种 1-bit 轴级缩放权重增量压缩方案，通过仅存储 Delta 符号和轻量级缩放因子，极大缩减了微调 LLM 的存储体积并降低了冷启动延迟。


<details>
  <summary>Details</summary>
Motivation: 许多特定任务的 LLM 变体模型由于微调权重大、冷启动延迟高，导致部署成本高昂且受限。由于微调权重与基座模型差异呈现结构化残差，可通过压缩 Delta 来优化。

Method: 提出一种 1-bit Delta 方案：仅存储权重差值的符号（Sign），并结合从少量校准集中学习到的轻量级“每轴（行/列）FP16 缩放因子”。系统层面通过流式加载器实现 packed deltas 的单次操作传输。

Result: 与标量量化方案相比，轴级缩放提高了重建质量；模型产物比原始 FP16 检查点小多倍，显著降低了冷启动延迟和存储开销，且无需进行复杂的密集重建。

Conclusion: 该方案证明了 1-bit 权重增量在保持推理效率的同时，通过简单的轴级量化即可显著降低大模型微调版本的存储和部署成本。

Abstract: Serving many task-specialized LLM variants is often limited by the large size of fine-tuned checkpoints and the resulting cold-start latency. Since fine-tuned weights differ from their base model by relatively small structured residuals, a natural approach is to represent them as compressed deltas. We propose a simple 1-bit delta scheme that stores only the sign of the weight difference together with lightweight per-axis (row/column) FP16 scaling factors, learned from a small calibration set. This design preserves the compactness of 1-bit deltas while more accurately capturing variation across weight dimensions, leading to improved reconstruction quality over scalar alternatives. From a systems perspective, a streamlined loader that transfers packed deltas in a single operation per module reduces cold-start latency and storage overhead, with artifacts several times smaller than a full FP16 checkpoint. The method is drop-in, requires minimal calibration data, and maintains inference efficiency by avoiding dense reconstruction. Our experimental setup and source code are available at https://github.com/kuiumdjiev/Per-Axis-Weight-Deltas-for-Frequent-Model-Updates.

</details>


### [40] [Information-directed sampling for bandits: a primer](https://arxiv.org/abs/2512.20096)
*Annika Hirling,Giorgio Nicoletti,Antonio Celani*

Main category: cs.LG

TL;DR: 本文在二态伯努利老虎机环境下研究了信息导向采样（IDS）策略，证明了其在不同场景下分别达到有界或对数级的遗憾表现，揭示了其性能与理论下界的一致性。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机问题中，为了通过更易处理的最小模型（二态伯努利 bandit）严谨地比较启发式策略（特别是 IDS）与最优策略之间的性能差异。

Method: 将 IDS 扩展至折扣无限时界（discounted infinite-horizon）设定，引入改进的信息度量和调节参数，并针对对称 bandit 和单公平硬币（one-fair-coin）两种特定场景进行理论分析。

Result: 在对称情况下，IDS 实现了有界的累积遗憾；而在单公平硬币情况下，IDS 的遗憾随时间呈对数增长，这与经典的渐近下界保持一致。

Conclusion: IDS 是平衡探索与利用的有效启发式框架，且在特定伯努利环境下具有可证明的最优性潜力，为统计物理学家研究强化学习提供了桥梁。

Abstract: The Multi-Armed Bandit problem provides a fundamental framework for analyzing the tension between exploration and exploitation in sequential learning. This paper explores Information Directed Sampling (IDS) policies, a class of heuristics that balance immediate regret against information gain. We focus on the tractable environment of two-state Bernoulli bandits as a minimal model to rigorously compare heuristic strategies against the optimal policy. We extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter to modulate the decision-making behavior. We examine two specific problem classes: symmetric bandits and the scenario involving one fair coin. In the symmetric case we show that IDS achieves bounded cumulative regret, whereas in the one-fair-coin scenario the IDS policy yields a regret that scales logarithmically with the horizon, in agreement with classical asymptotic lower bounds. This work serves as a pedagogical synthesis, aiming to bridge concepts from reinforcement learning and information theory for an audience of statistical physicists.

</details>


### [41] [Learning to Reason in LLMs by Expectation Maximization](https://arxiv.org/abs/2512.20169)
*Junghyun Lee,Branislav Kveton,Sunav Choudhary,Subhojyoti Mukherjee,Anup Rao,Ryan A. Rossi,Alexa Siu*

Main category: cs.LG

TL;DR: 本文将大模型推理形式化为 EM 优化过程，通过对比不同采样方案发现，简单的提示后验采样（PPS）在提升推理准确率方面效果最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过生成理由来解决推理问题，本文旨在通过将推理过程形式化为隐变量模型，利用期望最大化（EM）目标来优化这一过程。

Method: 通过 EM 目标函数对比了三种采样方案：带预算的拒绝采样（rejection sampling）、自教推理者（STaR）以及仅保留合理化阶段的提示后验采样（PPS）。

Result: 在 ARC、MMLU 和 OpenBookQA 数据集上的实验表明，PPS 的性能优于拒绝采样和 STaR。

Conclusion: 采样策略在学习推理模型中起着至关重要的作用。尽管 PPS 方法非常简单，但它是最有效的方案。

Abstract: Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.

</details>


### [42] [Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism](https://arxiv.org/abs/2512.19722)
*Alessandro Casadei,Clemens Grupp,Sreyoshi Bhaduri,Lu Guo,Wilson Fung,Rohit Malshe,Raj Ratan,Ankush Pole,Arkajit Rakshit*

Main category: cs.LG

TL;DR: 本文开发了一种基于成本不对称性的动态预测调整模型，通过自调节机制优化预测偏置，每年可实现 510 万美元的成本节省。


<details>
  <summary>Details</summary>
Motivation: 针对传统预测往往忽略不同预测偏误（高估或低估）导致的经济成本差异（即成本不对称性）问题，以及需应对校准误差和宏观经济变化等未建模因素。

Method: 提出一种动态调整机制，根据节点特定的成本函数不对称性调整预测。该模型将成本不对称性整合进预测误差的概率分布中，并引入自调节机制，依据观测到的节省金额动态调制调整幅度。

Result: 实证结果表明，该模型在实际应用中能够实现每年 510 万美元的成本节约。

Conclusion: 该模型通过动态调整预测偏置成功地将预测准确性转化为实际经济利益，展示了考虑成本不对称性在需求预测中的重要价值。

Abstract: This work introduces a methodology to adjust forecasts based on node-specific cost function asymmetry. The proposed model generates savings by dynamically incorporating the cost asymmetry into the forecasting error probability distribution to favor the least expensive scenario. Savings are calculated and a self-regulation mechanism modulates the adjustments magnitude based on the observed savings, enabling the model to adapt to station-specific conditions and unmodeled factors such as calibration errors or shifting macroeconomic dynamics. Finally, empirical results demonstrate the model's ability to achieve \$5.1M annual savings.

</details>


### [43] [Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning](https://arxiv.org/abs/2512.20363)
*Daniel M. Jimenez-Gutierrez,Mehrdad Hassanzadeh,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.LG

TL;DR: 本文提出一种基于聚类的个性化联邦学习框架，通过PSI指标量化数据分布差异并对客户端分组，显著提升了non-IID场景下的模型准确率和公平性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中非独立同分布（non-IID）数据会导致模型更新偏移和性能下降，现有指标不足以精准量化此类偏移。

Method: 提出Clust-PSI-PFL框架：利用加权群体稳定性指标（$WPSI^L$）量化非独立同分布程度，并通过K-means++和轮廓系数（silhouette）对分布同质的客户端进行聚类。

Result: 在六个数据集和多种分区协议下，该方法比现有基准提升了高达18%的全局准确率，并在严重non-IID情况下将客户端公平性提升了37%。

Conclusion: PSI-guided聚类是处理标签偏移下稳健个性化联邦学习的一种原则性、轻量化机制。

Abstract: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $α$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.

</details>


### [44] [End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment](https://arxiv.org/abs/2512.19723)
*Firas Bayram,Bestoun S. Ahmed,Erik Hallin*

Main category: cs.LG

TL;DR: 该研究开发并验证了一个集成实时数据质量评估与MLOps的端到端框架，显著提升了工业生产中的模型精度与处理效率。


<details>
  <summary>Details</summary>
Motivation: 弥补现有方法中数据质量评估与机器学习系统相互孤立、缺乏实时协同的理论与实践鸿沟。

Method: 提出一种轻量化的端到端框架，将动态漂移检测、自适应数据质量指标与MLOps相集成。

Result: 在钢铁制造（ESR工艺）中应用，模型性能提升12%（R2达94%），预测延迟降低至原来的四分之一。

Conclusion: 该框架为动态工业环境下的实时、数据驱动决策提供了一种鲁棒的解决方案，显著推进了MLOps领域的发展。

Abstract: This paper introduces a novel end-to-end framework that efficiently integrates data quality assessment with machine learning (ML) model operations in real-time production environments. While existing approaches treat data quality assessment and ML systems as isolated processes, our framework addresses the critical gap between theoretical methods and practical implementation by combining dynamic drift detection, adaptive data quality metrics, and MLOps into a cohesive, lightweight system. The key innovation lies in its operational efficiency, enabling real-time, quality-driven ML decision-making with minimal computational overhead. We validate the framework in a steel manufacturing company's Electroslag Remelting (ESR) vacuum pumping process, demonstrating a 12% improvement in model performance (R2 = 94%) and a fourfold reduction in prediction latency. By exploring the impact of data quality acceptability thresholds, we provide actionable insights into balancing data quality standards and predictive performance in industrial applications. This framework represents a significant advancement in MLOps, offering a robust solution for time-sensitive, data-driven decision-making in dynamic industrial environments.

</details>


### [45] [Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking](https://arxiv.org/abs/2512.19725)
*Srishti Gupta,Riccardo Balia,Daniele Angioni,Fabio Brau,Maura Pintor,Ambra Demontis,Alessandro Sebastian,Salvatore Mario Carta,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 本文探讨了在动态真实场景下，如何通过结合持续学习与 OOD 检测，克服传统模型无法适应分布变化及重新训练成本高的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型基于 i.i.d. 假设，难以应对真实世界中不断变化的数据分布（如新病毒变种、不同天气下的自动驾驶），且从头开始重新训练成本过高。

Method: 通过引入持续学习（解决增量学习问题）与分布外检测（解决异常识别问题），使模型能够处理非独立同分布（non-i.i.d.）的数据流。

Result: 模型能够从演化的数据流中增量学习而不遗忘旧知识，并能有效识别和响应未知或异常的输入。

Conclusion: 在动态真实环境中，结合持续学习（CL）与分布外（OOD）检测对于构建鲁棒、高效且具适应性的 AI 系统至关重要。

Abstract: Recent years have witnessed significant progress in the development of machine learning models across a wide range of fields, fueled by increased computational resources, large-scale datasets, and the rise of deep learning architectures. From malware detection to enabling autonomous navigation, modern machine learning systems have demonstrated remarkable capabilities. However, as these models are deployed in ever-changing real-world scenarios, their ability to remain reliable and adaptive over time becomes increasingly important. For example, in the real world, new malware families are continuously developed, whereas autonomous driving cars are employed in many different cities and weather conditions. Models trained in fixed settings can not respond effectively to novel conditions encountered post-deployment. In fact, most machine learning models are still developed under the assumption that training and test data are independent and identically distributed (i.i.d.), i.e., sampled from the same underlying (unknown) distribution. While this assumption simplifies model development and evaluation, it does not hold in many real-world applications, where data changes over time and unexpected inputs frequently occur. Retraining models from scratch whenever new data appears is computationally expensive, time-consuming, and impractical in resource-constrained environments. These limitations underscore the need for Continual Learning (CL), which enables models to incrementally learn from evolving data streams without forgetting past knowledge, and Out-of-Distribution (OOD) detection, which allows systems to identify and respond to novel or anomalous inputs. Jointly addressing both challenges is critical to developing robust, efficient, and adaptive AI systems.

</details>


### [46] [Hard Negative Sample-Augmented DPO Post-Training for Small Language Models](https://arxiv.org/abs/2512.19728)
*Haocheng Lu,Minjun Zhu,Henry Yu*

Main category: cs.LG

TL;DR: 本文提出一种轻量级数学后训练流水线，利用细粒度的MathVerifier分析解题错误特征，通过挖掘困难负样本和加权DPO优化，显著提升了小模型处理复杂逻辑错误的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM数学推理后训练通常采用二元对错判断，忽略了推理过程中微妙的逻辑或代数错误；同时，依赖大型奖励模型或LLM判官的RLHF成本高、难扩展且迭代不稳定。

Method: 1. 在SFT基础上，引入紧凑型MathVerifier将解题过程分解为六维错误特征，计算错误度（wrongness）和荒谬度（absurd score）。2. 利用该信号挖掘“近乎正确但有结构缺陷”的困难负样本。3. 采用验证器引导的加权DPO算法进行离线优化。

Result: 在1.5B参数的Qwen2.5模型上，该方案在数值接近但逻辑不一致的问题上表现优于标准SFT和无权DPO，且无需大型奖励模型或外部判官的高额开销。

Conclusion: 在有限的计算预算下，通过轻量级的错误分解验证器和加权DPO，可以有效提升小参数规模LLM的处理复杂数学逻辑错误的能力。

Abstract: Large language models (LLMs) continue to struggle with mathematical reasoning, and common post-training pipelines often reduce each generated solution to a binary outcome: correct or incorrect. This perspective is limiting in practice, as failures in chain-of-thought (CoT) reasoning are frequently structured; solutions may appear convincing while containing subtle logical, algebraic, or numerical flaws. Meanwhile, reinforcement learning from human feedback (RLHF) variants that rely on large reward models or LLM-as-a-judge signals are often expensive, difficult to scale, and unstable to iterate. We propose a lightweight and pragmatic post-training pipeline that targets such structured errors under realistic compute budgets. Starting from supervised fine-tuning (SFT) on MetaMathQA-style CoT data, we introduce a compact MathVerifier that decomposes a candidate solution into a six-dimensional error profile and aggregates it into interpretable wrongness and absurdity scores. These verifier signals serve two roles: (i) mining hard negatives that are near-correct yet structurally flawed, and (ii) defining per-sample importance weights that emphasize the most informative preference pairs. We integrate both into an offline Direct Preference Optimization (DPO) objective via a verifier-guided weighted formulation. Experiments on a 1.5B-parameter Qwen2.5 model show that verifier-guided, weighted DPO yields more targeted improvements than vanilla SFT and unweighted DPO, particularly on problems where solutions are numerically close to correct but logically inconsistent, while avoiding the overhead of training large reward models or relying on external judges.

</details>


### [47] [High-Performance Self-Supervised Learning by Joint Training of Flow Matching](https://arxiv.org/abs/2512.19729)
*Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: FlowFM 通过流匹配技术联合优化编码器与生成器，解决了扩散模型在自监督学习中效率低且性能难以兼顾的问题，显著提升了可穿戴设备的识别与生成效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在该自监督学习中潜力巨大，但面着生成质量与判别性能之间的权衡挑战，且其迭代采样导致计算量大、能耗高，难应用于工业及边缘 AI 设备。

Method: 提出 FlowFM 框架，采用解耦设计，联合训练一个表示编码器（Representation Encoder）和一个基于条件流匹配（Conditional Flow Matching）的生成器。通过学习简单的速度场代替复杂的扩散学习。

Result: 在可穿戴传感器数据上，FlowFM 训练时间缩短 50.4%，推理速度提升达 51 倍。在 5 个数据集上的性能均超越了当前的 SOTA 自监督方法（SSL-Wearables）。

Conclusion: FlowFM 在计算效率、生成质量和分类性能之间取得了极佳的平衡，是适用于穿戴设备等资源受限场景的强力自监督基座模型。

Abstract: Diffusion models can learn rich representations during data generation, showing potential for Self-Supervised Learning (SSL), but they face a trade-off between generative quality and discriminative performance. Their iterative sampling also incurs substantial computational and energy costs, hindering industrial and edge AI applications. To address these issues, we propose the Flow Matching-based Foundation Model (FlowFM), which jointly trains a representation encoder and a conditional flow matching generator. This decoupled design achieves both high-fidelity generation and effective recognition. By using flow matching to learn a simpler velocity field, FlowFM accelerates and stabilizes training, improving its efficiency for representation learning. Experiments on wearable sensor data show FlowFM reduces training time by 50.4\% compared to a diffusion-based approach. On downstream tasks, FlowFM surpassed the state-of-the-art SSL method (SSL-Wearables) on all five datasets while achieving up to a 51.0x inference speedup and maintaining high generative quality. The implementation code is available at https://github.com/Okita-Laboratory/jointOptimizationFlowMatching.

</details>


### [48] [Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis](https://arxiv.org/abs/2512.19732)
*Gaurav Kumar Sharma*

Main category: cs.LG

TL;DR: 本文通过剔除含泄漏信息的描述符，构建了一个更严谨的JARVIS-DFT带隙子集，并证明了在控制泄漏的前提下，介电常数是预测带隙的关键，且模型精度存在性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 针对带隙预测模型中可能存在的“数据泄漏”问题（如描述符中包含有效质量等能带结构信息），旨在建立一个更严谨、无泄漏的预测基准。

Method: 采用了三阶段建模框架，逐步引入基础物理描述符、工程特征和成分属性，利用树类模型（如随机森林或XGBoost）进行预测，并结合SHAP分析进行特征解释。

Result: 在清理后的2280个材料数据集中，树类模型的R²达到0.88至0.90。结果显示，在控制泄漏后，增加描述符空间并不能显著提升准确率；SHAP分析表明介电张量分量是最关键的预测特征。

Conclusion: 本研究强调了在机器学习预测带隙时控制数据泄漏的重要性，并建立了一个稳健的基线，为未来开发更具泛化能力的带隙预测模型提供了参考。

Abstract: In this study, we perform a systematic analysis of the JARVIS-DFT bandgap dataset and identify and remove descriptors that may inadvertently encode band-structure information, such as effective masses. This process yields a curated, leakage-controlled subset of 2280 materials. Using this dataset, a three-phase modeling framework is implemented that incrementally incorporates basic physical descriptors, engineered features, and compositional attributes. The results show that tree-based models achieve R2 values of approximately 0.88 to 0.90 across all phases, indicating that expanding the descriptor space does not substantially improve predictive accuracy when leakage is controlled. SHAP analysis consistently identifies the dielectric tensor components as the dominant contributors. This work provides a curated dataset and baseline performance metrics for future leakage-aware bandgap prediction studies.

</details>


### [49] [Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction](https://arxiv.org/abs/2512.19735)
*Gangxiong Zhang,Yongchao Long*

Main category: cs.LG

TL;DR: 本研究针对LLM在ICU死亡预测中的偏见问题，提出了一种名为CAP的提示框架，通过学习历史误判案例，实现在无须重新训练模型的前提下，同时大幅提升预测精度并消除人口统计学偏见。


<details>
  <summary>Details</summary>
Motivation: LLM在处理ICU患者死亡风险预测时展现出潜力，但预测结果往往存在性别、年龄和种族等人口统计学偏见，且现有的去偏方法通常会损害预测性能，导致公平与准确难以兼得。

Method: 提出了多维偏见评估方案以及临床自适应提示框架（CAse Prompting, CAP）。CAP结合了传统去偏提示与基于案例的推理（CBR），通过引导模型学习历史上类似的预测错误案例及其正确结果，来纠正偏见推理模式。

Result: 在MIMIC-IV数据集上的实验显示，CAP显著提升了性能：AUROC从0.806升至0.873，AUPRC从0.497升至0.694；同时，性别和种族层面的预测差异减少了90%以上，不同群体间的特征关注一致性得分超过0.98。

Conclusion: 该研究证明LLM在ICU死亡率预测中存在显著偏见，但通过精心的提示词设计（如CAP框架），可以在不重新训练模型的情况下，有效同时提升临床预测的准确性与公平性。

Abstract: Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we systematically examine bias in LLM-based ICU mortality prediction and propose a training-free, clinically adaptive prompting framework to simultaneously improve fairness and performance. We first develop a multi-dimensional bias assessment scheme for comprehensive model diagnosis. Building on this analysis, we introduce CAse Prompting (CAP), a novel prompting framework that integrates conventional debiasing prompts with case-based reasoning. CAP guides the model to learn from similar historical misprediction cases and their correct outcomes, enabling correction of biased reasoning patterns. Experiments on the MIMIC-IV dataset show that CAP substantially improves both predictive accuracy and fairness. CAP increases AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis further indicates highly consistent attention patterns across demographic groups, with similarity scores exceeding 0.98. These results demonstrate that LLMs exhibit measurable bias in ICU mortality prediction, and that a carefully designed prompting framework can effectively co-optimize fairness and performance without retraining, offering a transferable paradigm for equitable clinical decision support.

</details>


### [50] [OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting](https://arxiv.org/abs/2512.19738)
*Wilson Fung,Lu Guo,Drake Hilliard,Alessandro Casadei,Raj Ratan,Sreyoshi Bhaduri,Adi Surve,Nikhil Agarwal,Rohit Malshe,Pavan Mullapudi,Hungjen Wang,Saurabh Doodhwala,Ankush Pole,Arkajit Rakshit*

Main category: cs.LG

TL;DR: 本文提出 OpComm 框架，融合了监督学习、强化学习和生成式 AI，显著提升了物流末端配送站的预测精度与决策支持能力。


<details>
  <summary>Details</summary>
Motivation: 末端物流配送站的需求预测误差会导致资源分配不均和延迟，传统的预测方法难以平衡“未满足需求风险”与“资源低效”之间的权衡，且缺乏决策透明度。

Method: 提出 OpComm 框架：使用 LightGBM 进行基准需求预测，结合 PPO 强化学习算法根据预测背景动态选择缓冲区等级，并利用生成式 AI 层生成基于 SHAP 归因的决策摘要。

Result: 在 400 多个站点中，OpComm 的 WAPE 比人工预测降低了 21.65%，同时有效减少了缓冲不足的事件，并显著提升了决策的透明度和解释力。

Conclusion: OpComm 证明了将预测模型与强化学习决策、生成式 AI 解释器相结合，是解决高风险物流预测挑战的有效范式，成功填补了统计严谨性与实际决策需求之间的空白。

Abstract: Accurate forecasting of package volumes at delivery stations is critical for last-mile logistics, where errors lead to inefficient resource allocation, higher costs, and delivery delays. We propose OpComm, a forecasting and decision-support framework that combines supervised learning with reinforcement learning-based buffer control and a generative AI-driven communication module. A LightGBM regression model generates station-level demand forecasts, which serve as context for a Proximal Policy Optimization (PPO) agent that selects buffer levels from a discrete action set. The reward function penalizes under-buffering more heavily than over-buffering, reflecting real-world trade-offs between unmet demand risks and resource inefficiency. Station outcomes are fed back through a Monte Carlo update mechanism, enabling continual policy adaptation. To enhance interpretability, a generative AI layer produces executive-level summaries and scenario analyses grounded in SHAP-based feature attributions. Across 400+ stations, OpComm reduced Weighted Absolute Percentage Error (WAPE) by 21.65% compared to manual forecasts, while lowering under-buffering incidents and improving transparency for decision-makers. This work shows how contextual reinforcement learning, coupled with predictive modeling, can address operational forecasting challenges and bridge statistical rigor with practical decision-making in high-stakes logistics environments.

</details>


### [51] [OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting](https://arxiv.org/abs/2512.19739)
*Soumen Garai,Suman Samui*

Main category: cs.LG

TL;DR: 本文提出 OASI 初始化策略，通过多目标模拟退火替换传统的随机采样，显著优化了 TinyML 场景下关键词检测模型在精度与资源约束间的帕累托平衡。


<details>
  <summary>Details</summary>
Motivation: 在超低功耗 TinyML 设备上实现关键词检测（KWS）需要平衡精度与资源。多目标贝叶斯优化（MOBO）虽能处理此类权衡，但高度依赖初始化，而现有的随机或准随机采样方法未能针对 Pareto 前沿进行优化。

Method: 提出名为 OASI（Objective-Aware Surrogate Initialization）的策略，利用多目标模拟退火（MOSA）在实际建模前生成一个兼顾高准确率与小模型体积的种子 Pareto 集，以此作为 MOBO 的初始点。

Result: 在 TinyML KWS 实验中，OASI 达到了最高的超体积（0.0627）和最低的代际距离（0.0）。统计分析证明了其卓越的一致性，且计算开销仅略微增加。

Conclusion: OASI 能够显著提升极低功耗设备上 KWS 模型的自动优化质量，相较于传统初始化方法，它在超体积（Hypervolume）和收敛稳定性方面表现更优，具有更高的工程实用价值。

Abstract: Voice assistants utilize Keyword Spotting (KWS) to enable efficient, privacy-friendly activation. However, realizing accurate KWS models on ultra-low-power TinyML devices (often with less than $<2$ MB of flash memory) necessitates a delicate balance between accuracy with strict resource constraints. Multi-objective Bayesian Optimization (MOBO) is an ideal candidate for managing such a trade-off but is highly initialization-dependent, especially under the budgeted black-box setting. Existing methods typically fall back to naive, ad-hoc sampling routines (e.g., Latin Hypercube Sampling (LHS), Sobol sequences, or Random search) that are adapted to neither the Pareto front nor undergo rigorous statistical comparison. To address this, we propose Objective-Aware Surrogate Initialization (OASI), a novel initialization strategy that leverages Multi-Objective Simulated Annealing (MOSA) to generate a seed Pareto set of high-performing and diverse configurations that explicitly balance accuracy and model size. Evaluated in a TinyML KWS setting, OASI outperforms LHS, Sobol, and Random initialization, achieving the highest hypervolume (0.0627) and the lowest generational distance (0.0) across multiple runs, with only a modest increase in computation time (1934 s vs. $\sim$1500 s). A non-parametric statistical analysis using the Kruskal-Wallis test ($H = 5.40$, $p = 0.144$, $η^2 = 0.0007$) and Dunn's post-hoc test confirms OASI's superior consistency despite the non-significant overall difference with respect to the $α=0.05$ threshold.

</details>


### [52] [Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics](https://arxiv.org/abs/2512.19740)
*Kousar Raza,Faizan Ali*

Main category: cs.LG

TL;DR: 本文发布了一个关于 2025 年板球亚洲杯的开源结构化数据集，包含 19 场比赛的 61 个变量，旨在推动体育分析和预测建模研究。


<details>
  <summary>Details</summary>
Motivation: 缺乏高质量、结构化且公开的板球赛事数据集，制约了体育分析领域的科研进展和预测模型开发。

Method: 收集 2025 年亚洲杯全部 19 场比赛数据，构建包含 61 个变量的结构化数据集，并进行探索性数据分析（EDA）。

Result: 成功构建了涵盖球队得分、三柱门、强力攻势（Powerplay）统计、边界球等多维度的全面数据集，并验证了其在分析表现指标方面的有效性。

Conclusion: 该数据集为板球数据挖掘和战略决策提供了标准化的基准资源，填补了高质量、机器可读板球赛事数据的空白。

Abstract: This paper presents a structured and comprehensive dataset corresponding to the 2025 Asia Cup T20 cricket tournament, designed to facilitate data-driven research in sports analytics. The dataset comprises records from all 19 matches of the tournament and includes 61 variables covering team scores, wickets, powerplay statistics, boundary counts, toss decisions, venues, and player-specific highlights. To demonstrate its analytical value, we conduct an exploratory data analysis focusing on team performance indicators, boundary distributions, and scoring patterns. The dataset is publicly released through Zenodo under a CC-BY 4.0 license to support reproducibility and further research in cricket analytics, predictive modeling, and strategic decision-making. This work contributes an open, machine-readable benchmark dataset for advancing cricket analytics research.

</details>


### [53] [EdgeFlex-Transformer: Transformer Inference for Edge Devices](https://arxiv.org/abs/2512.19741)
*Shoaib Mohammad,Guanqun Song,Ting Zhu*

Main category: cs.LG

TL;DR: 本文通过结构化剪枝、混合精度和AWQ量化，在不需微调的情况下显著降低了ViT-Huge在边缘设备的内存消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 在大规模Transformer模型部署至边缘设备时，由于内存、计算能力和延迟的严格限制，现有的模型过于冗杂且难以直接运行。

Method: 提出一种无需重新训练的多阶段优化流水线：利用激活分析进行内存感知结构化剪枝，结合FP16选择性执行及激活感知量化（AWQ）将权重和激活转为INT8。

Result: 在ViT-Huge（6.32亿参数）上的实验显示，模型峰值内存占用减少76%，延迟降低6倍以上，且在CIFAR-10上的准确位保持甚至优于FP32基准。

Conclusion: 该多阶段优化框架为在边缘平台部署大型Transformer架构提供了一条实用路径，并为未来集成动态稀疏性和混合专家（MoE）结构奠定了基础。

Abstract: Deploying large-scale transformer models on edge devices presents significant challenges due to strict constraints on memory, compute, and latency. In this work, we propose a lightweight yet effective multi-stage optimization pipeline designed to compress and accelerate Vision Transformers (ViTs) for deployment in resource-constrained environments. Our methodology combines activation profiling, memory-aware pruning, selective mixed-precision execution, and activation-aware quantization (AWQ) to reduce the model's memory footprint without requiring costly retraining or task-specific fine-tuning. Starting from a ViT-Huge backbone with 632 million parameters, we first identify low-importance channels using activation statistics collected via forward hooks, followed by structured pruning to shrink the MLP layers under a target memory budget. We further apply FP16 conversion to selected components and leverage AWQ to quantize the remaining model weights and activations to INT8 with minimal accuracy degradation. Our experiments on CIFAR-10 demonstrate that the fully optimized model achieves a 76% reduction in peak memory usage and over 6x lower latency, while retaining or even improving accuracy compared to the original FP32 baseline. This framework offers a practical path toward efficient transformer inference on edge platforms, and opens future avenues for integrating dynamic sparsity and Mixture-of-Experts (MoE) architectures to further scale performance across diverse tasks.

</details>


### [54] [On-device Large Multi-modal Agent for Human Activity Recognition](https://arxiv.org/abs/2512.19742)
*Md Shakhrul Iman Siam,Ishtiaque Ahmed Showmik,Guanqun Song,Ting Zhu*

Main category: cs.LG

TL;DR: 本文开发了一个基于大语言模型的多模态智能体，在保持高精度人类活动分类的同时，极大地提升了系统的可解释性与交互能力。


<details>
  <summary>Details</summary>
Motivation: 传统HAR方法侧重于单一分类，缺乏解释性及与用户的交互，而LLM的兴起为增强HAR的理解力、解释性和交互性提供了新可能。

Method: 提出一种大语言多模态智能体（Large Multi-Modal Agent）框架，整合大语言模型（LLM）的推理能力与传感器数据处理，实现活动分类、推理分析及问答（Q&A）功能。

Result: 在HHAR、Shoaib和Motionsense等主流数据集上，该模型的分类准确度与当前最先进方法（SOTA）相当，并在推理和用户问答方面具有显著优势。

Conclusion: 该多模态智能体框架为复杂的人类活动识别任务提供了更具可解释性和交互性的解决方案，推动了HAR从单纯的分类任务向全面理解和互动的转变。

Abstract: Human Activity Recognition (HAR) has been an active area of research, with applications ranging from healthcare to smart environments. The recent advancements in Large Language Models (LLMs) have opened new possibilities to leverage their capabilities in HAR, enabling not just activity classification but also interpretability and human-like interaction. In this paper, we present a Large Multi-Modal Agent designed for HAR, which integrates the power of LLMs to enhance both performance and user engagement. The proposed framework not only delivers activity classification but also bridges the gap between technical outputs and user-friendly insights through its reasoning and question-answering capabilities. We conduct extensive evaluations using widely adopted HAR datasets, including HHAR, Shoaib, Motionsense to assess the performance of our framework. The results demonstrate that our model achieves high classification accuracy comparable to state-of-the-art methods while significantly improving interpretability through its reasoning and Q&A capabilities.

</details>


### [55] [From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning](https://arxiv.org/abs/2512.19743)
*Sasan Sharifipour,Constantino Álvarez Casado,Manuel Lage Cañellas,Miguel Bordallo López*

Main category: cs.LG

TL;DR: 本文提出了一种高效的稀疏 GPU 实现 CUDA-APML，旨在解决高精度点云匹配损失函数显存占用过高的问题，通过稀疏化计算使显存消耗降低了 99.9%。


<details>
  <summary>Details</summary>
Motivation: 现有的点云损失函数（如 Chamfer 和 EMD）在几何保真度和计算成本之间存在权衡。APML 虽然效果好但显存占用随点数呈二次方增长，限制了其在大规模场景的应用。

Method: 开发了 CUDA-APML，一种稀疏 GPU 实现。它通过阈值过滤忽略极小赋值，在 COO（坐标列表）格式下直接运行自适应 Softmax、双向对称化和 Sinkhorn 归一化。

Result: 在 ShapeNet 和 MM-Fi 数据集上，CUDA-APML 在保持与密集型 APML 几乎一致精度的同时，将显存峰值降低了 99.9%，并实现了近似线性的显存扩展。

Conclusion: CUDA-APML 成功将 APML 从理论上的优越性转化为工业级的可用性，为百万级点云的高精度匹配提供了可行路径。

Abstract: Loss functions are fundamental to learning accurate 3D point cloud models, yet common choices trade geometric fidelity for computational cost. Chamfer Distance is efficient but permits many-to-one correspondences, while Earth Mover Distance better reflects one-to-one transport at high computational cost. APML approximates transport with differentiable Sinkhorn iterations and an analytically derived temperature, but its dense formulation scales quadratically in memory. We present CUDA-APML, a sparse GPU implementation that thresholds negligible assignments and runs adaptive softmax, bidirectional symmetrization, and Sinkhorn normalization directly in COO form. This yields near-linear memory scaling and preserves gradients on the stored support, while pairwise distance evaluation remains quadratic in the current implementation. On ShapeNet and MM-Fi, CUDA-APML matches dense APML within a small tolerance while reducing peak GPU memory by 99.9%. Code available at: https://github.com/Multimodal-Sensing-Lab/apml

</details>


### [56] [A K-Means, Ward and DBSCAN repeatability study](https://arxiv.org/abs/2512.19772)
*Anthony Bertrand,Engelbert Mephu Nguifo,Violaine Antoine,David Hill*

Main category: cs.LG

TL;DR: 本文研究了聚类算法的可重复性，发现 scikit-learn 中的 K-Means 在多线程环境下（线程数 > 2）无法保证结果的一致性。


<details>
  <summary>Details</summary>
Motivation: 探讨机器学习中模型和实验结果的一致性（可重复性），特别是针对特定算法在位级层面实现相同结果对于科学诚信和调试的重要性。

Method: 将 K-Means、DBSCAN 和 Ward 三种聚类算法分解为基本步骤，识别每阶段实现重复性所需的条件，并基于 scikit-learn 库进行实验测试。

Result: 当 OpenMP 线程数超过两个时，K-Means 算法会产生不一致的结果。

Conclusion: 在机器学习中必须重视可重复性，不仅是为了科学完整性，也是为了调试的便利性，开发者和用户需警惕并行计算带来的不一致风险。

Abstract: Reproducibility is essential in machine learning because it ensures that a model or experiment yields the same scientific conclusion. For specific algorithms repeatability with bitwise identical results is also a key for scientific integrity because it allows debugging. We decomposed several very popular clustering algorithms: K-Means, DBSCAN and Ward into their fundamental steps, and we identify the conditions required to achieve repeatability at each stage. We use an implementation example with the Python library scikit-learn to examine the repeatable aspects of each method. Our results reveal inconsistent results with K-Means when the number of OpenMP threads exceeds two. This work aims to raise awareness of this issue among both users and developers, encouraging further investigation and potential fixes.

</details>


### [57] [Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy](https://arxiv.org/abs/2512.19805)
*Deepit Sapru*

Main category: cs.LG

TL;DR: 本文提出一个营销决策框架，通过因果推断（Uplift）与受限优化算法相结合，实现预算及业务限制下的精准营销定向，显著提升了收入和用户留存。


<details>
  <summary>Details</summary>
Motivation: 旨在解决如何将异构处理效应（Uplift）转化为受限的定向营销策略，从而在遵守业务红线的前提下最大化收入和留存。

Method: 采用增量模型（Uplift Learners）估算条件平均处理效应（CATE），并结合约束分配算法，在预算和销售下滑限制等条件下优化营销目标与优惠策略。

Result: 线下评估（基于Uplift AUC、IPS和SNIPS）显示该框架优于倾向性模型及静态基准；大规模线上A/B测试进一步证实了其在收入和转化率上的提升，同时满足了客户体验约束。

Conclusion: 该框架为营销人员提供了一套可复用的“行动指南”，使其能够在满足业务约束和战略KPI的同时，实现大规模因果定向营销。

Abstract: This paper introduces a marketing decision framework that converts heterogeneous-treatment uplift into constrained targeting strategies to maximize revenue and retention while honoring business guardrails. The approach estimates Conditional Average Treatment Effects (CATE) with uplift learners and then solves a constrained allocation to decide who to target and which offer to deploy under limits such as budget or acceptable sales deterioration. Applied to retention messaging, event rewards, and spend-threshold assignment, the framework consistently outperforms propensity and static baselines in offline evaluations using uplift AUC, Inverse Propensity Scoring (IPS), and Self-Normalized IPS (SNIPS). A production-scale online A/B test further validates strategic lift on revenue and completion while preserving customer-experience constraints. The result is a reusable playbook for marketers to operationalize causal targeting at scale, set guardrails, and align campaigns with strategic KPIs.

</details>


### [58] [Fine-Tuned In-Context Learners for Efficient Adaptation](https://arxiv.org/abs/2512.19879)
*Jorg Bornschein,Clare Lyle,Yazhe Li,Amal Rannen-Triki,Xu Owen He,Razvan Pascanu*

Main category: cs.LG

TL;DR: 本文提出了一种将上下文学习融入模型微调的统一方法，结合了二者的样本效率与性能增益，并引入前序评估优化小样本下的超参数设置，实现了在各类任务中超越传统适配方法的表现。


<details>
  <summary>Details</summary>
Motivation: 旨在解决提示词工程（Prompt Engineering）在数据量增加时性能陷入瓶颈，以及模型微调（Fine-tuning）在小样本场景下表现不佳的局限性，寻求一种能跨越不同数据规模的最优适配方案。

Method: 提出一种统一框架，在微调过程中直接将上下文示例（In-context examples）加入训练数据中，模仿k-shot提示词结构。同时，采用前序评估（Prequential Evaluation）进行超参数选择，以充分利用有限的数据并提供验证信号。

Result: 实验结果表明，该统一方法在不同数据规模下均能一致性地达到或显著超过传统的微调和上下文学习基准线。前序评估在低数据环境下也被证明是有效的验证手段。

Conclusion: 结合上下文学习的微调方法（Unified Approach）是一种比单一方法更稳健、更高效的LLM任务适配方案。通过引入前序评估，解决了小样本场景下的超参数选择难题。

Abstract: When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model's inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model's parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.

</details>


### [59] [Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling](https://arxiv.org/abs/2512.19905)
*Indranil Halder,Cengiz Pehlevan*

Main category: cs.LG

TL;DR: 该研究通过贝叶斯线性回归框架，理论性地证明了推理时缩放的有效性及其局限性，指出了奖励函数准确性对缩放效率的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在推理时增加计算资源（如 Best-of-k 采样）已被证明有效，但支撑这种“推理缩放法则”的数学原理和性能边界仍缺乏系统性的理解。

Method: 通过高维机制下的贝叶斯线性回归模型进行解析分析，引入基于 Softmax 的奖励加权采样器模拟“LLM-as-a-judge”场景，并结合极端值理论（Extreme Value Theory）推导泛化误差的闭式解。

Result: 1. 奖励函数与教师模型一致时，误差随采样数 $k$ 单调递减；2. 奖励不匹配会产生最优 $k$ 值，过度采样反而增加误差；3. 存在最优采样温度；4. 确定了推理缩放优于增加训练数据的理论边界；5. 任务难度增加会削弱推理缩放的优势。

Conclusion: 推理时缩放（inference-time scaling）在奖励函数与教师模型匹配时非常有效，其误差衰减率为 $Θ(1/k^2)$，但在奖励函数严重偏差或任务难度过高时，缩放效果会受限甚至产生负面影响。

Abstract: Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tractable model of inference-time scaling: Bayesian linear regression with a reward-weighted sampler, where the reward is determined from a linear model, modeling LLM-as-a-judge scenario. We study this problem in the high-dimensional regime, where the deterministic equivalents dictate a closed-form expression for the posterior predictive mean and variance. We analyze the generalization error when training data are sampled from a teacher model. We draw $k$ inference-time samples and select via softmax at a temperature applied to a quadratic reward. When the reward is not too different from the teacher, the generalization error decreases monotonically with increasing inference time samples $k$. However, the specific reward that optimizes inference-time selection generally differs from the teacher. In contrast, substantial reward misspecification induces a finite optimal $k$ beyond which more sampling can increase the generalization error. For fixed $k$, there exists an optimal sampling temperature. We experimentally verify these facts in large language model inference with an additional large language model as a judge. In the "best-of-$k$" limit with the teacher as reward, we theoretically show that the generalization error decays as $Θ(1/k^2)$ and determine the leading coefficient via extreme value theory. These formulas delineate domains where scaling inference-time computation is provably preferable to collecting more data. Finally, we demonstrate that when task difficulty increases, the previously mentioned advantage of inference-time compute degrades.

</details>


### [60] [Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra](https://arxiv.org/abs/2512.19909)
*Maxime Lacour,Pu Ren,Rie Nakata,Nori Nakata,Michael Mahoney*

Main category: cs.LG

TL;DR: 本研究开发了基于深度学习的CGM-FAS模型，通过CVAE架构替代传统高斯过程，实现了极速、高精度的非遍历性地震路径效应预测。


<details>
  <summary>Details</summary>
Motivation: 传统的非遍历性地震动模型（GMM）依赖高斯过程（GP），在大规模预测时面临计算限制，且需预设相关函数，难以捕捉复杂的频率间关系。

Method: 提出CGM-FAS模型，采用条件变分自编码器（CVAE）架构，以地震和台站的地理坐标为条件变量，直接从数据中学习空间模式和频率间相关性。

Result: 在旧金山湾区数据上的表现与GP模型一致，但在处理速度上有量级提升（10秒内生成万个站点、千个频率的地图），且无需预设相关函数。

Conclusion: CGM-FAS为大规模、多频率的非遍历性地震动预测提供了一个高效且准确的方向，克服了传统GP方法的计算瓶颈和预设函数的限制。

Abstract: Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.

</details>


### [61] [Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning](https://arxiv.org/abs/2512.19920)
*Jiayun Wu,Jiashuo Liu,Zhiyuan Zeng,Tianyang Zhan,Wenhao Huang*

Main category: cs.LG

TL;DR: 本文提出一种行为校准强化学习方法，通过优化不确定性表达来抑制LLM幻觉，使4B规模的小模型在不确定性量化指标上达到了顶级前沿模型的水平。


<details>
  <summary>Details</summary>
Motivation: 大模型的幻觉问题源于训练目标优先考虑模仿数据分布而非诚实性。现有的强化学习范式（如RLVR）使用二元奖励，诱导模型在正确概率大于零时进行猜测，而非诚实地表达不确定性。

Method: 提出并评估了行为校准强化学习方案。该方法利用严格概率评分规则（strictly proper scoring rules）作为奖励信号，引导模型根据自信度随机选择“弃答”或在不确定的陈述中标记出来，而非无条件盲目猜测。

Result: 在Qwen3-4B-Instruct上的实验显示，其在数学推理任务中的准确率-幻觉比（0.806）远超GPT-5；在跨领域QA测试中，其校准误差与Grok-4和Gemini-2.5-Pro持平，证明了校准能力可独立于模型规模和原始准确率提升。

Conclusion: 行为校准（Behavioral Calibration）是一种可以与原始预测能力解耦的迁移元技能。通过优化正确的评分规则，小型模型也能在不确定性量化上达到甚至超越顶级模型，为缓解LLM幻觉提供了新的路径。

Abstract: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.

</details>


### [62] [The Seismic Wavefield Common Task Framework](https://arxiv.org/abs/2512.19927)
*Alexey Yermakov,Yue Zhao,Marine Denolle,Yiyu Ni,Philippe M. Wyder,Judah Goldfeder,Stefano Riva,Jan Williams,David Zoro,Amy Sara Rude,Matteo Tomasetto,Joe Germany,Joseph Bakarji,Georg Maierhofer,Miles Cranmer,J. Nathan Kutz*

Main category: cs.LG

TL;DR: 本文提出了一个地震波场机器学习通用任务框架（CTF），旨在通过标准化的数据集和指标，解决地震学预测与重建中的算法评估不统一问题。


<details>
  <summary>Details</summary>
Motivation: 地震学在状态预测、重建以及应对地球模型参数多变性方面面临挑战。传统数值模拟计算量巨大，而现有机器学习研究缺乏统一的表征、公平的报告标准和严谨的算法比较。

Method: 引入了一个通用任务框架（Common Task Framework, CTF），包含全球、地壳和局部三个尺度的精选数据集，并制定了涵盖预测、重建及泛化能力的特定评估指标。

Result: 通过对两个数据集的评分展示了不同方法（包括基础模型）在模拟和真实地震传感器数据重建任务中的表现，揭示了各模型的优缺点及适用场景。

Conclusion: 该框架有望通过标准化评估和隐藏测试集取代随机对比，提升科学机器学习在地震学中的严谨性和可复现性。

Abstract: Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth's complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.

</details>


### [63] [Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress](https://arxiv.org/abs/2512.19935)
*Samruddhi Baviskar*

Main category: cs.LG

TL;DR: 本文发现金融机器学习模型的对抗鲁棒性在宏观经济压力期会系统性恶化，并为此提出了一套结合制度感知的评估框架与语义审计的治理方法。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习对抗鲁棒性评估多基于静态假设，忽略了金融环境非平稳性（如宏观经济压力期）对模型漏洞状态的影响。

Method: 提出“条件对抗脆弱性”框架，通过波动率对宏观经济进行状态分层，在保持模型和攻击方法不变的情况下，对比平静期与压力期的性能；并引入大语言模型（LLM）驱动的语义审计层进行解释性治理。

Result: 在压力时期，尽管模型基准性能保持稳定，但对抗扰动的敏感度显著增加，导致预测准确率大幅下降、假阴性率上升，进而增加了错过高风险案例的系统性风险。

Conclusion: 在金融高风险部署中，对抗鲁棒性应被视为一种随环境变化的“状态依赖”属性，必须实施具有“压力感知”能力的模型风险评估框架。

Abstract: Machine learning models used in financial decision systems operate in nonstationary economic environments, yet adversarial robustness is typically evaluated under static assumptions. This work introduces Conditional Adversarial Fragility, a regime dependent phenomenon in which adversarial vulnerability is systematically amplified during periods of macroeconomic stress. We propose a regime aware evaluation framework for time indexed tabular financial classification tasks that conditions robustness assessment on external indicators of economic stress. Using volatility based regime segmentation as a proxy for macroeconomic conditions, we evaluate model behavior across calm and stress periods while holding model architecture, attack methodology, and evaluation protocols constant. Baseline predictive performance remains comparable across regimes, indicating that economic stress alone does not induce inherent performance degradation. Under adversarial perturbations, however, models operating during stress regimes exhibit substantially greater degradation across predictive accuracy, operational decision thresholds, and risk sensitive outcomes. We further demonstrate that this amplification propagates to increased false negative rates, elevating the risk of missed high risk cases during adverse conditions. To complement numerical robustness metrics, we introduce an interpretive governance layer based on semantic auditing of model explanations using large language models. Together, these results demonstrate that adversarial robustness in financial machine learning is a regime dependent property and motivate stress aware approaches to model risk assessment in high stakes financial deployments.

</details>


### [64] [LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models](https://arxiv.org/abs/2512.20002)
*Jiacheng You,Jingcheng Yang,Yuhang Xie,Zhongxuan Wu,Xiucheng Li,Feng Li,Pengjie Wang,Jian Xu,Bo Zheng,Xinyang Chen*

Main category: cs.LG

TL;DR: 本文提出 LoFT-LLM，通过低频趋势提取、残差建模及大语言模型语义校准，解决了时序预测中的噪声干扰和数据稀疏难题。


<details>
  <summary>Details</summary>
Motivation: 针对现实世界时间序列预测中存在的训练数据有限、噪声大、长期趋势不明确以及领域专家知识（辅助变量）利用不足的问题。

Method: 提出 LoFT-LLM 框架：1) 补丁低频预测模块（PLFM）提取稳定趋势；2) 残差学习器建模高频波动；3) 微调 LLM 通过结构化提示词结合辅助上下文校准预测结果。

Result: 在全数据和少样本（few-shot）场景下，该模型显著超越了现有的深度预测模型基准。

Conclusion: LoFT-LLM 在金融和能源数据集上表现优于现有基准，具有更高的准确性、稳健性和可解释性。

Abstract: Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.

</details>


### [65] [Control Variate Score Matching for Diffusion Models](https://arxiv.org/abs/2512.20003)
*Khaled Kahouli,Romuald Elie,Klaus-Robert Müller,Quentin Berthet,Oliver T. Unke,Arnaud Doucet*

Main category: cs.LG

TL;DR: 本文提出 CVSI 框架，通过控制变量法融合 DSI 与 TSI 的优势，有效降低了扩散模型在不同噪声水平下的估计方差。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型中，传统的 DSI 在低噪声时方差大，而基于能量函数的 TSI 在高噪声时方差大，两者存在方差权衡问题。

Method: 在控制变量（Control Variates）的正规框架下统一了 DSI 和 TSI，提出了 CVSI。通过推导随时间变化的最佳控制系数，理论上保证了在整个噪声谱范围内的方差最小化。

Result: CVSI 解决了 DSI 和 TSI 的局限性，实现了在全噪声阶段的方差最小化，提高了采样质量和学习效率。

Conclusion: CVSI 是一个鲁棒且低方差的即插即用型估计器，能显著提升无数据采样器训练和推理时扩散采样的效率。

Abstract: Diffusion models offer a robust framework for sampling from unnormalized probability densities, which requires accurately estimating the score of the noise-perturbed target distribution. While the standard Denoising Score Identity (DSI) relies on data samples, access to the target energy function enables an alternative formulation via the Target Score Identity (TSI). However, these estimators face a fundamental variance trade-off: DSI exhibits high variance in low-noise regimes, whereas TSI suffers from high variance at high noise levels. In this work, we reconcile these approaches by unifying both estimators within the principled framework of control variates. We introduce the Control Variate Score Identity (CVSI), deriving an optimal, time-dependent control coefficient that theoretically guarantees variance minimization across the entire noise spectrum. We demonstrate that CVSI serves as a robust, low-variance plug-in estimator that significantly enhances sample efficiency in both data-free sampler learning and inference-time diffusion sampling.

</details>


### [66] [Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance](https://arxiv.org/abs/2512.20006)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: 本文提出了一种新的激活函数OGAB，通过正交变换和群体感知偏置在训练阶段直接优化特征空间，通过增强少数类特征的独立性和可分性来解决深度学习中的类不平衡挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在特征提取方面表现出色，但在处理类不平衡数据时性能仍会下降；现有方法多依赖预处理或后处理，缺乏在嵌入学习层面直接处理该问题的算子。

Method: 提出一种名为OGAB的激活函数，结合正交变换（Orthogenicity）以保持特征独立性，并引入群体感知偏置（Group-aware Bias）学习机制来自动识别数据簇并调整嵌入空间。

Result: 在真实和合成数据集上的实验表明，OGAB在处理类不平衡问题上优于传统的和可学习的激活函数，显著提升了分类器的鲁棒性。

Conclusion: OGAB为类不平衡问题提供了一种创新的激活函数解决方案，证明了通过归纳偏置增强特征区分度是解决复杂数据挑战的有效途径。

Abstract: Class imbalance is a common challenge in machine learning and data mining, often leading to suboptimal performance in classifiers. While deep learning excels in feature extraction, its performance still deteriorates under imbalanced data. In this work, we propose a novel activation function, named OGAB, designed to alleviate class imbalance in deep learning classifiers. OGAB incorporates orthogonality and group-aware bias learning to enhance feature distinguishability in imbalanced scenarios without explicitly requiring label information. Our key insight is that activation functions can be used to introduce strong inductive biases that can address complex data challenges beyond traditional non-linearity. Our work demonstrates that orthogonal transformations can preserve information about minority classes by maintaining feature independence, thereby preventing the dominance of majority classes in the embedding space. Further, the proposed group-aware bias mechanism automatically identifies data clusters and adjusts embeddings to enhance class separability without the need for explicit supervision. Unlike existing approaches that address class imbalance through preprocessing data modifications or post-processing corrections, our proposed approach tackles class imbalance during the training phase at the embedding learning level, enabling direct integration with the learning process. We demonstrate the effectiveness of our solution on both real-world and synthetic imbalanced datasets, showing consistent performance improvements over both traditional and learnable activation functions.

</details>


### [67] [An Optimal Policy for Learning Controllable Dynamics by Exploration](https://arxiv.org/abs/2512.20053)
*Peter N. Loxley*

Main category: cs.LG

TL;DR: 本文提出了一种针对未知环境的高效探索策略，通过在有限时界内动态调整控制约束和贪婪最大化信息增益，解决了复杂动力学系统中的最优控制与学习问题。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在未知环境的顺序决策任务中，如何克服具有限制性特征（如瞬时态、吸收态、非回溯态）的状态，以实现高效的可控动力学学习。

Method: 通过对控制集进行简单参数化，并利用动态规划中的序列改进属性（Sequential Improvement Property），提出了一种在有限时间范围内通过贪婪最大化信息增益来更新控制约束的算法。

Result: 该策略计算高效且易于实现，能够让智能体通过“在探索中学习”的方式最大化信息增益，其最优性通过计数论证和与次优策略的对比得到了验证。

Conclusion: 在存在复杂状态约束（如吸收态、不可逆状态）的环境中，非平稳策略对于实现最优探索是必不可少的。

Abstract: Controllable Markov chains describe the dynamics of sequential decision making tasks and are the central component in optimal control and reinforcement learning. In this work, we give the general form of an optimal policy for learning controllable dynamics in an unknown environment by exploring over a limited time horizon. This policy is simple to implement and efficient to compute, and allows an agent to ``learn by exploring" as it maximizes its information gain in a greedy fashion by selecting controls from a constraint set that changes over time during exploration. We give a simple parameterization for the set of controls, and present an algorithm for finding an optimal policy. The reason for this policy is due to the existence of certain types of states that restrict control of the dynamics; such as transient states, absorbing states, and non-backtracking states. We show why the occurrence of these states makes a non-stationary policy essential for achieving optimal exploration. Six interesting examples of controllable dynamics are treated in detail. Policy optimality is demonstrated using counting arguments, comparing with suboptimal policies, and by making use of a sequential improvement property from dynamic programming.

</details>


### [68] [PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models](https://arxiv.org/abs/2512.20063)
*Mingue Park,Jisung Hwang,Seungwoo Yoo,Kyeongmin Yeo,Minhyuk Sung*

Main category: cs.LG

TL;DR: PairFlow 是一种用于离散流模型（DFM）的极轻量级预处理技术，通过构建源-目标样本对实现少步采样，无需教师模型且计算成本仅为全量训练的 1.7%。


<details>
  <summary>Details</summary>
Motivation: 离散流模型（DFMs）虽然在离散数据生成上表现强劲，但采样速度慢。现有的加速方法主要依赖微调，这会带来巨大的额外计算开销，因此需要一种更高效的加速预训练手段。

Method: 提出一种名为 PairFlow 的轻量级预训练方法。该方法利用 DFM 的闭式逆变换（closed-form inversion）来构建源分布和目标分布之间的耦合样本（paired samples），无需预训练教师模型即可通过类似 ReFlow 的方式进行训练。

Result: PairFlow 的计算开销极低（仅为完整训练的 1.7% 以内），但在性能上能媲美甚至超越需要两阶段微调的方法。在分子数据、二值图像和 RGB 图像实验中均验证了其有效性。

Conclusion: PairFlow 为离散流模型提供了一种极低成本的加速方案，不仅在大样本生成上表现出色，还为后续的蒸馏微调提供了更高质量的基座模型。

Abstract: We introduce $\texttt{PairFlow}$, a lightweight preprocessing step for training Discrete Flow Models (DFMs) to achieve few-step sampling without requiring a pretrained teacher. DFMs have recently emerged as a new class of generative models for discrete data, offering strong performance. However, they suffer from slow sampling due to their iterative nature. Existing acceleration methods largely depend on finetuning, which introduces substantial additional training overhead. $\texttt{PairFlow}$ addresses this issue with a lightweight preprocessing step. Inspired by ReFlow and its extension to DFMs, we train DFMs from coupled samples of source and target distributions, without requiring any pretrained teacher. At the core of our approach is a closed-form inversion for DFMs, which allows efficient construction of paired source-target samples. Despite its extremely low cost, taking only up to 1.7% of the compute needed for full model training, $\texttt{PairFlow}$ matches or even surpasses the performance of two-stage training involving finetuning. Furthermore, models trained with our framework provide stronger base models for subsequent distillation, yielding further acceleration after finetuning. Experiments on molecular data as well as binary and RGB images demonstrate the broad applicability and effectiveness of our approach.

</details>


### [69] [Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection](https://arxiv.org/abs/2512.20086)
*Jeehong Kim,Youngseok Hwang,Minchan Kim,Sungho Bae,Hyunwoo Park*

Main category: cs.LG

TL;DR: 本文针对海上交通等非网格时空系统，通过LLM智能体增强技术，推出了支持多粒度分析的图异常检测基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的ST-GNN在固定节点（如公路）表现良好，但在海上交通等缺乏固定锚点、数据稀疏且不规则的非网格环境中，构建时空图并进行多粒度异常检测面临巨大挑战。

Method: 通过扩展OMTAD数据集，并利用两种专门的LLM智能体（轨迹合成器和异常注入器）来构建丰富的交互上下文并生成具有语义意义的异常数据。

Result: 构建了一个全新的海上领域异常检测基准数据集，支持在节点级、边级和图级三个不同粒度上进行系统的算法评估。

Conclusion: 该基准测试有望推动非网格时空系统异常检测方法的研究进展，并提升该领域的研究可复现性。

Abstract: Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: \emph{Trajectory Synthesizer} and \emph{Anomaly Injector} to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems.

</details>


### [70] [Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering](https://arxiv.org/abs/2512.20115)
*Yuanhao Chen,Qi Liu,Pengbin Chen,Zhongjian Qiao,Yanjie Li*

Main category: cs.LG

TL;DR: 针对离线强化学习中低质量数据导致性能受限的问题，本文提出通过平均奖励评分来过滤并提取高质量样本的简单高效方法，显著提升了学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有的策略约束离线强化学习方法过度依赖行为策略质量，若数据集中包含大量低奖励数据，习得策略会受限于次优参考策略，导致学习慢、效率低及性能差。

Method: 提出一种样本过滤方法：首先根据轨迹（episode）的平均奖励和平均折扣奖励对转换（transitions）进行评分；然后仅提取高分样本用于离线强化学习训练。

Result: 在多个离线强化学习算法和基准测试任务中，该方法均优于基准线方案，提升了样本效率和最终性能。

Conclusion: 该简单的样本过滤方法能有效改善离线强化学习在包含低质量数据时的性能表现。

Abstract: Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected return using a given static dataset of transitions. However, offline RL faces the distribution shift problem. The policy constraint offline RL method is proposed to solve the distribution shift problem. During the policy constraint offline RL training, it is important to ensure the difference between the learned policy and behavior policy within a given threshold. Thus, the learned policy heavily relies on the quality of the behavior policy. However, a problem exists in existing policy constraint methods: if the dataset contains many low-reward transitions, the learned will be contained with a suboptimal reference policy, leading to slow learning speed, low sample efficiency, and inferior performances. This paper shows that the sampling method in policy constraint offline RL that uses all the transitions in the dataset can be improved. A simple but efficient sample filtering method is proposed to improve the sample efficiency and the final performance. First, we evaluate the score of the transitions by average reward and average discounted reward of episodes in the dataset and extract the transition samples of high scores. Second, the high-score transition samples are used to train the offline RL algorithms. We verify the proposed method in a series of offline RL algorithms and benchmark tasks. Experimental results show that the proposed method outperforms baselines.

</details>


### [71] [NeuralCrop: Combining physics and machine learning for improved crop yield predictions](https://arxiv.org/abs/2512.20177)
*Yunan Lin,Sebastian Bathiany,Maha Badri,Maximilian Gelbrecht,Philipp Hess,Brian Groenke,Jens Heinke,Christoph Müller,Niklas Boers*

Main category: cs.LG

TL;DR: 研究开发了 NeuralCrop 混合模型，通过将物理过程模型与机器学习融合，克服了传统农业模型精度不足和纯 AI 模型泛化能力差的问题，显著提升了极端天气下的作物产量预测能力。


<details>
  <summary>Details</summary>
Motivation: 传统的全球网格作物模型（GGCMs）受限于对复杂生物物理过程理解不足而存在不确定性，而纯机器学习模型虽有潜力，但在处理训练分布外的数据（如气候变化场景）时泛化能力差。

Method: 提出 NeuralCrop 混合模型：首先训练机器学习组件模拟先进的基于过程的 GGCM 行为，随后利用观测数据进行微调，将物理逻辑与数据驱动方法相结合。

Result: NeuralCrop 在站点和区域尺度上均优于现有的 GGCMs。在 2000-2019 年欧洲小麦和美国玉米带的产量模拟中，尤其是在干旱极端情况下，其准确性显著提高，且在面对未见气候条件时比纯机器学习模型更具鲁棒性。

Conclusion: NeuralCrop 结合了物理模型的可解释性和机器学习的预测精度，是提高气候变化下全球粮食安全评估可靠性的有效途径。

Abstract: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.

</details>


### [72] [Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud](https://arxiv.org/abs/2512.20218)
*Jixiao Yang,Jinyu Chen,Zixiao Huang,Chengda Xu,Chi Zhang,Sijia Li*

Main category: cs.LG

TL;DR: 本文提出 Cost-TrustFL 框架，通过高效的信誉评估和层次化聚合，在提升联邦学习抗毒性攻击能力的同时，有效降低了多云环境下的跨云通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习防御机制多关注模型准确性，忽视了多云环境下非独立同分布（Non-IID）数据处理、恶意节点检测以及高昂的跨云通信成本（Egress fees）。

Method: 提出 Cost-TrustFL 层次化框架：1. 采用基于梯度的近似 Shapley 值（线性复杂度）进行参与者信誉评价；2. 实施成本感知聚合策略，优先进行云内通信以减少昂贵的跨云流量。

Result: 在 CIFAR-10 和 FEMNIST 数据集上，该方法在 30% 恶意客户端存在时仍达 86.7% 准确率，且较基准方法降低了 32% 的通信成本。

Conclusion: Cost-TrustFL 是一种适用于多云环境的实用联邦学习框架，在保障安全性的同时显著降低了运营成本。

Abstract: Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.

</details>


### [73] [Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning](https://arxiv.org/abs/2512.20220)
*Kausthubh Manda,Raghuram Bharadwaj Diddigi*

Main category: cs.LG

TL;DR: 本文研究了具有低秩表示的离线多任务强化学习，证明了联合学习共享表示能显著提升样本效率，并降低下游新任务的学习难度。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨在离线强化学习中，多个任务共享动作价值函数的低秩表示时，如何通过利用共享结构来提高统计效率和泛化能力。

Method: 分析了一种多任务拟合Q迭代（Fitted Q-iteration）变体，通过在离线数据上最小化Bellman误差，联合学习共享表示和特定任务的价值函数。

Result: 在标准假设下，证明了收敛速度与总样本量 $nT$ 的平方根倒数成正比，并证实复用学习到的表示可以显著降低下游新任务的学习复杂度。

Conclusion: 低秩表示学习在离线多任务RL中具有显著的统计优势，不仅能通过任务间的数据聚合提高当前学习效率，更能有效赋能新任务的零/少样本迁移学习。

Abstract: We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.

</details>


### [74] [Adaptive Multi-task Learning for Probabilistic Load Forecasting](https://arxiv.org/abs/2512.20232)
*Onintze Zaballa,Verónica Álvarez,Santiago Mazuelas*

Main category: cs.LG

TL;DR: 本文提出了一种基于向量值隐马尔可夫模型的自适应多任务学习方法，用于实现多实体负荷的实时监测与概率预测，有效解决了消费模式动态变化带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务学习负荷预测大多局限于离线模型，无法捕捉电力消耗模式的动态变化以及多实体间的复杂相关性。

Method: 基于向量值隐马尔可夫模型（vector-valued hidden Markov models），并利用递归过程实时更新模型参数，从而实现对最新消费模式的捕捉和概率预测。

Result: 实验结果表明，该方法在预测准确性以及不确定性评估（概率预测）方面均优于现有方法，展示了更强的适应性。

Conclusion: 该自适应多任务学习方法能够有效处理电力系统中的动态特征与空间相关性，为多实体负荷预测提供了一个可靠且高效的在线解决方案。

Abstract: Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment.

</details>


### [75] [How I Met Your Bias: Investigating Bias Amplification in Diffusion Models](https://arxiv.org/abs/2512.20233)
*Nathan Roos,Ekaterina Iakovleva,Ani Gjergji,Vito Paolo Pastore,Enzo Tartaglione*

Main category: cs.LG

TL;DR: 本文首次分析了扩散模型采样算法对数据集偏见放大的影响，揭示了无需重训模型，仅通过调整采样器和超参数即可显著改变（减少或增加）生成结果中的偏见。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在图像合成领域处于领先地位，但其倾向于复制并放大训练数据集中偏见的机制尚不明确；既往研究多认为这是模型固有的，而本文旨在探究采样算法如何影响这一现象。

Method: 在 Biased MNIST、Multi-Color MNIST、BFFHQ 以及 Stable Diffusion 等多个数据集上进行受控实验，比较不同采样器及其超参数对生成结果中偏见分布的影响。

Result: 实验证明，仅通过改变采样超参数，即使在训练模型固定的情况下，也能显著诱导偏见的减少或进一步放大。采样过程对偏见分布具有可衡量的实质性影响。

Conclusion: 扩散模型的采样算法及其超参数是影响偏见放大程度的关键因素，这意味着可以通过调整采样过程（而非重新训练模型）来减轻偏见。

Abstract: Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.

</details>


### [76] [Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion](https://arxiv.org/abs/2512.20249)
*Xuanyu Hu*

Main category: cs.LG

TL;DR: 本文提出BrainROI模型，通过改进fMRI编码器的空间对齐机制和引入可解释的Prompt自动优化流程，显著提升了脑活动信号转换为自然语言描述的跨受试者泛化性和准确度。


<details>
  <summary>Details</summary>
Motivation: 针对多模态脑解码（Brain Decoding）中存在的跨受试者泛化能力差以及Prompt设计缺乏可解释性和稳定性等挑战。

Method: 1) 设计新型fMRI编码器，利用多图谱软功能分区（soft-ROI）作为共享空间，并采用体素级门控融合机制（Voxel-gate）处理异构数据；2) 引入基于本地Qwen模型的迭代Prompt优化过程；3) 在推理阶段实施参数化解码约束。

Result: 在NSD数据集的Brain-captioning任务上达到领先水平。在跨受试者设定下，BLEU-4和CIDEr等指标较SOTA方法及基准模型有显著提升。

Conclusion: BrainROI模型通过结构化的软分区特征对齐和可解释的Prompt优化，成功提升了跨受试者脑解压任务的泛化能力和生成质量。

Abstract: Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.

</details>


### [77] [DeepONet-accelerated Bayesian inversion for moving boundary problems](https://arxiv.org/abs/2512.20268)
*Marco A. Iglesias,Michael. E. Causon,Mikhail Y. Matveev,Andreas Endruweit,Michael . V. Tretyakov*

Main category: cs.LG

TL;DR: 本文提出一种基于 DeepONet 的神经算子框架，用于快速模拟移动边界系统，并结合 EKI 实现了树脂传递模塑 (RTM) 过程中材料特性的实时高精度反演。


<details>
  <summary>Details</summary>
Motivation: 针对多孔介质中单相 Darcy 流的移动边界问题，寻求一种快速、准确且具有泛化能力的仿真框架，以支持工业 RTM 过程的实时监控。

Method: 采用 DeepOperator Network (DeepONet) 架构构建替代模型，并将其与集成卡尔曼反演 (EKI) 算法耦合，用于解决贝叶斯反演问题。

Result: 通过合成和实验数据验证，DeepONet 替代模型比全模型 EKI 反演速度提高了数个数量级，且具有网格无关性，可在任意传感器配置下运行。

Conclusion: 该框架标志着数字孪生在工业实际部署中迈出了重要一步，为包含移动边界流的其他应用提供了通用的解决方案。

Abstract: This work demonstrates that neural operator learning provides a powerful and flexible framework for building fast, accurate emulators of moving boundary systems, enabling their integration into digital twin platforms. To this end, a Deep Operator Network (DeepONet) architecture is employed to construct an efficient surrogate model for moving boundary problems in single-phase Darcy flow through porous media. The surrogate enables rapid and accurate approximation of complex flow dynamics and is coupled with an Ensemble Kalman Inversion (EKI) algorithm to solve Bayesian inverse problems.
  The proposed inversion framework is demonstrated by estimating the permeability and porosity of fibre reinforcements for composite materials manufactured via the Resin Transfer Moulding (RTM) process. Using both synthetic and experimental in-process data, the DeepONet surrogate accelerates inversion by several orders of magnitude compared with full-model EKI. This computational efficiency enables real-time, accurate, high-resolution estimation of local variations in permeability, porosity, and other parameters, thereby supporting effective monitoring and control of RTM processes, as well as other applications involving moving boundary flows. Unlike prior approaches for RTM inversion that learn mesh-dependent mappings, the proposed neural operator generalises across spatial and temporal domains, enabling evaluation at arbitrary sensor configurations without retraining, and represents a significant step toward practical industrial deployment of digital twins.

</details>


### [78] [HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training](https://arxiv.org/abs/2512.20272)
*Yuanjian Xu,Yuan Shuai,Jianing Hao,Guang Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于 Hermite 函数的新型 GAN 框架（HGAN-SDEs），旨在通过更轻量、高效的鉴别器设计，解决神经随机微分方程在生成建模中计算慢和训练不稳定的难题。


<details>
  <summary>Details</summary>
Motivation: 现有的神经随机微分方程 (Neural SDEs) 判别器（如基于 CDEs 的架构）在捕捉路径时间依赖时，存在计算成本过高和对抗训练不稳定的问题。

Method: 提出 HGAN-SDEs 框架，核心是利用 **神经 Hermite 函数 (Neural Hermite functions)** 构建结构化的鉴别器，以替代传统的神经受控微分方程 (CDEs) 鉴别器。

Result: 实验证明，该模型在合成数据集和真实世界系统上均取得了更优的样本质量和学习效率，且理论上证明了其具有通用近似性质。

Conclusion: HGAN-SDEs 结合了 Hermite 函数的数学优越性与 GAN 的生成能力，是处理连续时间随机过程建模的一种高效且稳健的新方案。

Abstract: Neural Stochastic Differential Equations (Neural SDEs) provide a principled framework for modeling continuous-time stochastic processes and have been widely adopted in fields ranging from physics to finance. Recent advances suggest that Generative Adversarial Networks (GANs) offer a promising solution to learning the complex path distributions induced by SDEs. However, a critical bottleneck lies in designing a discriminator that faithfully captures temporal dependencies while remaining computationally efficient. Prior works have explored Neural Controlled Differential Equations (CDEs) as discriminators due to their ability to model continuous-time dynamics, but such architectures suffer from high computational costs and exacerbate the instability of adversarial training. To address these limitations, we introduce HGAN-SDEs, a novel GAN-based framework that leverages Neural Hermite functions to construct a structured and efficient discriminator. Hermite functions provide an expressive yet lightweight basis for approximating path-level dynamics, enabling both reduced runtime complexity and improved training stability. We establish the universal approximation property of our framework for a broad class of SDE-driven distributions and theoretically characterize its convergence behavior. Extensive empirical evaluations on synthetic and real-world systems demonstrate that HGAN-SDEs achieve superior sample quality and learning efficiency compared to existing generative models for SDEs

</details>


### [79] [Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity](https://arxiv.org/abs/2512.20291)
*Yuxing Gan,Ziyu Lei*

Main category: cs.LG

TL;DR: 本文提出 CDSP-MoE，通过在共享权重子空间内利用梯度冲突驱动的拓扑剪枝，解决了传统 MoE 的参数遗忘和指令过拟合问题，实现了更强的自发模块化与语义表征。


<details>
  <summary>Details</summary>
Motivation: 现有的 MoE 架构存在两个局限：一是结构性的参数孤立导致灾难性遗忘；二是模型过度依赖指令，在无指令场景下表现不佳。

Method: 提出 CDSP-MoE 框架，采用超完整参数骨架。利用可学习的拓扑掩码在此共享子空间中刻画逻辑专家，并引入“滞后梯度博弈”（Lagged Gradient Game）机制，将梯度冲突转化为结构监督信号，自动剪枝冲突路径。

Result: 实验证明 CDSP-MoE 能够实现鲁棒的内容驱动路由，无需人工任务标签。即使在缺乏明确指令的盲推理协议下，模型仍能保持高效的语义专业化表现。

Conclusion: CDSP-MoE 通过一种演化而非预设的专家构建方式，证明了在不依赖指令引导的情况下，模型依然可以通过内部梯度逻辑实现稳健的语义分化。

Abstract: Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts

</details>


### [80] [FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning](https://arxiv.org/abs/2512.20329)
*Mrinmay Sen,Subhrajit Nag*

Main category: cs.LG

TL;DR: FedDPC 通过局部更新投影和自适应缩放技术，有效缓解了数据异构与部分客户端参与导致的模型漂移，提升了联邦学习的收敛速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 数据异构性和客户端部分参与共同导致了全局模型更新的偏差和方差，造成 FL 训练不稳定、收敛慢且性能下降，而现有研究较少关注部分参与带来的负面影响。

Method: 提出 FedDPC 算法：1. 将当前的局部更新投影到之前的全局更新上，以控制局部和全局更新的方差；2. 在聚合前对局部更新采用自适应缩放（adaptive scaling）来加速收敛。

Result: 在多个异构分区数据集上的图像分类实验表明，FedDPC 在降低训练损失的速度和提高测试准确率方面均优于现有最先进的联邦学习算法。

Conclusion: FedDPC 在处理高度非独立同分布（non-IID）数据和部分参与场景下，相比现有最先进算法表现出更强的鲁棒性和卓越的性能。

Abstract: Data heterogeneity is a significant challenge in modern federated learning (FL) as it creates variance in local model updates, causing the aggregated global model to shift away from the true global optimum. Partial client participation in FL further exacerbates this issue by skewing the aggregation of local models towards the data distribution of participating clients. This creates additional variance in the global model updates, causing the global model to converge away from the optima of the global objective. These variances lead to instability in FL training, which degrades global model performance and slows down FL training. While existing literature primarily focuses on addressing data heterogeneity, the impact of partial client participation has received less attention. In this paper, we propose FedDPC, a novel FL method, designed to improve FL training and global model performance by mitigating both data heterogeneity and partial client participation. FedDPC addresses these issues by projecting each local update onto the previous global update, thereby controlling variance in both local and global updates. To further accelerate FL training, FedDPC employs adaptive scaling for each local update before aggregation. Extensive experiments on image classification tasks with multiple heterogeneously partitioned datasets validate the effectiveness of FedDPC. The results demonstrate that FedDPC outperforms state-of-the-art FL algorithms by achieving faster reduction in training loss and improved test accuracy across communication rounds.

</details>


### [81] [Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation](https://arxiv.org/abs/2512.20346)
*Emilia Majerz,Witold Dzwinel,Jacek Kitowski*

Main category: cs.LG

TL;DR: 本文提出一种基于物理知识的生成式机器学习模型（正规化流），用于加速CERN ZDC探测器仿真，实现了精度提升及421倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 旨在通过将领域知识嵌入机器学习过程，加速CERN ALICE实验中零度量能器（ZDC）的仿真，并提升模型对粒子簇射空间分布的表征精度。

Method: 在师生生成框架下利用正规化流（Normalizing Flows），引入创新的损失函数和基于输出变异性的缩放机制。

Result: 该方法在模拟粒子簇射的形貌和空间分布上优于传统数据驱动模型，且速度比现有文献中的NF实现快421倍。

Conclusion: 结合物理先验的模型在加速高能物理模拟方面具有巨大潜力，这种“老师-学生”框架为HEP探测器仿真提供了高效的新路径。

Abstract: Physics-based machine learning blends traditional science with modern data-driven techniques. Rather than relying exclusively on empirical data or predefined equations, this methodology embeds domain knowledge directly into the learning process, resulting in models that are both more accurate and robust. We leverage this paradigm to accelerate simulations of the Zero Degree Calorimeter (ZDC) of the ALICE experiment at CERN. Our method introduces a novel loss function and an output variability-based scaling mechanism, which enhance the model's capability to accurately represent the spatial distribution and morphology of particle showers in detector outputs while mitigating the influence of rare artefacts on the training. Leveraging Normalizing Flows (NFs) in a teacher-student generative framework, we demonstrate that our approach not only outperforms classic data-driven model assimilation but also yields models that are 421 times faster than existing NF implementations in ZDC simulation literature.

</details>


### [82] [Simplifying Multi-Task Architectures Through Task-Specific Normalization](https://arxiv.org/abs/2512.20420)
*Mihai Suteu,Ovidiu Serban*

Main category: cs.LG

TL;DR: 本文证明了简单的“任务特定归一化”即可取代复杂的 MTL 架构。提出的 TS$σ$BN 机制在不显著增加参数的情况下，显著提升了多任务模型的稳定性和性能，并提供了任务关联性的可解释分析。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务学习（MTL）架构往往设计过于复杂，包含繁琐的任务特定模块或路由方案，导致参数量激增且难以优化。作者旨在探索是否能通过更简单、更底层的机制解决任务干扰和资源平衡问题。

Method: 提出任务特定 Sigmoid 批量归一化（TS$σ$BN）。通过将共享归一化替换为各任务独立的变体，并引入 Sigmoid 门控机制，让任务在共享特征提取器的同时实现容量的软分配。

Result: 在 NYUv2、Cityscapes、CelebA 和 PascalContext 等主流数据集上，TS$σ$BN 在 CNN 和 Transformer 架构下均达到或超过了现有复杂模型的性能；同时，学习到的门控参数为分析任务间的关系和滤波器专业化提供了直观的可解释性。

Conclusion: 多任务学习未必需要复杂的架构设计，任务特定归一化层是平衡性能与复杂度的极佳手段，值得作为MTL研究的新基准。

Abstract: Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$σ$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$σ$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.

</details>


### [83] [Field-Space Attention for Structure-Preserving Earth System Transformers](https://arxiv.org/abs/2512.20350)
*Maximilian Witte,Johannes Meuer,Étienne Plésiat,Christopher Kadow*

Main category: cs.LG

TL;DR: 本文提出了一种 Field-Space Transformer，通过在物理场空间计算注意力，实现了高效、可解释且符合物理几何约束的地球系统建模。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习架构通常在学习到的潜在空间中运行，缺乏物理一致性且难以解释。地球系统建模需要能够直接处理连续地球物理场并保持其底层几何结构的架构。

Method: 提出 Field-Space Transformer 架构，引入“场空间注意力”机制。它直接在物理域（而非潜在空间）处理球面连续场，采用固定的多尺度分解和结构保持变形技术，而非学习化的单一尺度表征。

Result: 在 HEALPix 网格的全球温度超分辨率任务中，该模型比传统 Vision Transformer 和 U-Net 收敛更快、更稳定，且参数量显著减少。

Conclusion: Field-Space Attention 为下一代地球系统预测和生成模型提供了一个紧凑、可解释且符合物理规律的基础模块。

Abstract: Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.

</details>


### [84] [Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs](https://arxiv.org/abs/2512.20573)
*Rui Pan,Zhuofu Chen,Ravi Netravali*

Main category: cs.LG

TL;DR: 本文提出 FailFast，一种基于扩散模型（dLLM）的投机解码框架，通过动态调整投机长度实现并行预测，显著提升了自回归大模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）虽支持并行生成但存在效率与质量的权衡，作者旨在利用 dLLM 的速度优势，通过投机解码（Speculative Decoding）解决自回归模型推理慢的问题。

Method: 提出 FailFast 框架，利用 dLLM 的并行解码特性，通过动态适应投机长度（Speculation Length）：在难预测区域“快速失败”以减少延迟，在易预测区域“大举获胜”（一次性投机多达 70 个 token）以提高加速比。

Result: 在无需微调的情况下，FailFast 实现了自回归 LLM 的无损加速。相比原始解码提升高达 4.9 倍，相比最优的常规 dLLM 草拟器提升 1.7 倍，相比 EAGLE-3 提升 1.4 倍。

Conclusion: FailFast 证明了 dLLMs 作为投机解码草拟器的巨大潜力，通过动态调整策略，在不牺牲质量的前提下大幅提升了自回归模型的推理效率。

Abstract: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It "fails fast" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and "wins big" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.

</details>


### [85] [Performative Policy Gradient: Optimality in Performative Reinforcement Learning](https://arxiv.org/abs/2512.20576)
*Debabrota Basu,Udvas Das,Brahim Driss,Uddalak Mukherjee*

Main category: cs.LG

TL;DR: 本文通过扩展策略梯度理论，提出了PePG算法，解决了强化学习中因模型影响环境（执行性）而导致的分布偏移问题，实现了从“性能稳定”到“性能最优”的跨越。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型部署后会反向影响环境，导致数据分布发生变化。传统的强化学习忽视了这种“执行性”（Performativity）反馈，现有研究主要关注稳定性而非最优性。

Method: 提出了Performative Policy Gradient (PePG) 算法，并证明了其在Softmax参数化（含或不含熵正则化）下的收敛性。此外，本文还推导出了执行性决策下的性能差异引理（Performance Difference Lemma）和策略梯度定理。

Result: PePG是首个考虑到执行性响应的策略梯度算法，能够收敛至“执行性最优策略”（Performatively Optimal Policies），即在自身诱导的分布偏移下依然保持最优。

Conclusion: PePG算法在理论和实验上均优于现有的追求性能稳定性的强化学习算法，能够成功应对环境反馈循环带来的挑战。

Abstract: Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.

</details>


### [86] [BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples](https://arxiv.org/abs/2512.20403)
*Xuan-An Le,Minh-Nam Tran,Son Nguyen*

Main category: cs.LG

TL;DR: 本文提出 BRIDGE 框架，通过引入“助教”模型中转和智能采样策略，在节省 90% 以上 API 成本的同时，显著提升了超小规模模型对复杂推理任务的掌握能力。


<details>
  <summary>Details</summary>
Motivation: 解决大模型知识蒸馏到超小模型（<1B）时的“容量陷阱”（1000倍规模差距）以及 API 获取高质量数据成本过高的问题。

Method: 提出 BRIDGE 两阶段框架：1. 预算感知筛选：利用本地助教（TA）模型，通过零成本流水线筛选出高难度且具多样性的少量数据（3-5%）请求大模型接口。2. 异步蒸馏：利用 TA 在全量数据上生成合成推理链，通过指令微调课程平衡学生模型的行为对齐与推理能力。

Result: 在医疗、法律、金融基准测试中，学生模型性能提升 28-41%，且仅需 1/10 的教师模型查询量。其表现超越了使用 100% 预算的直接蒸馏基准。

Conclusion: BRIDGE 框架证明了“少而精”的教师引导结合“大容量”的中间模型生成，能低成本地提升超小规模模型的推理性能，为受限资源下的知识蒸馏提供了新范式。

Abstract: Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.

</details>


### [87] [Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning](https://arxiv.org/abs/2512.20605)
*Seijin Kobayashi,Yanick Schimpf,Maximilian Schlegel,Angelika Steger,Maciej Wolczyk,Johannes von Oswald,Nino Scherre,Kaitlin Maile,Guillaume Lajoie,Blake A. Richards,Rif A. Saurous,James Manyika,Blaise Agüera y Arcas,Alexander Meulemans,João Sacramento*

Main category: cs.LG

TL;DR: 本文提出“内部强化学习”，通过在高阶表征空间而非逐个Token层面进行探索，解决了自回归模型在稀疏奖励环境下的学习效率难题。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归模型在强化学习中采用逐词（token-by-token）采样进行探索，这在奖励稀疏的任务中效率极低。

Method: 提出一种非因果的高阶序列模型，通过控制基础自回归模型的残差流激活（Residual Stream Activations）来发现时间抽象动作，并训练具备终止条件的内部控制器。

Result: 在Grid World和MuJoCo任务中，该方法能将长序列动作压缩为行为有意义的内部控制器，通过“内部强化学习（Internal RL）”成功解决了标准RL无法处理的稀疏奖励问题。

Conclusion: 在自回归模型中引入潜空间动作生成和强化学习是实现分层RL的有效途径，能显著提升大模型解决复杂、长期目标任务的能力。

Abstract: Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.

</details>


### [88] [Machine Learning to Predict Digital Frustration from Clickstream Data](https://arxiv.org/abs/2512.20438)
*Jibin Joseph*

Main category: cs.LG

TL;DR: 本研究利用点击流数据训练了 XGBoost 和 LSTM 模型来预测电商用户交互中的挫折感，其中 LSTM 表现最优，且能在用户操作早期实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 用户在移动端或网站完成任务时的挫折感会导致销售损失和投诉，因此企业需要一种自动化的方法来识别并预测用户的负面情绪状态。

Method: 基于真实电商网站的点击流数据（540万个事件），利用愤怒点击、U型导航、购物车流失等规则定义挫折感。分别训练了基于表格特征的机器学习模型（如 XGBoost）和基于全事件序列的判别式 LSTM 模型。

Result: XGBoost 模型达到了 90% 的准确率（AUC 0.9579），而 LSTM 表现最佳，准确率约 91%（AUC 0.9705）。此外，仅需 20 到 30 次交互，模型即可实现可靠预测。

Conclusion: Frustration 可以在用户交互的早期阶段（前20-30个动作内）被准确识别，为企业提供了实时优化用户体验的可能性。

Abstract: Many businesses depend on their mobile apps and websites, so user frustration while trying to complete a task on these channels can cause lost sales and complaints. In this research, I use clickstream data from a real e-commerce site to predict whether a session is frustrated or not. Frustration is defined using certain rules based on rage bursts, back and forth navigation (U turns), cart churn, search struggle, and long wandering sessions, and applies these rules to 5.4 million raw clickstream events (304,881 sessions). From each session, I build tabular features and train standard classifier models. I also use the full event sequence to train a discriminative LSTM classifier. XGBoost reaches about 90% accuracy, ROC AUC of 0.9579, while the LSTM performs best with about 91% accuracy and a ROC AUC of 0.9705. Finally, the research shows that with only the first 20 to 30 interactions, the LSTM already predicts frustration reliably.

</details>


### [89] [Explainable time-series forecasting with sampling-free SHAP for Transformers](https://arxiv.org/abs/2512.20514)
*Matthias Hertel,Sebastian Pütz,Ralf Mikut,Veit Hagenmeyer,Benjamin Schäfer*

Main category: cs.LG

TL;DR: 本文提出 SHAPformer，一种高效、准确且无需采样的 Transformer 时间序列预测模型，能够以极快的速度生成符合数据逻辑的 Shapley 解释。


<details>
  <summary>Details</summary>
Motivation: 现有的 SHAP 方法在处理时间序列时效率较低，且往往假设特征独立，这与时间序列数据的现实特征（高度依赖性）不符。

Method: 基于 Transformer 架构，通过操纵注意力机制（Attention Manipulation）使模型能够基于特征子集进行预测，从而实现无采样（sampling-free）的解释生成。

Result: 解释生成速度比 SHAP Permutation Explainer 快几个数量级（不足 1 秒）；在合成数据中表现出高度的真实性；在电力负载预测中提供了具有洞察力的局部和全局解释。

Conclusion: SHAPformer 通过将 Shapley 值的计算直接集成到模型架构中，成功解决了时间序列解释中的计算效率和特征依赖问题。

Abstract: Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.

</details>


### [90] [Improving ML Training Data with Gold-Standard Quality Metrics](https://arxiv.org/abs/2512.20577)
*Leslie Barrett,Michael W. Sherman*

Main category: cs.LG

TL;DR: 本文研究了如何通过统计方法评估和提升手工标注数据的质量，强调了多轮次迭代评估的重要性，并指出传统的标注员磨合期不足以完全消除错误。


<details>
  <summary>Details</summary>
Motivation: 手工标注的数据对机器学习至关重要，但学术界对标注质量控制的关注较少，且标注质量往往随练习过程波动较大。

Method: 提出使用统计学方法衡量标注的一致性（Consistency）和协议度（Agreement），并建议进行多轮次标注记录，通过观察方差的下降来衡量质量提升。同时探索了在无需对每个项目进行多次标注的情况下获取高质量数据的方法。

Result: 实验表明，多轮次记录协议指标比单次记录更可靠；标注员的“磨合期”（Burn-in period）不足以消除误差；通过特定方法可以在减少重复标注工作量的同时维持高质量。

Conclusion: 有效的标注质量控制不仅依赖于初始的培训（Burn-in），更需要多轮次的质量评估和动态的反馈机制，仅依赖单人单次标注无法保证数据可靠性。

Abstract: Hand-tagged training data is essential to many machine learning tasks. However, training data quality control has received little attention in the literature, despite data quality varying considerably with the tagging exercise. We propose methods to evaluate and enhance the quality of hand-tagged training data using statistical approaches to measure tagging consistency and agreement. We show that agreement metrics give more reliable results if recorded over multiple iterations of tagging, where declining variance in such recordings is an indicator of increasing data quality. We also show one way a tagging project can collect high-quality training data without requiring multiple tags for every work item, and that a tagger burn-in period may not be sufficient for minimizing tagger errors.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [91] [LongVideoAgent: Multi-Agent Reasoning with Long Videos](https://arxiv.org/abs/2512.20618)
*Runtao Liu,Ziyi Liu,Jiaqi Tang,Yue Ma,Renjie Pi,Jipeng Zhang,Qifeng Chen*

Main category: cs.AI

TL;DR: 本文提出一种通过强化学习训练的多智能体框架，通过主智能体协调定位与视觉观察，实现了在长视频中精准、高效的推理与问答。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型在处理长视频QA时，往往依赖有损压缩或有限的工具集，导致时间定位能力弱且容易丢失细粒度视觉信息。

Method: 提出一个由主LLM（Master LLM）协调的多智能体框架，包含定位智能体（Grounding Agent）和视觉智能体（Vision Agent），并利用强化学习（RL）进行训练，以优化规划逻辑和步骤效率。

Result: 在LongTVQA和LongTVQA+数据集上，该多智能体系统显著优于非智能体基线模型，且强化学习进一步提升了模型的推理与规划能力。

Conclusion: 多智能体协作结合强化学习是处理长视频问答的一种高效且可解释的方案，优于传统的有损压缩或简单工具调用方法。

Abstract: Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.

</details>


### [92] [PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research](https://arxiv.org/abs/2512.19799)
*Tingjia Miao,Jiawen Dai,Jingkun Liu,Jinxin Tan,Muhua Zhang,Wenkai Jin,Yuwen Du,Tian Jin,Xianghe Pang,Zexi Liu,Tu Guo,Zhengliang Zhang,Yunjie Huang,Shuo Chen,Rui Ye,Yuzhi Zhang,Linfeng Zhang,Kun Chen,Wei Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: 本文提出了一款名为 PhysMaster 的物理学 LLM 智能体，它通过结合分层知识库和自适应探索策略，实现了物理研究从推理到计算的自动化端到端发现。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 智能体多局限于标准基准测试或通用任务，难以解决物理学中需要严谨数学推理、编程计算以及长程规划的端到端开放性科研问题。

Method: 提出了 PhysMaster 智能体，集成 LANDAU（分层学术数据宇宙）以存储文献与验证痕迹，并采用自适应探索策略，将抽象推理与数值计算深度融合。

Result: 在粒子物理、凝聚态和天体物理等领域表现出色：将数月的科研工作缩短至数小时，实现了从假设驱动到独立发现开放问题的全流程自动化。

Conclusion: PhysMaster 展示了 LLM 智能体在处理高度抽象、数学密集型且具有超长决策链条的物理研究任务中的巨大潜力，平衡了计算效率与开探索能力。

Abstract: Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.

</details>


### [93] [A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution](https://arxiv.org/abs/2512.19882)
*Mahdi Mostajabdaveh,F. Sibel Salman,Walter J. Gutjahr*

Main category: cs.AI

TL;DR: 本文针对震后物资短缺背景下的车辆路径与资源分配问题，提出了平衡效率与公平的双目标优化模型，并开发了高效的分支定价算法，通过土耳其地震数据验证了其在降低分配不公方面的显著效果。


<details>
  <summary>Details</summary>
Motivation: 大灾后预置物资常无法满足全部需求，需在有限物资分配中平衡“物流效率（运输时间）”与“社会公平（各避难所未满足需求的均衡度）”。

Method: 构建双目标混合整数规划（MIP）模型，采用ε-约束法处理目标函数，并开发了一种基于数学特性优化、包含有效不等式的分支定价（B&P）算法。

Result: 所提B&P算法效率远超商业求解器；在不牺牲效率的前提下，该方法可将援助分配的不公平性降低34%。

Conclusion: 双目标优化模型在适度的时间压力下对平衡公平与效率至关重要，且所提算法具备解决大规模实际灾害问题的能力。

Abstract: The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $ε$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.

</details>


### [94] [Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs](https://arxiv.org/abs/2512.19937)
*Eric Yeh,John Cadigan,Ran Chen,Dick Crouch,Melinda Gervasio,Dayne Freitag*

Main category: cs.AI

TL;DR: 本文通过插值解码技术，实现在连续维度上精确控制LLM的人格特征，并成功模拟了人类在经济博弈中的决策行为。


<details>
  <summary>Details</summary>
Motivation: 研究由于需要为每种个性特征手动编写提示词，导致使用LLM模拟人类经济行为时面临实验开销大且可复制性差的问题。

Method: 提出一种“插值解码”技术：将人格的每一维度表示为一对相反的提示词，并通过调整插值参数在连续空间内模拟特定的人格特征。

Result: 1. 该方法能可靠调节大模型在大五人格测试中的得分；2. 成功复现了人类心理学研究中个性与经济博弈行为的相关性；3. 初步实现了通过搜索空间参数来“孪生”特定人类玩家的决策行为。

Conclusion: 插值解码（Interpolative Decoding）是探索大语言模型（LLM）个性和决策行为的一种稳健且可扩展的方法，为社会科学模拟和个性化人工智能开发提供了新途径。

Abstract: Recent research has explored using very large language models (LLMs) as proxies for humans in tasks such as simulation, surveys, and studies. While LLMs do not possess a human psychology, they often can emulate human behaviors with sufficiently high fidelity to drive simulations to test human behavioral hypotheses, exhibiting more nuance and range than the rule-based agents often employed in behavioral economics. One key area of interest is the effect of personality on decision making, but the requirement that a prompt must be created for every tested personality profile introduces experimental overhead and degrades replicability. To address this issue, we leverage interpolative decoding, representing each dimension of personality as a pair of opposed prompts and employing an interpolation parameter to simulate behavior along the dimension. We show that interpolative decoding reliably modulates scores along each of the Big Five dimensions. We then show how interpolative decoding causes LLMs to mimic human decision-making behavior in economic games, replicating results from human psychological research. Finally, we present preliminary results of our efforts to ``twin'' individual human players in a collaborative game through systematic search for points in interpolation space that cause the system to replicate actions taken by the human subject.

</details>


### [95] [Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification](https://arxiv.org/abs/2512.19957)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Rodrigo Pereira David,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 本文提出一种基于类别原型引导和定制化ViT的方法，用于解决高分辨率植被图像的多标签物种识别，并在PlantCLEF 2025挑战赛中获得第五名。


<details>
  <summary>Details</summary>
Motivation: 针对PlantClef 2025挑战赛中的高分辨率、细粒度多标签植物物种识别问题，探索如何将单物种识别能力迁移至复杂的植被样方多标签分类中。

Method: 1) 利用K-Means在训练集上聚类生成类别原型。2) 构建定制化窄ViT，以预训练的DinoV2作为冻结的Patch Embedding层。3) 训练模型通过测试图像重建训练集的类别原型。4) 利用注意力分数定位感兴趣区域，从而实现多标签分类。

Result: 在PlantCLEF 2025竞赛私有排行榜排名第五，F1得分为0.33331，与第一名分差仅为0.03。

Conclusion: 该方法证明了利用原型引导和自监督特征提取进行跨域任务（从单物种到多物种识别）的有效性，尤其在处理高分辨率植被图像方面具有潜力。

Abstract: This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.

</details>


### [96] [FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification](https://arxiv.org/abs/2512.19960)
*Luciano Araujo Dourado Filho,Rodrigo Tripodi Calumby*

Main category: cs.AI

TL;DR: 本文通过对各类进行独立聚类生成伪标签，并结合分层分类任务，旨在提升细粒度视觉分类中应对类内变异的能力。


<details>
  <summary>Details</summary>
Motivation: 在细粒度图像分类（FGVC）任务中，极大的类内变异（intra-class variability）和样本代表性不足会显著阻碍深度学习模型的学习效果。

Method: 提出一种分层分类方法（FGDCC），对每个类别单独进行聚类以发现编码潜在相似性的伪标签，并利用这些伪标签通过类内聚类分配任务来学习更细粒度的视觉特征。

Result: 在 PlantNet300k 数据集上达到了 SOTA（州级/最先进）性能，证明了通过聚类挖掘类内特征的有效性。

Conclusion: 初步实验揭示了未来值得改进的关键方向，目前即使在某些组件未完全优化的情况下，该方法仍具竞争力，为解决 FGVC 中的类内变异提供了新视角。

Abstract: Intra-class variability is given according to the significance in the degree of dissimilarity between images within a class. In that sense, depending on its intensity, intra-class variability can hinder the learning process for DL models, specially when such classes are also underrepresented, which is a very common scenario in Fine-Grained Visual Categorization (FGVC) tasks. This paper proposes a novel method that aims at leveraging classification performance in FGVC tasks by learning fine-grained features via classification of class-wise cluster assignments. Our goal is to apply clustering over each class individually, which can allow to discover pseudo-labels that encodes a latent degree of similarity between images. In turn, those labels can be employed in a hierarchical classification process that allows to learn more fine-grained visual features and thereby mitigating intra-class variability issues. Initial experiments over the PlantNet300k enabled to shed light upon several key points in which future work will have to be developed in order to find more conclusive evidence regarding the effectiveness of our method. Our method still achieves state-of-the-art performance on the PlantNet300k dataset even though some of its components haven't been shown to be fully optimized. Our code is available at \href{https://github.com/ADAM-UEFS/FGDCC}{https://github.com/ADAM-UEFS/FGDCC}.

</details>


### [97] [S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test](https://arxiv.org/abs/2512.19992)
*Zhe Sun,Xueyuan Yang,Yujie Lu,Zhenliang Zhang*

Main category: cs.AI

TL;DR: 本文引入了S$^{3}$IT benchmark，通过3D排座任务评估智能体同时处理物理约束与社交规范的“具身社交智能”，发现现有LLM在空间智能上远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法要么侧重于脱离身体的社交推理（纯文本），要么侧重于缺乏社交维度的物理任务。这导致无法评估智能体在现实具身情境中整合并平衡物理约束与社交规范的能力。

Method: 提出了S$^{3}$IT基准测试，核心是一个排座任务。该任务要求智能体在3D环境中为具有不同身份和关系的LLM驱动型NPC安排座位。框架具有程序化可扩展性，要求智能体通过主动对话获取偏好、通过自主探索感知环境，并在复杂约束下进行多目标优化。

Result: 实验对照SOTA LLMs与人类基准。结果显示LLMs在处理此类整合任务时仍然困难，表现出明显的差距，特别是在空间智能方面表现欠佳。

Conclusion: 目前的LLM在空间智能方面存在明显短板，但在处理具有明确文本线索的社交冲突时能接近人类水平。本研究强调了开发能够同时理解物理环境和复杂社交规则的具身智能体的必要性。

Abstract: The integration of embodied agents into human environments demands embodied social intelligence: reasoning over both social norms and physical constraints. However, existing evaluations fail to address this integration, as they are limited to either disembodied social reasoning (e.g., in text) or socially-agnostic physical tasks. Both approaches fail to assess an agent's ability to integrate and trade off both physical and social constraints within a realistic, embodied context. To address this challenge, we introduce Spatially Situated Social Intelligence Test (S$^{3}$IT), a benchmark specifically designed to evaluate embodied social intelligence. It is centered on a novel and challenging seat-ordering task, requiring an agent to arrange seating in a 3D environment for a group of large language model-driven (LLM-driven) NPCs with diverse identities, preferences, and intricate interpersonal relationships. Our procedurally extensible framework generates a vast and diverse scenario space with controllable difficulty, compelling the agent to acquire preferences through active dialogue, perceive the environment via autonomous exploration, and perform multi-objective optimization within a complex constraint network. We evaluate state-of-the-art LLMs on S$^{3}$IT and found that they still struggle with this problem, showing an obvious gap compared with the human baseline. Results imply that LLMs have deficiencies in spatial intelligence, yet simultaneously demonstrate their ability to achieve near human-level competence in resolving conflicts that possess explicit textual cues.

</details>


### [98] [Discovering Lie Groups with Flow Matching](https://arxiv.org/abs/2512.20043)
*Jung Yeon Park,Yuxuan Chen,Floor Eijkelboom,Jan-Willem van de Meent,Lawson L. S. Wong,Robin Walters*

Main category: cs.AI

TL;DR: 本文提出 LieFlow，一种利用流匹配（Flow Matching）在 Lie 群上直接从数据中自动发现对称性的灵活方法。


<details>
  <summary>Details</summary>
Motivation: 对称性对理解物理系统和提升机器学习性能至关重要，但现有方法在可发现的群类型及所需先验假设方面存在局限。

Method: 提出 LieFlow 方法，将对称性发现建模为在较大的假设群上学习分布。通过流匹配（Flow Matching）技术使该分布与数据中的观测对称性对齐，并引入了新型插值方案以解决“最后时刻收敛”挑战。

Result: 在 2D 和 3D 点云实验中，该方法成功发现包括反射在内的离散群。通过在复数域进行流匹配，有效解决了对称目标分布导致的样本停滞问题。

Conclusion: LieFlow 是一种通过流匹配在 Lie 群上直接从数据中学习对称性的新框架，具有更高的灵活性和更少的先验假设。

Abstract: Symmetry is fundamental to understanding physical systems, and at the same time, can improve performance and sample efficiency in machine learning. Both pursuits require knowledge of the underlying symmetries in data. To address this, we propose learning symmetries directly from data via flow matching on Lie groups. We formulate symmetry discovery as learning a distribution over a larger hypothesis group, such that the learned distribution matches the symmetries observed in data. Relative to previous works, our method, \lieflow, is more flexible in terms of the types of groups it can discover and requires fewer assumptions. Experiments on 2D and 3D point clouds demonstrate the successful discovery of discrete groups, including reflections by flow matching over the complex domain. We identify a key challenge where the symmetric arrangement of the target modes causes ``last-minute convergence,'' where samples remain stationary until relatively late in the flow, and introduce a novel interpolation scheme for flow matching for symmetry discovery.

</details>


### [99] [Learning Skills from Action-Free Videos](https://arxiv.org/abs/2512.20052)
*Hung-Chieh Fang,Kuo-Han Hung,Chu-Rong Chen,Po-Jung Chou,Chun-Kai Yang,Po-Chen Ko,Yu-Chiang Wang,Yueh-Hua Wu,Min-Hung Chen,Shao-Hua Sun*

Main category: cs.AI

TL;DR: 本文提出 SOF 框架，通过光流学习视频中的潜在技能，解决了视频预测与机器人动作执行之间的脱节问题，显著增强了机器人在复杂任务中的规划和执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型难以转化为低级机器人动作，而潜在动作模型又缺乏高层规划能力。研究者希望从海量无动作标注的视频中学习可规划、可转化的潜在技能。

Method: 提出 SOF 框架，利用光流（Optical Flow）作为中间表示来学习潜在技能空间。该空间既能捕捉视频中的动态运动信息，又与机器人动作对齐，从而支持高层规划并简化技能向动作的转化。

Result: 实验表明，SOF 在多任务处理和长时程（long-horizon）任务场景中均表现出显著的性能提升，证明了其从原始视觉数据中直接获取和组合技能的能力。

Conclusion: SOF 通过光流中间表示弥补了视频生成模型与低级控制之间的差距，为从互联网规模的视频数据中学习通用的、可组合的机器人技能提供了一种有效途径。

Abstract: Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.

</details>


### [100] [Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach](https://arxiv.org/abs/2512.20056)
*Hao Li,Fabian Deuser,Wenping Yin,Steffen Knoblauch,Wufan Zhao,Filip Biljecki,Yong Xue,Wei Huang*

Main category: cs.AI

TL;DR: 本文提出 ProbGLC 框架，结合概率与确定性模型，实现在多种灾害场景下高精度的跨视图地理定位，并提供具有解释性的不确定性量化指标，助力快速灾害响应。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致极端灾害频发，快速准确地识别灾害位置对于应急响应至关重要，但现有方法在复杂灾害场景下的跨视图定位性能和可解释性仍有待提升。

Method: 提出 ProbGLC 框架，通过结合概率模型与确定性地理定位模型，实现跨视图地块匹配，并引入不确定性量化（概率分布和可定位性评分）来增强模型可解释性。

Result: 在 MultiIAN 和 SAGAIN-Disaster 两个多灾种数据集上的实验显示，ProbGLC 取得了 SOTA 性能（Acc@1km 达 0.86，Acc@25km 达 0.97），并能有效提供地理位置的概率分布和定位评分。

Conclusion: ProbGLC 证明了生成式跨视图方法在提升灾害响应位置感知方面的巨大潜力，为气候韧性和可持续发展提供了技术支持。

Abstract: As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC

</details>


### [101] [Scaling Reinforcement Learning for Content Moderation with Large Language Models](https://arxiv.org/abs/2512.20061)
*Hamed Firooz,Rui Liu,Yuchen Lu,Zhenyu Hou,Fangzhou Xiong,Xiaoyang Zhang,Changshu Jian,Zhicheng Zhu,Jiayuan Ma,Jacob Tao,Chaitali Gupta,Xiaochang Peng,Shike Mei,Hang Cui,Yang Qin,Shuo Tang,Jason Gaedtke,Arpit Mittal*

Main category: cs.AI

TL;DR: 本文研究了如何通过强化学习（RL）提升 LLM 在大规模内容审核中的性能，发现 RL 在复杂推理任务上极其高效，数据效率比监督学习高出两个数量级。


<details>
  <summary>Details</summary>
Motivation: 尽管 LLM 在内容审核中表现出潜力，但在标签稀缺、策略演变和需要深度推理的实际复杂场景中，如何达到专家级准确率仍缺乏深入研究。

Method: 系统评估了多种 RL 训练方案和奖励塑造策略（包括可验证奖励和 LLM-as-judge 框架），在三个真实内容审核任务中训练专用分类器。

Result: RL 训练呈现 S 型扩展特性（随数据和迭代增加而平滑提升并最终饱和）；在复杂任务上表现显著优于 SFT，且数据效率比 SFT 高出 100 倍。

Conclusion: RL 是构建高性能内容审核系统的有效途径，特别是在需要复杂推理和专家数据稀缺的场景下，其表现优于传统的监督微调。

Abstract: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.

</details>


### [102] [Reason2Decide: Rationale-Driven Multi-Task Learning](https://arxiv.org/abs/2512.20074)
*H M Quamran Hasan,Housam Khalifa Bashier,Jiayi Dai,Mi-Young Kim,Randy Goebel*

Main category: cs.AI

TL;DR: 本文提出了 Reason2Decide 框架，通过两阶段训练和计划采样，在显著减小模型规模的同时，解决了临床决策中预测与解释不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在临床决策支持中面临预测准确性与分述理由（Rationale）不一致的挑战。现有方法常因暴露偏差导致预测结果与解释相互矛盾。

Method: 提出 Reason2Decide 两阶段框架：第一阶段训练理由生成（Rationale Generation）；第二阶段通过计划采样（Scheduled Sampling）进行标签预测与理由生成的联合训练，以解决暴露偏差（Exposure Bias）。

Result: 在三个医学数据集上，该方法在 F1 分数及理由忠实度（BERTScore, BLEU, LLM-as-a-Judge）上均优于基线模型；即使使用 LLM 生成的理由进行训练，其表现依然强劲。

Conclusion: Reason2Decide 证明了通过小规模模型（比大模型小40倍）结合结构化微调，可以在临床预测任务中实现高准确度与解释一致性，减少了对人力标注的依赖并提升了系统鲁棒性。

Abstract: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.

</details>


### [103] [Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches](https://arxiv.org/abs/2512.20082)
*Chaithra,Kamesh Kadimisetty,Biju R Mohan*

Main category: cs.AI

TL;DR: 本文提出了一个整合大语言模型与实时股市反馈的自适应框架，通过微调 LLaMA 模型、RAG 技术及 PPO 强化学习，实现了情感分析与印度股市走势的动态对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的金融情感分析研究往往忽略了股票价格或市场反馈对情感判断的实时影响，导致模型缺乏市场感知能力。

Method: 微调 LLaMA 3.2 3B 模型，构建基于余弦相似度的 RAG 检索管线，并引入反馈驱动模块；最后利用 PPO 强化学习算法，根据情感预测与次日股票收益的一致性动态优化来源权重。

Result: 在 2024-2025 年 NIFTY 50 指数相关新闻数据集上的实验表明，该系统在分类准确率、F1 分数以及与市场的对齐度方面均显著优于基准模型和静态检索方法。

Conclusion: 结合指令微调 LLM、动态反馈机制和强化学习，通过实时市场反馈闭环，显著提升了金融文本情感分析的稳健性和市场意识。

Abstract: Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.

</details>


### [104] [MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization](https://arxiv.org/abs/2512.20135)
*Zhuo Yang,Yeyun chen,Jiaqing Xie,Ben Gao,Shuaike Shen,Wanhao Liu,Liujia Yang,Beilun Wang,Tianfan Fu,Yuqiang Li*

Main category: cs.AI

TL;DR: MolAct 是首个将分子设计形式化为智能体强化学习（Agentic RL）的问题，通过两阶段训练和工具增强，显著提升了 LLM 在分子编辑与优化任务中的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 分子编辑与优化是复杂的多步迭代过程，需在保持化学有效性和结构相似性的同时提升属性，传统方法难以平衡这些约束。

Method: 提出 MolAct 框架，采用两阶段 RL 训练：先培养基础编辑能力（MolEditAgent），再重用该能力进行属性优化（MolOptAgent），使 LLM 能够交替执行推理、工具调用及优化逻辑。

Result: MolEditAgent 在编辑任务上超越了 DeepSeek-R1 等模型；MolOptAgent 在 LogP 优化等任务上优于 Claude 3.7，且在溶解度等多个目标中保持极具竞争力的表现。

Conclusion: 将分子设计建模为多步、工具增强的 agentic 过程，是实现可靠且可解释的分子改进的关键途径。

Abstract: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed "thinking" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open "thinking" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed "thinking" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.

</details>


### [105] [Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection](https://arxiv.org/abs/2512.20140)
*Xingyou Yin,Ceyao Zhang,Min Hu,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出通过在标记化前为原始数据注入噪声，来增强全冻结大语言模型在时间序列预测任务中的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的全冻结（fully frozen）大语言模型在处理时间序列时，对数值的文本表示极其敏感，且无法通过微调适应分布偏移，导致性能不稳定。

Method: 提出在数值序列进行文本标记化（tokenization）之前，向原始时间序列中注入噪声，作为一种推理时增强（inference-time augmentation）手段。

Result: 理论分析和多项基准测试证明了该方法的有效性；在确保无数据污染的新数据集上，该方法依然能够显著提升预测性能。

Conclusion: 在推理阶段针对数值序列注入噪声是一种提升零样本时间序列预测鲁棒性的有效非侵入式策略。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.

</details>


### [106] [A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers](https://arxiv.org/abs/2512.20161)
*Dhivya Dharshini Kannan,Anupam Trivedi,Dipti Srinivasan*

Main category: cs.AI

TL;DR: 本研究提出了一种基于 BiGRU 的数据中心 PUE 预测模型，利用 RFECV 算法优化特征选择，有效提升了能效预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着边缘计算和 AI 的发展，数据中心能耗激增。精准预测电力使用效率（PUE）对于优化能源管理、降低成本及降低环境碳足迹至关重要。

Method: 使用 EnergyPlus 模拟新加坡数据中心生成 52,560 条样本（117 个特征），采用带交叉验证的递归特征消除（RFECV）进行特征筛选，并以此训练及优化双向门控循环单元（BiGRU）预测模型。

Result: 在 MSE、MAE 和 R-squared 等性能指标的评估下，基于优化后的 BiGRU 模型展现了比标准 GRU 模型更好的预测性能。

Conclusion: BiGRU 模型在数据中心 PUE 预测方面优于传统的 GRU 模型，通过结合特征选择和超参数优化，能够显著提升能效管理的预测精度。

Abstract: Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.

</details>


### [107] [Concept Generalization in Humans and Large Language Models: Insights from the Number Game](https://arxiv.org/abs/2512.20162)
*Arghavan Bazigaran,Hansem Sohn*

Main category: cs.AI

TL;DR: 研究通过数字游戏发现，人类在数学概念推理上比LLM更灵活且擅长少样本学习，而LLM更依赖硬性数学规则。


<details>
  <summary>Details</summary>
Motivation: 旨在通过概念推理任务，探讨并对比人类与大语言模型在数学概念泛化能力上的差异及其背后的逻辑机制。

Method: 使用贝叶斯模型（Bayesian model）作为分析框架，对比人类与LLM在数字游戏（Number Game）中的归纳偏置（Inductive Biases）和推理策略。

Result: 1. 人类能灵活结合规则和相似性进行推理，而LLM更偏向于数学规则；2. 人类具备单样本泛化能力，而LLM需要更多样本才能有效泛化。

Conclusion: 人类与LLM在概念推理上存在本质差异：人类的推理更为灵活且具备极强的少样本学习能力，而LLM过于依赖规则，在直觉化或基于相似性的泛化方面表现较弱。

Abstract: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.

</details>


### [108] [Offline Safe Policy Optimization From Heterogeneous Feedback](https://arxiv.org/abs/2512.20173)
*Ze Gong,Pradeep Varakantham,Akshat Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种名为 PreSa 的离线安全强化学习框架，它绕过显式的奖励和模型学习，直接通过偏好和安全标签优化策略，有效解决了长程控制任务中的误差累积和安全对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有的离线偏好强化学习（PbRL）在长程连续控制任务中，若先学习奖励/代价模型再进行约束强化学习，会因为误差累积导致性能受损及安全无法保障。

Method: 提出了 PreSa 框架。该方法结合偏好学习与安全对齐，将其转化为约束优化问题并利用拉格朗日乘子法求解。其核心在于直接利用轨迹段的对向偏好和安全标签来学习策略，而无需显式维护奖励和代价函数，从而减少了长程任务中的误差累积。

Result: 在合成及真实人类反馈的连续控制任务实验中，PreSa 学习到了高奖励且安全的策略，表现优于现有的离线安全强化学习基准（即便基准方法使用了真实的奖励和代价数据）。

Conclusion: PreSa 在不依赖显式奖励和代价模型的前提下，通过直接对策略进行偏好与安全对齐，能够比传统基于模型或带地面真值（ground-truth）的安全强化学习方法取得更好的性能。

Abstract: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.

</details>


### [109] [TongSIM: A General Platform for Simulating Intelligent Machines](https://arxiv.org/abs/2512.20206)
*Zhe Sun,Kunlun Wu,Chuanjian Fu,Zeming Song,Langyong Shi,Zihe Xue,Bohan Jing,Ying Yang,Xiaomeng Gao,Aijia Li,Tianyu Guo,Huiying Li,Xueyuan Yang,Rongkai Liu,Xinyi He,Yuxi Wang,Yue Li,Mingyuan Liu,Yujie Lu,Hongzhao Xie,Shiyun Zhao,Bo Dai,Wei Wang,Tao Yuan,Song-Chun Zhu,Yujia Peng,Zhenliang Zhang*

Main category: cs.AI

TL;DR: 本文介绍了 TongSIM，这是一个面向具身智能的高保真通用模拟平台，通过丰富的室内外场景和全面的评估基准，支持从基础导航到复杂社会协作的各类研究。


<details>
  <summary>Details</summary>
Motivation: 现有的具身智能模拟平台设计过于单一且任务受限，缺乏能够支持从底层导航到高层社会协作等复杂任务的通用平台。

Method: 推出了名为 TongSIM 的高保真、通用型平台，提供超过 100 个室内多房间场景和开放式户外城镇模拟，并配有全面的评估框架和基准测试。

Result: TongSIM 能够评估 Agent 的感知、认知、决策、人机协作及社会推理等多种能力，并支持自定义场景、任务自适应保真度和动态环境模拟。

Conclusion: TongSIM 作为一个统一、灵活且可扩展的环境，将有效加速通用具身智能的训练、评估和技术进步。

Abstract: As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.

</details>


### [110] [MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents](https://arxiv.org/abs/2512.20237)
*Xingbo Du,Loka Li,Duzhen Zhang,Le Song*

Main category: cs.AI

TL;DR: MemR$^3$ 通过引入路由选择和证据追踪机制，将记忆检索转变为一个自主闭环的智能体系统，显著提升了 LLM 在复杂任务中的记忆检索精度和回答质量。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 记忆系统多侧重于压缩与存储，缺乏对存储检索的显式、闭环控制。

Method: 提出 MemR$^3$ 系统，核心包含：1) 路由机制（Router），在检索、反思和回答动作间进行选择以优化质量；2) 全局证据缺失追踪器（Evidence-gap Tracker），使回答过程透明化并追踪证据收集。

Result: 在 LoCoMo 基准测试中，MemR$^3$ 优于强基线模型。使用 GPT-4.1-mini 时，使 RAG 性能提升 7.29%，Zep 提升 1.94%，显著增强了现有检索器的表现。

Conclusion: MemR$^3$ 为现有的存储系统提供了一个即插即用的闭环控制方案，有效解决了传统检索-回答架构中缺乏自主控制的问题。

Abstract: Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.

</details>


### [111] [ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge](https://arxiv.org/abs/2512.20276)
*Yuntao Dai,Hang Gu,Teng Wang,Qianyu Cheng,Yifei Zheng,Zhiyong Qiu,Lei Gong,Wenqi Lou,Xuehai Zhou*

Main category: cs.AI

TL;DR: 本文提出 ActionFlow，一种针对边缘平台优化的 VLA 推理框架。通过创新的跨请求流水平衡计算与内存负载，在不损失精度且无需重训的前提下，将 OpenVLA 推理速度提升 2.55 倍，达到机器人实时控制标准。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型由于自回归解码的内存限制，推理延迟高（仅3-5 Hz），无法满足机器人实时动态交互（20-30 Hz）的需求，且现有优化方法往往需要重新训练或牺牲模型精度。

Method: 提出跨请求流水线（Cross-Request Pipelining）调度策略，将不同步的解码阶段（Decode）与预填充阶段（Prefill）进行批处理。同时，设计并实现了跨请求状态打包前向算子（Cross-Request State Packed Forward）和统一 KV 环形缓冲区（Unified KV Ring Buffer），以优化内存操作。

Result: 在无需重新训练的情况下，ActionFlow 将 OpenVLA-7B 模型的推理帧率（FPS）提升了 2.55 倍，成功在资源受限的边缘硬件上实现了实时动态操作。

Conclusion: ActionFlow 为视觉-语言-动作模型在边缘设备上的实时部署提供了一种高效且无损的系统级解决方案，显著提升了机器人系统的动态交互能力。

Abstract: Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.

</details>


### [112] [Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation](https://arxiv.org/abs/2512.20278)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.AI

TL;DR: 本文研究了 LLM 如何通过自主编写可执行代码来构建程序化记忆，通过解决发现、验证、分解和扩展四大瓶颈，使智能体能从零开始自主生成跨服务处理的生产级工作流。


<details>
  <summary>Details</summary>
Motivation: 虽然代码是智能体程序化记忆的最佳表示形式，但在没有任何预设知识的情况下，如何自主合成这些记忆并解决从工具调用到工作流架构过渡中的瓶颈仍待探索。

Method: 提出了一种基于科学方法论（“假设、探测、编码”）的自主技能生成框架，并引入“线性状态锚定”（Linear State Anchoring）来替代传统的低效搜索。

Result: 成功识别并解决了自动化技能生成中的四个关键瓶颈：发现差距（工具检索）、验证差距（响应校验）、分解差距（效率优化）和缩放差距（并发与持久化）。

Conclusion: LLM 能够从零开始自主构建健壮的、生产级的工作流代码，通过结构化的“假设-探测-编码”方法论实现从工具使用者到架构师的转变。

Abstract: While CodeMem establishes executable code as the optimal representation for agentic procedural memory, the mechanism for autonomously synthesizing this memory from a blank slate remains underexplored. This paper operationalizes the transition of Large Language Models from passive tool-users to active workflow architects. Through a high-fidelity case study of a cross-service orchestration task involving Outlook and OneDrive, we identify and address four structural bottlenecks in automated skill generation: the Discovery Gap involving navigation of large tool registries, the Verification Gap regarding grounding tool response structures, the Decomposition Gap which replaces inefficient search with Linear State Anchoring, and the Scaling Gap focused on concurrency and persistence. We demonstrate that by enforcing a scientific methodology of hypothesize, probe, and code, agents can autonomously write robust, production-grade code skills.

</details>


### [113] [A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2512.20344)
*Yaowei Bai,Ruiheng Zhang,Yu Lei,Xuhua Duan,Jingfeng Yao,Shuguang Ju,Chaoyang Wang,Wei Yao,Yiwan Guo,Guilin Zhang,Chao Wan,Qian Yuan,Lei Chen,Wenjuan Tang,Biqiang Zhu,Xinggang Wang,Tao Sun,Wei Zhou,Dacheng Tao,Yongchao Xu,Chuansheng Zheng,Huangxuan Zhao,Bo Du*

Main category: cs.AI

TL;DR: 本文推出并验证了 Janus-Pro-CXR 模型，通过前瞻性临床试验证明其在提升胸片报告质量和缩短医生阅片时间方面优于 ChatGPT 4o 等大型模型。


<details>
  <summary>Details</summary>
Motivation: 针对全球放射科医生短缺及胸部 X 光工作量巨大的现状，目前的 AI 评估多依赖自动化指标或回顾性分析，缺乏严格的前瞻性临床验证。

Method: 基于 DeepSeek Janus-Pro 模型开发了 1B 参数的 Janus-Pro-CXR 系统，并开展了多中心前瞻性临床试验（NCT07117266），将 AI 生成报告与 ChatGPT 4o 等大模型及人工报告进行对比评估。

Result: 该系统在报告生成准确性上超越了 ChatGPT 4o 等大型模型；临床应用中使报告质量显著提升，判读时间缩短 18.3%（P < 0.001），且在 54.3% 的病例中获专家青睐。

Conclusion: Janus-Pro-CXR 通过轻量化架构和领域优化，显著提升了放射科诊断的可靠性与工作流效率，特别适用于医疗资源受限的环境，且具备开源特性。

Abstract: A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.

</details>


### [114] [Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale](https://arxiv.org/abs/2512.20469)
*Linfeng Zhang,Siheng Chen,Yuzhu Cai,Jingyi Chai,Junhan Chang,Kun Chen,Zhi X. Chen,Zhaohan Ding,Yuwen Du,Yuanpeng Gao,Yuan Gao,Jing Gao,Zhifeng Gao,Qiangqiang Gu,Yanhui Hong,Yuan Huang,Xi Fang,Xiaohong Ji,Guolin Ke,Zixing Lei,Xinyu Li,Yongge Li,Ruoxue Liao,Hang Lin,Xiaolu Lin,Yuxiang Liu,Xinzijian Liu,Zexi Liu,Jintan Lu,Tingjia Miao,Haohui Que,Weijie Sun,Yanfeng Wang,Bingyang Wu,Tianju Xue,Rui Ye,Jinzhe Zeng,Duo Zhang,Jiahui Zhang,Linfeng Zhang,Tianhan Zhang,Wenchang Zhang,Yuzhi Zhang,Zezhong Zhang,Hang Zheng,Hui Zhou,Tong Zhu,Xinyu Zhu,Qingguo Zhou,Weinan E*

Main category: cs.AI

TL;DR: 本文针对AI智能体在科学研发中难以规模化的问题，提出了Bohrium+SciMaster基础设施，通过系统化集成科学资产与编排长航时工作流，显著提升了科研效率与过程的可追溯性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI智能体在科学工作流中极具潜力，但目前面临大规模扩展难题：工作流难以观察和复现、实验室系统不具备智能体接口、执行过程缺乏溯源与治理，且现有系统多为定制化、难以复用。

Method: 提出了Bohrium+SciMaster架构：Bohrium作为科学资产中心（类比AI4S界的HuggingFace），将工具与系统转化为智能体就绪的能力；SciMaster则作为编排层，支撑长航时科学工作流。两者之间通过“科学智能基座”组织可复用的模型与知识。

Result: 通过11个具有代表性的主智能体在真实工作流中进行了验证，实现了端到端科学周期时间数个数量级的下降，并在数百万量级的真实负载中产生了基于执行的有效信号。

Conclusion: 扩展“智能体科学”需要一种结合基础设施与生态系统的方法，通过构建可重现、可追溯且可治理的架构，将AI从简单的辅助工具演变为驱动科学全流程的核心能力。

Abstract: AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.
  However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.
  We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.
  We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.

</details>


### [115] [Benchmarking LLMs for Predictive Applications in the Intensive Care Units](https://arxiv.org/abs/2512.20520)
*Chehak Malhotra,Mehak Gopal,Akshaya Devadiga,Pradeep Singh,Ridam Pal,Ritwik Kashyap,Tavpritesh Sethi*

Main category: cs.AI

TL;DR: 本研究对比了多种大语言模型与小规模模型在预测 ICU 患者休克中的表现，发现 LLM 的预测能力并未显著优于针对临床优化的 SLM。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理领域表现卓越，但在预测临床任务（如及时预测休克以实现早期干预）中的表现研究尚不充分。

Method: 使用 MIMIC-III 数据库中 17,294 次 ICU 住院数据，对比了 LLM（如 GatorTron-Base、Llama 8B、Mistral 7B）与传统模型（如 BioBERT、DocBERT、Word2Vec）在预测休克方面的表现，并采用焦距损失（Focal Loss）和交叉熵损失处理类别不平衡问题。

Result: GatorTron-Base 达到了最高的加权召回率（80.5%），但整体指标显示 LLM 与 SLM 的表现相当，并没有体现出明显的先天优势。

Conclusion: 在临床预测任务中，LLM 并不总是优于 SLM。未来 LLM 的临床研究应优先转向预测更复杂的临床轨迹，而非仅专注于文本识别或表型分析。

Abstract: With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.

</details>


### [116] [Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model](https://arxiv.org/abs/2512.20548)
*Zhiyi Duan,Xiangren Wang,Hongyu Yuan,Qianli Xing*

Main category: cs.AI

TL;DR: 本文通过构建首个大规模教师多模态情感数据集T-MED并提出非对称注意力模型AAM-TSA，解决了教学场景下情感分析的准确性与跨模态特征融合问题。


<details>
  <summary>Details</summary>
Motivation: 教师情感对教学至关重要，但现有研究难以应对教师情感的“表演性”，且忽略了教学内容本身对情感表达的影响。

Method: 构建了首个大规模教师多模态情感数据集T-MED，并提出了一种基于非对称注意力机制和分层门控单元的新型模型AAM-TSA，用于差异化的跨模态特征融合。

Result: 实验结果表明，AAM-TSA在T-MED数据集上的表现显著优于现有的最先进方法。

Conclusion: 结合教学信息的非对称多模态融合模型是提升教师情感识别准确性与解释性的关键。

Abstract: Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.

</details>
